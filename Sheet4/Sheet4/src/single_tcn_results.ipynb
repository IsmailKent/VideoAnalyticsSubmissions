{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "928d8edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from dataset import TCNDataset\n",
    "from model import TCN , MultiStageTCN , ParallelTCNs\n",
    "\n",
    "\n",
    "def train(model,dataloader,optimizer):\n",
    "    i=0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    running_loss = 0 \n",
    "    for features, labels, masks in dataloader:\n",
    "        out = model(features,masks)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 10 == 0:\n",
    "                print(\"    Batch {}: loss = {}\".format(i ,loss.item()))\n",
    "        i += 1\n",
    "        running_loss = loss.item()\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "    \n",
    "def train_parallel(model, dataloader,optimizer):\n",
    "    model.train()\n",
    "    i=0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    running_loss = 0 \n",
    "    for features, labels, masks in dataloader:\n",
    "        out1, out2, out3 ,out_average = model(features,masks)\n",
    "        optimizer.zero_grad()\n",
    "        loss1 = criterion(out1, labels)\n",
    "        loss2 = criterion(out2, labels)\n",
    "        loss3 = criterion(out3, labels)\n",
    "        loss_average = criterion(out_average, labels)\n",
    "        loss = loss1 + loss2 + loss3 + loss_average\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 10 == 0:\n",
    "                print(\"    Batch {}: combined loss = {}\".format(i ,loss.item()))\n",
    "        i += 1\n",
    "        running_loss = loss.item()\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "\n",
    "\n",
    "# function for zero padding for dataloader because of variable video length\n",
    "# inspired by the code from the paper\n",
    "def collate_fn_padd(batch):\n",
    "        batch_input , batch_target = [list(t) for t in zip(*batch)] \n",
    "        length_of_sequences = list(map(len, batch_target))\n",
    "        batch_input_tensor = torch.zeros(len(batch_input), np.shape(batch_input[0])[0], max(length_of_sequences), dtype=torch.float)\n",
    "        \n",
    "        batch_target_tensor = torch.ones(len(batch_input), max(length_of_sequences), dtype=torch.long)*(-100)\n",
    "        \n",
    "        mask = torch.zeros(len(batch_input), num_classes, max(length_of_sequences), dtype=torch.float)\n",
    "        \n",
    "        for i in range(len(batch_input)):\n",
    "            batch_input_tensor[i, :, :np.shape(batch_input[i])[1]] = batch_input[i]\n",
    "            \n",
    "            batch_target_tensor[i, :np.shape(batch_target[i])[0]] = batch_target[i]\n",
    "            \n",
    "            mask[i, :, :np.shape(batch_target[i])[0]] = torch.ones(num_classes, batch_target[i].shape[0])\n",
    "            \n",
    "        return batch_input_tensor, batch_target_tensor, mask\n",
    "            \n",
    "            \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 4\n",
    "epochs = 50\n",
    "num_classes = 48\n",
    "\n",
    "\n",
    "# DATA LOADERS \n",
    "\n",
    "training_dataset = TCNDataset(training=True)\n",
    "training_dataloader = torch.utils.data.DataLoader(training_dataset,collate_fn=collate_fn_padd,  batch_size=batch_size, shuffle=True, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23389ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "single_TCN = TCN()\n",
    "single_TCN_optimizer = torch.optim.Adam(single_TCN.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b12bbe4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING EPOCH: 1\n",
      "    Batch 0: loss = 3.891010046005249\n",
      "    Batch 10: loss = 3.343764305114746\n",
      "    Batch 20: loss = 2.984891176223755\n",
      "    Batch 30: loss = 2.8843634128570557\n",
      "    Batch 40: loss = 2.3481693267822266\n",
      "    Batch 50: loss = 3.232736349105835\n",
      "    Batch 60: loss = 2.3321526050567627\n",
      "    Batch 70: loss = 2.704082727432251\n",
      "    Batch 80: loss = 2.7374885082244873\n",
      "    Batch 90: loss = 1.9123780727386475\n",
      "    Batch 100: loss = 2.063551187515259\n",
      "    Batch 110: loss = 2.005189895629883\n",
      "    Batch 120: loss = 2.5331056118011475\n",
      "    Batch 130: loss = 2.5087087154388428\n",
      "    Batch 140: loss = 2.4343767166137695\n",
      "    Batch 150: loss = 1.7807143926620483\n",
      "    Batch 160: loss = 2.907348871231079\n",
      "    Batch 170: loss = 2.2333271503448486\n",
      "    Batch 180: loss = 2.4040868282318115\n",
      "RUNNING EPOCH: 2\n",
      "    Batch 0: loss = 2.128330945968628\n",
      "    Batch 10: loss = 2.0890893936157227\n",
      "    Batch 20: loss = 2.159660816192627\n",
      "    Batch 30: loss = 2.1074116230010986\n",
      "    Batch 40: loss = 2.2682130336761475\n",
      "    Batch 50: loss = 1.9619486331939697\n",
      "    Batch 60: loss = 2.4074666500091553\n",
      "    Batch 70: loss = 2.4899520874023438\n",
      "    Batch 80: loss = 2.5580430030822754\n",
      "    Batch 90: loss = 2.233919143676758\n",
      "    Batch 100: loss = 2.275123119354248\n",
      "    Batch 110: loss = 1.8315705060958862\n",
      "    Batch 120: loss = 1.9992746114730835\n",
      "    Batch 130: loss = 2.0548062324523926\n",
      "    Batch 140: loss = 1.7755714654922485\n",
      "    Batch 150: loss = 2.0436508655548096\n",
      "    Batch 160: loss = 2.202310085296631\n",
      "    Batch 170: loss = 2.212451934814453\n",
      "    Batch 180: loss = 2.050194501876831\n",
      "RUNNING EPOCH: 3\n",
      "    Batch 0: loss = 2.155571222305298\n",
      "    Batch 10: loss = 2.053579807281494\n",
      "    Batch 20: loss = 2.0243897438049316\n",
      "    Batch 30: loss = 2.140730619430542\n",
      "    Batch 40: loss = 1.9128211736679077\n",
      "    Batch 50: loss = 2.407933235168457\n",
      "    Batch 60: loss = 1.9097474813461304\n",
      "    Batch 70: loss = 2.1900558471679688\n",
      "    Batch 80: loss = 2.4877121448516846\n",
      "    Batch 90: loss = 1.9184249639511108\n",
      "    Batch 100: loss = 1.6137168407440186\n",
      "    Batch 110: loss = 1.3629510402679443\n",
      "    Batch 120: loss = 2.427849769592285\n",
      "    Batch 130: loss = 2.3935022354125977\n",
      "    Batch 140: loss = 1.6469451189041138\n",
      "    Batch 150: loss = 1.7347133159637451\n",
      "    Batch 160: loss = 1.6796196699142456\n",
      "    Batch 170: loss = 1.8291478157043457\n",
      "    Batch 180: loss = 1.9108598232269287\n",
      "RUNNING EPOCH: 4\n",
      "    Batch 0: loss = 1.7616527080535889\n",
      "    Batch 10: loss = 1.6102659702301025\n",
      "    Batch 20: loss = 1.7443039417266846\n",
      "    Batch 30: loss = 1.64102303981781\n",
      "    Batch 40: loss = 2.003382921218872\n",
      "    Batch 50: loss = 2.840404510498047\n",
      "    Batch 60: loss = 2.0738930702209473\n",
      "    Batch 70: loss = 1.8986170291900635\n",
      "    Batch 80: loss = 1.9616047143936157\n",
      "    Batch 90: loss = 1.4812768697738647\n",
      "    Batch 100: loss = 1.6155574321746826\n",
      "    Batch 110: loss = 2.348975419998169\n",
      "    Batch 120: loss = 2.523325204849243\n",
      "    Batch 130: loss = 1.6552705764770508\n",
      "    Batch 140: loss = 1.790597915649414\n",
      "    Batch 150: loss = 1.3440665006637573\n",
      "    Batch 160: loss = 1.7480170726776123\n",
      "    Batch 170: loss = 1.942672610282898\n",
      "    Batch 180: loss = 1.9214309453964233\n",
      "RUNNING EPOCH: 5\n",
      "    Batch 0: loss = 1.493936538696289\n",
      "    Batch 10: loss = 1.4695496559143066\n",
      "    Batch 20: loss = 1.3018691539764404\n",
      "    Batch 30: loss = 1.6786283254623413\n",
      "    Batch 40: loss = 1.1706857681274414\n",
      "    Batch 50: loss = 1.41214919090271\n",
      "    Batch 60: loss = 2.329680919647217\n",
      "    Batch 70: loss = 1.6599359512329102\n",
      "    Batch 80: loss = 1.3680461645126343\n",
      "    Batch 90: loss = 1.2823518514633179\n",
      "    Batch 100: loss = 1.739121913909912\n",
      "    Batch 110: loss = 1.7970139980316162\n",
      "    Batch 120: loss = 1.230753779411316\n",
      "    Batch 130: loss = 0.9543797969818115\n",
      "    Batch 140: loss = 1.5463556051254272\n",
      "    Batch 150: loss = 1.619823694229126\n",
      "    Batch 160: loss = 1.7330198287963867\n",
      "    Batch 170: loss = 1.7254087924957275\n",
      "    Batch 180: loss = 1.383750319480896\n",
      "RUNNING EPOCH: 6\n",
      "    Batch 0: loss = 1.407593011856079\n",
      "    Batch 10: loss = 1.4398348331451416\n",
      "    Batch 20: loss = 1.2027674913406372\n",
      "    Batch 30: loss = 1.9445921182632446\n",
      "    Batch 40: loss = 1.5070083141326904\n",
      "    Batch 50: loss = 1.702282428741455\n",
      "    Batch 60: loss = 1.7879725694656372\n",
      "    Batch 70: loss = 1.509070634841919\n",
      "    Batch 80: loss = 1.5420897006988525\n",
      "    Batch 90: loss = 1.4904215335845947\n",
      "    Batch 100: loss = 2.6417202949523926\n",
      "    Batch 110: loss = 1.6688108444213867\n",
      "    Batch 120: loss = 2.3002686500549316\n",
      "    Batch 130: loss = 1.2166833877563477\n",
      "    Batch 140: loss = 1.714765191078186\n",
      "    Batch 150: loss = 1.4446747303009033\n",
      "    Batch 160: loss = 1.1765938997268677\n",
      "    Batch 170: loss = 2.418335199356079\n",
      "    Batch 180: loss = 1.6702995300292969\n",
      "RUNNING EPOCH: 7\n",
      "    Batch 0: loss = 1.085283637046814\n",
      "    Batch 10: loss = 1.303565502166748\n",
      "    Batch 20: loss = 1.5787022113800049\n",
      "    Batch 30: loss = 1.4671708345413208\n",
      "    Batch 40: loss = 1.8203370571136475\n",
      "    Batch 50: loss = 1.4012911319732666\n",
      "    Batch 60: loss = 1.2384933233261108\n",
      "    Batch 70: loss = 1.1106828451156616\n",
      "    Batch 80: loss = 1.8220655918121338\n",
      "    Batch 90: loss = 1.899490237236023\n",
      "    Batch 100: loss = 1.6772022247314453\n",
      "    Batch 110: loss = 1.3505935668945312\n",
      "    Batch 120: loss = 1.2957795858383179\n",
      "    Batch 130: loss = 1.8720262050628662\n",
      "    Batch 140: loss = 1.8882420063018799\n",
      "    Batch 150: loss = 1.1301486492156982\n",
      "    Batch 160: loss = 1.7332719564437866\n",
      "    Batch 170: loss = 1.640336513519287\n",
      "    Batch 180: loss = 1.1395689249038696\n",
      "RUNNING EPOCH: 8\n",
      "    Batch 0: loss = 1.41225004196167\n",
      "    Batch 10: loss = 1.5068278312683105\n",
      "    Batch 20: loss = 1.7241214513778687\n",
      "    Batch 30: loss = 1.2469844818115234\n",
      "    Batch 40: loss = 1.5795495510101318\n",
      "    Batch 50: loss = 0.9209269285202026\n",
      "    Batch 60: loss = 1.7523471117019653\n",
      "    Batch 70: loss = 1.0300369262695312\n",
      "    Batch 80: loss = 1.7429835796356201\n",
      "    Batch 90: loss = 1.574302315711975\n",
      "    Batch 100: loss = 1.4962855577468872\n",
      "    Batch 110: loss = 1.463477373123169\n",
      "    Batch 120: loss = 2.0641682147979736\n",
      "    Batch 130: loss = 0.9827006459236145\n",
      "    Batch 140: loss = 1.015196681022644\n",
      "    Batch 150: loss = 1.7802540063858032\n",
      "    Batch 160: loss = 1.5033568143844604\n",
      "    Batch 170: loss = 2.0747697353363037\n",
      "    Batch 180: loss = 1.3223456144332886\n",
      "RUNNING EPOCH: 9\n",
      "    Batch 0: loss = 1.4943397045135498\n",
      "    Batch 10: loss = 0.838038444519043\n",
      "    Batch 20: loss = 0.7950124144554138\n",
      "    Batch 30: loss = 1.8406487703323364\n",
      "    Batch 40: loss = 1.5918245315551758\n",
      "    Batch 50: loss = 1.1851403713226318\n",
      "    Batch 60: loss = 0.8379846215248108\n",
      "    Batch 70: loss = 1.2711412906646729\n",
      "    Batch 80: loss = 1.368994951248169\n",
      "    Batch 90: loss = 1.49713933467865\n",
      "    Batch 100: loss = 1.7926559448242188\n",
      "    Batch 110: loss = 1.2546130418777466\n",
      "    Batch 120: loss = 1.8754730224609375\n",
      "    Batch 130: loss = 1.003717303276062\n",
      "    Batch 140: loss = 1.2712551355361938\n",
      "    Batch 150: loss = 1.2585465908050537\n",
      "    Batch 160: loss = 1.8121616840362549\n",
      "    Batch 170: loss = 1.027437686920166\n",
      "    Batch 180: loss = 1.29206383228302\n",
      "RUNNING EPOCH: 10\n",
      "    Batch 0: loss = 1.5569485425949097\n",
      "    Batch 10: loss = 0.9072279334068298\n",
      "    Batch 20: loss = 1.5314364433288574\n",
      "    Batch 30: loss = 1.261026382446289\n",
      "    Batch 40: loss = 1.2144607305526733\n",
      "    Batch 50: loss = 1.2367669343948364\n",
      "    Batch 60: loss = 2.1451256275177\n",
      "    Batch 70: loss = 1.1213970184326172\n",
      "    Batch 80: loss = 0.9995938539505005\n",
      "    Batch 90: loss = 1.3775193691253662\n",
      "    Batch 100: loss = 1.3822394609451294\n",
      "    Batch 110: loss = 0.9103528261184692\n",
      "    Batch 120: loss = 1.7365341186523438\n",
      "    Batch 130: loss = 1.3450653553009033\n",
      "    Batch 140: loss = 1.0476773977279663\n",
      "    Batch 150: loss = 1.509377121925354\n",
      "    Batch 160: loss = 1.3488738536834717\n",
      "    Batch 170: loss = 1.6270793676376343\n",
      "    Batch 180: loss = 1.3625266551971436\n",
      "RUNNING EPOCH: 11\n",
      "    Batch 0: loss = 1.335042953491211\n",
      "    Batch 10: loss = 1.0144299268722534\n",
      "    Batch 20: loss = 1.6290020942687988\n",
      "    Batch 30: loss = 1.1411912441253662\n",
      "    Batch 40: loss = 1.0467602014541626\n",
      "    Batch 50: loss = 1.4341822862625122\n",
      "    Batch 60: loss = 1.355109453201294\n",
      "    Batch 70: loss = 0.8918945789337158\n",
      "    Batch 80: loss = 1.1323330402374268\n",
      "    Batch 90: loss = 0.9986702799797058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch 100: loss = 1.7902750968933105\n",
      "    Batch 110: loss = 1.0403409004211426\n",
      "    Batch 120: loss = 1.0449045896530151\n",
      "    Batch 130: loss = 1.1544429063796997\n",
      "    Batch 140: loss = 1.2821348905563354\n",
      "    Batch 150: loss = 1.4884865283966064\n",
      "    Batch 160: loss = 1.5512279272079468\n",
      "    Batch 170: loss = 1.5238364934921265\n",
      "    Batch 180: loss = 1.11223304271698\n",
      "RUNNING EPOCH: 12\n",
      "    Batch 0: loss = 0.968706488609314\n",
      "    Batch 10: loss = 1.2494111061096191\n",
      "    Batch 20: loss = 1.7284404039382935\n",
      "    Batch 30: loss = 1.2753329277038574\n",
      "    Batch 40: loss = 1.5542116165161133\n",
      "    Batch 50: loss = 1.4011132717132568\n",
      "    Batch 60: loss = 1.3854570388793945\n",
      "    Batch 70: loss = 1.080426573753357\n",
      "    Batch 80: loss = 1.069066047668457\n",
      "    Batch 90: loss = 0.7510177493095398\n",
      "    Batch 100: loss = 0.9014356732368469\n",
      "    Batch 110: loss = 1.4500170946121216\n",
      "    Batch 120: loss = 1.6375812292099\n",
      "    Batch 130: loss = 1.2451589107513428\n",
      "    Batch 140: loss = 0.9888614416122437\n",
      "    Batch 150: loss = 1.3264074325561523\n",
      "    Batch 160: loss = 1.0990686416625977\n",
      "    Batch 170: loss = 1.3229366540908813\n",
      "    Batch 180: loss = 1.3899048566818237\n",
      "RUNNING EPOCH: 13\n",
      "    Batch 0: loss = 1.6452680826187134\n",
      "    Batch 10: loss = 0.7471999526023865\n",
      "    Batch 20: loss = 1.418379783630371\n",
      "    Batch 30: loss = 1.0907719135284424\n",
      "    Batch 40: loss = 1.5955291986465454\n",
      "    Batch 50: loss = 0.96364825963974\n",
      "    Batch 60: loss = 1.052264928817749\n",
      "    Batch 70: loss = 0.8196068406105042\n",
      "    Batch 80: loss = 1.1246594190597534\n",
      "    Batch 90: loss = 1.8457653522491455\n",
      "    Batch 100: loss = 1.1185064315795898\n",
      "    Batch 110: loss = 1.011401653289795\n",
      "    Batch 120: loss = 1.2268062829971313\n",
      "    Batch 130: loss = 1.1173629760742188\n",
      "    Batch 140: loss = 1.116988182067871\n",
      "    Batch 150: loss = 1.1904733180999756\n",
      "    Batch 160: loss = 1.4398833513259888\n",
      "    Batch 170: loss = 1.1252202987670898\n",
      "    Batch 180: loss = 1.1848645210266113\n",
      "RUNNING EPOCH: 14\n",
      "    Batch 0: loss = 0.9775075912475586\n",
      "    Batch 10: loss = 1.2649788856506348\n",
      "    Batch 20: loss = 1.138458013534546\n",
      "    Batch 30: loss = 0.6974101066589355\n",
      "    Batch 40: loss = 0.9814582467079163\n",
      "    Batch 50: loss = 0.7749814987182617\n",
      "    Batch 60: loss = 1.2528446912765503\n",
      "    Batch 70: loss = 1.1729185581207275\n",
      "    Batch 80: loss = 0.7968460917472839\n",
      "    Batch 90: loss = 0.9891670346260071\n",
      "    Batch 100: loss = 1.3506097793579102\n",
      "    Batch 110: loss = 0.8901689052581787\n",
      "    Batch 120: loss = 0.9660674333572388\n",
      "    Batch 130: loss = 0.9243304133415222\n",
      "    Batch 140: loss = 1.2849698066711426\n",
      "    Batch 150: loss = 0.8594297766685486\n",
      "    Batch 160: loss = 0.4545533359050751\n",
      "    Batch 170: loss = 0.8951898813247681\n",
      "    Batch 180: loss = 1.5328307151794434\n",
      "RUNNING EPOCH: 15\n",
      "    Batch 0: loss = 1.0251293182373047\n",
      "    Batch 10: loss = 0.8397378921508789\n",
      "    Batch 20: loss = 1.1710293292999268\n",
      "    Batch 30: loss = 0.990349292755127\n",
      "    Batch 40: loss = 0.8256416916847229\n",
      "    Batch 50: loss = 1.3747254610061646\n",
      "    Batch 60: loss = 0.8155689239501953\n",
      "    Batch 70: loss = 0.9099233746528625\n",
      "    Batch 80: loss = 0.8765065670013428\n",
      "    Batch 90: loss = 1.0115222930908203\n",
      "    Batch 100: loss = 1.1232532262802124\n",
      "    Batch 110: loss = 1.1911890506744385\n",
      "    Batch 120: loss = 0.8181206583976746\n",
      "    Batch 130: loss = 0.7189112901687622\n",
      "    Batch 140: loss = 0.8751665949821472\n",
      "    Batch 150: loss = 0.9578186273574829\n",
      "    Batch 160: loss = 1.1907099485397339\n",
      "    Batch 170: loss = 0.922224760055542\n",
      "    Batch 180: loss = 1.131706953048706\n",
      "RUNNING EPOCH: 16\n",
      "    Batch 0: loss = 0.8956586718559265\n",
      "    Batch 10: loss = 0.9493095278739929\n",
      "    Batch 20: loss = 0.6199077367782593\n",
      "    Batch 30: loss = 1.4648078680038452\n",
      "    Batch 40: loss = 1.360392689704895\n",
      "    Batch 50: loss = 0.7131573557853699\n",
      "    Batch 60: loss = 1.1213299036026\n",
      "    Batch 70: loss = 1.1796125173568726\n",
      "    Batch 80: loss = 0.8622984886169434\n",
      "    Batch 90: loss = 1.1533288955688477\n",
      "    Batch 100: loss = 0.9814923405647278\n",
      "    Batch 110: loss = 0.9286026358604431\n",
      "    Batch 120: loss = 0.7855533957481384\n",
      "    Batch 130: loss = 1.1330333948135376\n",
      "    Batch 140: loss = 0.870974063873291\n",
      "    Batch 150: loss = 0.7227105498313904\n",
      "    Batch 160: loss = 1.0487674474716187\n",
      "    Batch 170: loss = 0.8812268376350403\n",
      "    Batch 180: loss = 1.5606818199157715\n",
      "RUNNING EPOCH: 17\n",
      "    Batch 0: loss = 0.6651805639266968\n",
      "    Batch 10: loss = 1.2294902801513672\n",
      "    Batch 20: loss = 0.8607262969017029\n",
      "    Batch 30: loss = 0.7789825797080994\n",
      "    Batch 40: loss = 0.899303674697876\n",
      "    Batch 50: loss = 0.9237551093101501\n",
      "    Batch 60: loss = 0.5000565648078918\n",
      "    Batch 70: loss = 0.6752111911773682\n",
      "    Batch 80: loss = 1.1811026334762573\n",
      "    Batch 90: loss = 1.1146385669708252\n",
      "    Batch 100: loss = 1.3086570501327515\n",
      "    Batch 110: loss = 0.9244110584259033\n",
      "    Batch 120: loss = 0.8129399418830872\n",
      "    Batch 130: loss = 0.9925413727760315\n",
      "    Batch 140: loss = 0.9169679284095764\n",
      "    Batch 150: loss = 0.6137257218360901\n",
      "    Batch 160: loss = 1.3225406408309937\n",
      "    Batch 170: loss = 0.6142287850379944\n",
      "    Batch 180: loss = 0.8439591526985168\n",
      "RUNNING EPOCH: 18\n",
      "    Batch 0: loss = 0.9138441681861877\n",
      "    Batch 10: loss = 1.2466404438018799\n",
      "    Batch 20: loss = 0.9835700392723083\n",
      "    Batch 30: loss = 1.0847326517105103\n",
      "    Batch 40: loss = 0.8373703956604004\n",
      "    Batch 50: loss = 0.9763616919517517\n",
      "    Batch 60: loss = 0.6568771004676819\n",
      "    Batch 70: loss = 0.7798013687133789\n",
      "    Batch 80: loss = 1.613875150680542\n",
      "    Batch 90: loss = 0.8367799520492554\n",
      "    Batch 100: loss = 1.1188150644302368\n",
      "    Batch 110: loss = 1.021769404411316\n",
      "    Batch 120: loss = 1.2827969789505005\n",
      "    Batch 130: loss = 1.0052316188812256\n",
      "    Batch 140: loss = 0.7061715722084045\n",
      "    Batch 150: loss = 0.5311789512634277\n",
      "    Batch 160: loss = 0.6097474098205566\n",
      "    Batch 170: loss = 1.46678626537323\n",
      "    Batch 180: loss = 1.0802466869354248\n",
      "RUNNING EPOCH: 19\n",
      "    Batch 0: loss = 0.9059857726097107\n",
      "    Batch 10: loss = 1.669648289680481\n",
      "    Batch 20: loss = 0.7832030057907104\n",
      "    Batch 30: loss = 0.5609290599822998\n",
      "    Batch 40: loss = 1.1395679712295532\n",
      "    Batch 50: loss = 1.2289131879806519\n",
      "    Batch 60: loss = 1.0085585117340088\n",
      "    Batch 70: loss = 0.9616437554359436\n",
      "    Batch 80: loss = 0.5835171341896057\n",
      "    Batch 90: loss = 0.9841877222061157\n",
      "    Batch 100: loss = 0.7168360352516174\n",
      "    Batch 110: loss = 1.624828577041626\n",
      "    Batch 120: loss = 0.7149462699890137\n",
      "    Batch 130: loss = 0.7105445265769958\n",
      "    Batch 140: loss = 1.005800724029541\n",
      "    Batch 150: loss = 1.0574853420257568\n",
      "    Batch 160: loss = 0.9396234750747681\n",
      "    Batch 170: loss = 1.0700879096984863\n",
      "    Batch 180: loss = 0.8684167861938477\n",
      "RUNNING EPOCH: 20\n",
      "    Batch 0: loss = 0.8969250917434692\n",
      "    Batch 10: loss = 0.8881739377975464\n",
      "    Batch 20: loss = 0.8490694165229797\n",
      "    Batch 30: loss = 0.7507933378219604\n",
      "    Batch 40: loss = 0.6546518802642822\n",
      "    Batch 50: loss = 0.7809144854545593\n",
      "    Batch 60: loss = 0.5608001351356506\n",
      "    Batch 70: loss = 0.7269523739814758\n",
      "    Batch 80: loss = 1.524249792098999\n",
      "    Batch 90: loss = 1.5790164470672607\n",
      "    Batch 100: loss = 1.1866447925567627\n",
      "    Batch 110: loss = 1.116420030593872\n",
      "    Batch 120: loss = 0.5398473739624023\n",
      "    Batch 130: loss = 0.5331453680992126\n",
      "    Batch 140: loss = 0.6316549777984619\n",
      "    Batch 150: loss = 0.9581250548362732\n",
      "    Batch 160: loss = 1.0751073360443115\n",
      "    Batch 170: loss = 0.45057228207588196\n",
      "    Batch 180: loss = 0.6643720865249634\n",
      "RUNNING EPOCH: 21\n",
      "    Batch 0: loss = 0.8770802617073059\n",
      "    Batch 10: loss = 1.0582664012908936\n",
      "    Batch 20: loss = 0.8886368274688721\n",
      "    Batch 30: loss = 1.222816824913025\n",
      "    Batch 40: loss = 1.1268202066421509\n",
      "    Batch 50: loss = 0.9077785015106201\n",
      "    Batch 60: loss = 0.6552261114120483\n",
      "    Batch 70: loss = 0.5588632225990295\n",
      "    Batch 80: loss = 0.7544528841972351\n",
      "    Batch 90: loss = 0.6823886632919312\n",
      "    Batch 100: loss = 0.5446649789810181\n",
      "    Batch 110: loss = 0.739903450012207\n",
      "    Batch 120: loss = 0.6981180310249329\n",
      "    Batch 130: loss = 0.8688586354255676\n",
      "    Batch 140: loss = 0.5891689658164978\n",
      "    Batch 150: loss = 0.7042565941810608\n",
      "    Batch 160: loss = 0.656206488609314\n",
      "    Batch 170: loss = 1.160723328590393\n",
      "    Batch 180: loss = 0.5795454978942871\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RUNNING EPOCH: 22\n",
      "    Batch 0: loss = 0.7999970316886902\n",
      "    Batch 10: loss = 0.4883362650871277\n",
      "    Batch 20: loss = 0.8373667001724243\n",
      "    Batch 30: loss = 0.8382042646408081\n",
      "    Batch 40: loss = 0.9634243249893188\n",
      "    Batch 50: loss = 0.6439028382301331\n",
      "    Batch 60: loss = 0.6303288340568542\n",
      "    Batch 70: loss = 0.8969907760620117\n",
      "    Batch 80: loss = 0.7182168960571289\n",
      "    Batch 90: loss = 0.4932064414024353\n",
      "    Batch 100: loss = 0.8019971251487732\n",
      "    Batch 110: loss = 0.6895590424537659\n",
      "    Batch 120: loss = 0.5185454487800598\n",
      "    Batch 130: loss = 1.0687166452407837\n",
      "    Batch 140: loss = 0.45304515957832336\n",
      "    Batch 150: loss = 0.8687803149223328\n",
      "    Batch 160: loss = 1.1277669668197632\n",
      "    Batch 170: loss = 0.874983012676239\n",
      "    Batch 180: loss = 0.6292275190353394\n",
      "RUNNING EPOCH: 23\n",
      "    Batch 0: loss = 1.0053290128707886\n",
      "    Batch 10: loss = 0.8596996665000916\n",
      "    Batch 20: loss = 0.6544319987297058\n",
      "    Batch 30: loss = 0.7395153641700745\n",
      "    Batch 40: loss = 0.6544396281242371\n",
      "    Batch 50: loss = 0.6725771427154541\n",
      "    Batch 60: loss = 0.9301669597625732\n",
      "    Batch 70: loss = 0.6647299528121948\n",
      "    Batch 80: loss = 0.43053051829338074\n",
      "    Batch 90: loss = 0.6246136426925659\n",
      "    Batch 100: loss = 0.6431899666786194\n",
      "    Batch 110: loss = 0.7260561585426331\n",
      "    Batch 120: loss = 0.6700122952461243\n",
      "    Batch 130: loss = 1.1042418479919434\n",
      "    Batch 140: loss = 0.6184086203575134\n",
      "    Batch 150: loss = 0.8959342241287231\n",
      "    Batch 160: loss = 1.603116512298584\n",
      "    Batch 170: loss = 0.8515291213989258\n",
      "    Batch 180: loss = 0.6162261962890625\n",
      "RUNNING EPOCH: 24\n",
      "    Batch 0: loss = 0.42503273487091064\n",
      "    Batch 10: loss = 0.9321735501289368\n",
      "    Batch 20: loss = 0.6912904977798462\n",
      "    Batch 30: loss = 0.7422637343406677\n",
      "    Batch 40: loss = 1.0809484720230103\n",
      "    Batch 50: loss = 1.3363487720489502\n",
      "    Batch 60: loss = 0.8429682850837708\n",
      "    Batch 70: loss = 0.5870758295059204\n",
      "    Batch 80: loss = 0.6235081553459167\n",
      "    Batch 90: loss = 0.707397997379303\n",
      "    Batch 100: loss = 0.7897695899009705\n",
      "    Batch 110: loss = 0.8121228218078613\n",
      "    Batch 120: loss = 0.7539114952087402\n",
      "    Batch 130: loss = 0.7717834711074829\n",
      "    Batch 140: loss = 0.7300960421562195\n",
      "    Batch 150: loss = 0.6519748568534851\n",
      "    Batch 160: loss = 1.2562737464904785\n",
      "    Batch 170: loss = 0.5352284908294678\n",
      "    Batch 180: loss = 0.5897917151451111\n",
      "RUNNING EPOCH: 25\n",
      "    Batch 0: loss = 1.1109187602996826\n",
      "    Batch 10: loss = 0.7720250487327576\n",
      "    Batch 20: loss = 0.7138203382492065\n",
      "    Batch 30: loss = 0.9050270915031433\n",
      "    Batch 40: loss = 0.9268559813499451\n",
      "    Batch 50: loss = 0.7080792784690857\n",
      "    Batch 60: loss = 0.9295995831489563\n",
      "    Batch 70: loss = 0.5266173481941223\n",
      "    Batch 80: loss = 1.178634524345398\n",
      "    Batch 90: loss = 0.479088693857193\n",
      "    Batch 100: loss = 1.7317852973937988\n",
      "    Batch 110: loss = 0.9064347147941589\n",
      "    Batch 120: loss = 1.276763677597046\n",
      "    Batch 130: loss = 0.5746949315071106\n",
      "    Batch 140: loss = 0.8153491616249084\n",
      "    Batch 150: loss = 0.9084814190864563\n",
      "    Batch 160: loss = 0.7341659069061279\n",
      "    Batch 170: loss = 0.8650961518287659\n",
      "    Batch 180: loss = 1.1632256507873535\n",
      "RUNNING EPOCH: 26\n",
      "    Batch 0: loss = 0.7205624580383301\n",
      "    Batch 10: loss = 0.5344963073730469\n",
      "    Batch 20: loss = 0.7222378253936768\n",
      "    Batch 30: loss = 0.3514259159564972\n",
      "    Batch 40: loss = 0.6288309693336487\n",
      "    Batch 50: loss = 0.7336788177490234\n",
      "    Batch 60: loss = 0.8450224995613098\n",
      "    Batch 70: loss = 0.41044193506240845\n",
      "    Batch 80: loss = 0.8488882184028625\n",
      "    Batch 90: loss = 0.9284164905548096\n",
      "    Batch 100: loss = 0.9772056937217712\n",
      "    Batch 110: loss = 0.8606398105621338\n",
      "    Batch 120: loss = 0.5936381816864014\n",
      "    Batch 130: loss = 0.6483939290046692\n",
      "    Batch 140: loss = 0.6158084869384766\n",
      "    Batch 150: loss = 0.564594030380249\n",
      "    Batch 160: loss = 0.6481672525405884\n",
      "    Batch 170: loss = 0.7828711271286011\n",
      "    Batch 180: loss = 0.600188672542572\n",
      "RUNNING EPOCH: 27\n",
      "    Batch 0: loss = 0.6794509291648865\n",
      "    Batch 10: loss = 1.0360604524612427\n",
      "    Batch 20: loss = 0.5917526483535767\n",
      "    Batch 30: loss = 0.6205102801322937\n",
      "    Batch 40: loss = 0.7303863763809204\n",
      "    Batch 50: loss = 0.6864777207374573\n",
      "    Batch 60: loss = 0.407130628824234\n",
      "    Batch 70: loss = 0.8237150311470032\n",
      "    Batch 80: loss = 0.6068020462989807\n",
      "    Batch 90: loss = 0.5919698476791382\n",
      "    Batch 100: loss = 0.5945592522621155\n",
      "    Batch 110: loss = 0.4670797288417816\n",
      "    Batch 120: loss = 0.40173816680908203\n",
      "    Batch 130: loss = 0.9855314493179321\n",
      "    Batch 140: loss = 0.36171188950538635\n",
      "    Batch 150: loss = 0.4710726737976074\n",
      "    Batch 160: loss = 0.48847851157188416\n",
      "    Batch 170: loss = 0.7591260075569153\n",
      "    Batch 180: loss = 0.5385441184043884\n",
      "RUNNING EPOCH: 28\n",
      "    Batch 0: loss = 0.5753895044326782\n",
      "    Batch 10: loss = 0.7281213998794556\n",
      "    Batch 20: loss = 0.598488450050354\n",
      "    Batch 30: loss = 0.8756678700447083\n",
      "    Batch 40: loss = 0.5032573938369751\n",
      "    Batch 50: loss = 0.472831130027771\n",
      "    Batch 60: loss = 0.5757206082344055\n",
      "    Batch 70: loss = 0.7313434481620789\n",
      "    Batch 80: loss = 0.5502735376358032\n",
      "    Batch 90: loss = 0.5281362533569336\n",
      "    Batch 100: loss = 0.47882816195487976\n",
      "    Batch 110: loss = 1.0144816637039185\n",
      "    Batch 120: loss = 0.7552279233932495\n",
      "    Batch 130: loss = 0.9247052669525146\n",
      "    Batch 140: loss = 0.49281445145606995\n",
      "    Batch 150: loss = 0.6300684213638306\n",
      "    Batch 160: loss = 0.8557311296463013\n",
      "    Batch 170: loss = 0.4932933747768402\n",
      "    Batch 180: loss = 0.8636398911476135\n",
      "RUNNING EPOCH: 29\n",
      "    Batch 0: loss = 0.5359639525413513\n",
      "    Batch 10: loss = 0.4634793996810913\n",
      "    Batch 20: loss = 0.4237549901008606\n",
      "    Batch 30: loss = 0.3823297321796417\n",
      "    Batch 40: loss = 0.4680972397327423\n",
      "    Batch 50: loss = 0.8147789239883423\n",
      "    Batch 60: loss = 0.39771768450737\n",
      "    Batch 70: loss = 0.9564444422721863\n",
      "    Batch 80: loss = 0.7757906317710876\n",
      "    Batch 90: loss = 0.5413761138916016\n",
      "    Batch 100: loss = 0.6920789480209351\n",
      "    Batch 110: loss = 0.9094125628471375\n",
      "    Batch 120: loss = 0.8237415552139282\n",
      "    Batch 130: loss = 0.7525397539138794\n",
      "    Batch 140: loss = 1.0338598489761353\n",
      "    Batch 150: loss = 0.6191325783729553\n",
      "    Batch 160: loss = 0.8308632373809814\n",
      "    Batch 170: loss = 0.9632886052131653\n",
      "    Batch 180: loss = 0.8712813258171082\n",
      "RUNNING EPOCH: 30\n",
      "    Batch 0: loss = 0.3263700604438782\n",
      "    Batch 10: loss = 0.5513463020324707\n",
      "    Batch 20: loss = 0.6607668399810791\n",
      "    Batch 30: loss = 0.46030664443969727\n",
      "    Batch 40: loss = 0.40235477685928345\n",
      "    Batch 50: loss = 0.5079805850982666\n",
      "    Batch 60: loss = 0.5111836791038513\n",
      "    Batch 70: loss = 0.6313782930374146\n",
      "    Batch 80: loss = 0.648991584777832\n",
      "    Batch 90: loss = 0.8004122972488403\n",
      "    Batch 100: loss = 0.6184887290000916\n",
      "    Batch 110: loss = 0.7582945823669434\n",
      "    Batch 120: loss = 0.8273510932922363\n",
      "    Batch 130: loss = 0.5346287488937378\n",
      "    Batch 140: loss = 0.16414359211921692\n",
      "    Batch 150: loss = 0.2949886620044708\n",
      "    Batch 160: loss = 0.7226049304008484\n",
      "    Batch 170: loss = 0.8631522059440613\n",
      "    Batch 180: loss = 0.42316490411758423\n",
      "RUNNING EPOCH: 31\n",
      "    Batch 0: loss = 0.505089282989502\n",
      "    Batch 10: loss = 0.6879640817642212\n",
      "    Batch 20: loss = 0.2702435851097107\n",
      "    Batch 30: loss = 0.3447014391422272\n",
      "    Batch 40: loss = 0.7693530917167664\n",
      "    Batch 50: loss = 0.5835186243057251\n",
      "    Batch 60: loss = 0.575503408908844\n",
      "    Batch 70: loss = 0.6587908267974854\n",
      "    Batch 80: loss = 0.8853774666786194\n",
      "    Batch 90: loss = 0.7133346199989319\n",
      "    Batch 100: loss = 0.6402236819267273\n",
      "    Batch 110: loss = 0.6869487166404724\n",
      "    Batch 120: loss = 0.5566460490226746\n",
      "    Batch 130: loss = 0.4120338559150696\n",
      "    Batch 140: loss = 0.4526337683200836\n",
      "    Batch 150: loss = 1.1657462120056152\n",
      "    Batch 160: loss = 0.859150767326355\n",
      "    Batch 170: loss = 0.975153923034668\n",
      "    Batch 180: loss = 0.7700067758560181\n",
      "RUNNING EPOCH: 32\n",
      "    Batch 0: loss = 0.295429527759552\n",
      "    Batch 10: loss = 0.5742253065109253\n",
      "    Batch 20: loss = 0.5751227140426636\n",
      "    Batch 30: loss = 0.6543392539024353\n",
      "    Batch 40: loss = 0.4410816729068756\n",
      "    Batch 50: loss = 0.33197692036628723\n",
      "    Batch 60: loss = 0.6386404037475586\n",
      "    Batch 70: loss = 0.7521413564682007\n",
      "    Batch 80: loss = 0.5349416732788086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch 90: loss = 0.6280130743980408\n",
      "    Batch 100: loss = 0.4807473123073578\n",
      "    Batch 110: loss = 0.5148897171020508\n",
      "    Batch 120: loss = 0.559730589389801\n",
      "    Batch 130: loss = 0.4541029930114746\n",
      "    Batch 140: loss = 0.5253770351409912\n",
      "    Batch 150: loss = 0.4756004214286804\n",
      "    Batch 160: loss = 0.481520414352417\n",
      "    Batch 170: loss = 0.3923790156841278\n",
      "    Batch 180: loss = 0.8623682856559753\n",
      "RUNNING EPOCH: 33\n",
      "    Batch 0: loss = 0.6993626952171326\n",
      "    Batch 10: loss = 0.45590347051620483\n",
      "    Batch 20: loss = 0.3383035361766815\n",
      "    Batch 30: loss = 0.6009023189544678\n",
      "    Batch 40: loss = 0.4285874664783478\n",
      "    Batch 50: loss = 0.5243211388587952\n",
      "    Batch 60: loss = 0.7607300877571106\n",
      "    Batch 70: loss = 0.5710909962654114\n",
      "    Batch 80: loss = 0.6057955622673035\n",
      "    Batch 90: loss = 0.34043124318122864\n",
      "    Batch 100: loss = 0.3883204162120819\n",
      "    Batch 110: loss = 0.3809094727039337\n",
      "    Batch 120: loss = 0.42326799035072327\n",
      "    Batch 130: loss = 0.36993029713630676\n",
      "    Batch 140: loss = 0.4067525565624237\n",
      "    Batch 150: loss = 0.49488595128059387\n",
      "    Batch 160: loss = 0.7084454298019409\n",
      "    Batch 170: loss = 0.6657861471176147\n",
      "    Batch 180: loss = 0.7950276732444763\n",
      "RUNNING EPOCH: 34\n",
      "    Batch 0: loss = 0.3273772895336151\n",
      "    Batch 10: loss = 1.0463122129440308\n",
      "    Batch 20: loss = 0.3241574764251709\n",
      "    Batch 30: loss = 0.3638322651386261\n",
      "    Batch 40: loss = 0.5075451731681824\n",
      "    Batch 50: loss = 0.47283121943473816\n",
      "    Batch 60: loss = 0.5326293706893921\n",
      "    Batch 70: loss = 0.5223325490951538\n",
      "    Batch 80: loss = 0.5315698385238647\n",
      "    Batch 90: loss = 0.5006022453308105\n",
      "    Batch 100: loss = 0.3297208547592163\n",
      "    Batch 110: loss = 0.44945693016052246\n",
      "    Batch 120: loss = 0.46874111890792847\n",
      "    Batch 130: loss = 0.6600204706192017\n",
      "    Batch 140: loss = 0.4723566174507141\n",
      "    Batch 150: loss = 0.5382301807403564\n",
      "    Batch 160: loss = 0.468977153301239\n",
      "    Batch 170: loss = 0.3683452308177948\n",
      "    Batch 180: loss = 1.1996463537216187\n",
      "RUNNING EPOCH: 35\n",
      "    Batch 0: loss = 0.43414536118507385\n",
      "    Batch 10: loss = 0.47372323274612427\n",
      "    Batch 20: loss = 0.8407623767852783\n",
      "    Batch 30: loss = 0.3288379907608032\n",
      "    Batch 40: loss = 0.5420010685920715\n",
      "    Batch 50: loss = 0.6234018206596375\n",
      "    Batch 60: loss = 0.43669092655181885\n",
      "    Batch 70: loss = 0.4783749282360077\n",
      "    Batch 80: loss = 0.4224961996078491\n",
      "    Batch 90: loss = 0.39617857336997986\n",
      "    Batch 100: loss = 0.4067951440811157\n",
      "    Batch 110: loss = 0.6064103245735168\n",
      "    Batch 120: loss = 0.18945159018039703\n",
      "    Batch 130: loss = 0.47498971223831177\n",
      "    Batch 140: loss = 0.40177232027053833\n",
      "    Batch 150: loss = 0.6603793501853943\n",
      "    Batch 160: loss = 0.48255473375320435\n",
      "    Batch 170: loss = 0.4708330035209656\n",
      "    Batch 180: loss = 0.5480990409851074\n",
      "RUNNING EPOCH: 36\n",
      "    Batch 0: loss = 0.5664886832237244\n",
      "    Batch 10: loss = 0.41210511326789856\n",
      "    Batch 20: loss = 0.6022260189056396\n",
      "    Batch 30: loss = 0.7930576205253601\n",
      "    Batch 40: loss = 0.4639165699481964\n",
      "    Batch 50: loss = 0.6907766461372375\n",
      "    Batch 60: loss = 0.46401447057724\n",
      "    Batch 70: loss = 0.38169994950294495\n",
      "    Batch 80: loss = 0.4220788776874542\n",
      "    Batch 90: loss = 0.8985247015953064\n",
      "    Batch 100: loss = 0.7897478342056274\n",
      "    Batch 110: loss = 0.45375874638557434\n",
      "    Batch 120: loss = 0.48976245522499084\n",
      "    Batch 130: loss = 0.4723053276538849\n",
      "    Batch 140: loss = 0.5017512440681458\n",
      "    Batch 150: loss = 0.4323405921459198\n",
      "    Batch 160: loss = 0.5479042530059814\n",
      "    Batch 170: loss = 0.39159703254699707\n",
      "    Batch 180: loss = 0.5000274777412415\n",
      "RUNNING EPOCH: 37\n",
      "    Batch 0: loss = 0.5743228197097778\n",
      "    Batch 10: loss = 0.7341036200523376\n",
      "    Batch 20: loss = 0.7353709936141968\n",
      "    Batch 30: loss = 0.2752584218978882\n",
      "    Batch 40: loss = 0.330513060092926\n",
      "    Batch 50: loss = 0.20723702013492584\n",
      "    Batch 60: loss = 0.40258026123046875\n",
      "    Batch 70: loss = 0.6011481285095215\n",
      "    Batch 80: loss = 0.8128843903541565\n",
      "    Batch 90: loss = 0.9277636408805847\n",
      "    Batch 100: loss = 0.33322274684906006\n",
      "    Batch 110: loss = 0.5845778584480286\n",
      "    Batch 120: loss = 0.5267081260681152\n",
      "    Batch 130: loss = 0.6909438967704773\n",
      "    Batch 140: loss = 0.6029868125915527\n",
      "    Batch 150: loss = 0.34021568298339844\n",
      "    Batch 160: loss = 0.5566284656524658\n",
      "    Batch 170: loss = 0.4110231101512909\n",
      "    Batch 180: loss = 0.26361560821533203\n",
      "RUNNING EPOCH: 38\n",
      "    Batch 0: loss = 0.7142150402069092\n",
      "    Batch 10: loss = 0.24472768604755402\n",
      "    Batch 20: loss = 0.6652276515960693\n",
      "    Batch 30: loss = 0.25264641642570496\n",
      "    Batch 40: loss = 0.47926604747772217\n",
      "    Batch 50: loss = 0.4740368127822876\n",
      "    Batch 60: loss = 0.5242009162902832\n",
      "    Batch 70: loss = 0.5450601577758789\n",
      "    Batch 80: loss = 0.5339498519897461\n",
      "    Batch 90: loss = 0.40254512429237366\n",
      "    Batch 100: loss = 0.3235666751861572\n",
      "    Batch 110: loss = 0.43569260835647583\n",
      "    Batch 120: loss = 0.4250863194465637\n",
      "    Batch 130: loss = 0.26927024126052856\n",
      "    Batch 140: loss = 0.258171409368515\n",
      "    Batch 150: loss = 0.2926291823387146\n",
      "    Batch 160: loss = 0.6003333926200867\n",
      "    Batch 170: loss = 0.748113751411438\n",
      "    Batch 180: loss = 0.5378233790397644\n",
      "RUNNING EPOCH: 39\n",
      "    Batch 0: loss = 0.31378066539764404\n",
      "    Batch 10: loss = 0.4994676113128662\n",
      "    Batch 20: loss = 0.4418543875217438\n",
      "    Batch 30: loss = 0.18908534944057465\n",
      "    Batch 40: loss = 0.5479865670204163\n",
      "    Batch 50: loss = 0.7016988396644592\n",
      "    Batch 60: loss = 0.4076538681983948\n",
      "    Batch 70: loss = 1.188258171081543\n",
      "    Batch 80: loss = 0.4136207699775696\n",
      "    Batch 90: loss = 0.36803779006004333\n",
      "    Batch 100: loss = 0.20975838601589203\n",
      "    Batch 110: loss = 0.5522499680519104\n",
      "    Batch 120: loss = 0.22389601171016693\n",
      "    Batch 130: loss = 0.22423002123832703\n",
      "    Batch 140: loss = 0.3046664297580719\n",
      "    Batch 150: loss = 0.5407032370567322\n",
      "    Batch 160: loss = 0.31528976559638977\n",
      "    Batch 170: loss = 0.4331141412258148\n",
      "    Batch 180: loss = 0.46003809571266174\n",
      "RUNNING EPOCH: 40\n",
      "    Batch 0: loss = 0.6933101415634155\n",
      "    Batch 10: loss = 0.626875638961792\n",
      "    Batch 20: loss = 0.2782275676727295\n",
      "    Batch 30: loss = 0.3467461168766022\n",
      "    Batch 40: loss = 0.3410136103630066\n",
      "    Batch 50: loss = 0.31265273690223694\n",
      "    Batch 60: loss = 0.27837422490119934\n",
      "    Batch 70: loss = 0.6221538186073303\n",
      "    Batch 80: loss = 0.2903812825679779\n",
      "    Batch 90: loss = 0.4157417416572571\n",
      "    Batch 100: loss = 0.8473397493362427\n",
      "    Batch 110: loss = 0.5552322268486023\n",
      "    Batch 120: loss = 0.24822795391082764\n",
      "    Batch 130: loss = 0.5331878662109375\n",
      "    Batch 140: loss = 0.3772130608558655\n",
      "    Batch 150: loss = 0.6638009548187256\n",
      "    Batch 160: loss = 0.6737565994262695\n",
      "    Batch 170: loss = 0.4261736273765564\n",
      "    Batch 180: loss = 0.5647528171539307\n",
      "RUNNING EPOCH: 41\n",
      "    Batch 0: loss = 0.31454890966415405\n",
      "    Batch 10: loss = 0.33574551343917847\n",
      "    Batch 20: loss = 0.4565559923648834\n",
      "    Batch 30: loss = 0.46019789576530457\n",
      "    Batch 40: loss = 0.2777723968029022\n",
      "    Batch 50: loss = 0.4395095705986023\n",
      "    Batch 60: loss = 0.5062866806983948\n",
      "    Batch 70: loss = 0.3861275017261505\n",
      "    Batch 80: loss = 0.29665878415107727\n",
      "    Batch 90: loss = 0.4862743616104126\n",
      "    Batch 100: loss = 0.47730258107185364\n",
      "    Batch 110: loss = 0.2837299406528473\n",
      "    Batch 120: loss = 0.47948992252349854\n",
      "    Batch 130: loss = 0.4469912052154541\n",
      "    Batch 140: loss = 0.6628048419952393\n",
      "    Batch 150: loss = 0.5050879716873169\n",
      "    Batch 160: loss = 0.4037240147590637\n",
      "    Batch 170: loss = 0.6066181063652039\n",
      "    Batch 180: loss = 0.43393272161483765\n",
      "RUNNING EPOCH: 42\n",
      "    Batch 0: loss = 0.4314301908016205\n",
      "    Batch 10: loss = 0.3059181571006775\n",
      "    Batch 20: loss = 0.5313325524330139\n",
      "    Batch 30: loss = 0.33508333563804626\n",
      "    Batch 40: loss = 0.3352434039115906\n",
      "    Batch 50: loss = 0.3155606985092163\n",
      "    Batch 60: loss = 0.4050951302051544\n",
      "    Batch 70: loss = 0.33506786823272705\n",
      "    Batch 80: loss = 0.16842667758464813\n",
      "    Batch 90: loss = 0.5790619850158691\n",
      "    Batch 100: loss = 0.28795814514160156\n",
      "    Batch 110: loss = 0.2971297800540924\n",
      "    Batch 120: loss = 0.3217366337776184\n",
      "    Batch 130: loss = 0.2863081097602844\n",
      "    Batch 140: loss = 0.39085108041763306\n",
      "    Batch 150: loss = 0.24060148000717163\n",
      "    Batch 160: loss = 0.4336395561695099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Batch 170: loss = 0.30756068229675293\n",
      "    Batch 180: loss = 0.3696420192718506\n",
      "RUNNING EPOCH: 43\n",
      "    Batch 0: loss = 0.5478893518447876\n",
      "    Batch 10: loss = 0.27844205498695374\n",
      "    Batch 20: loss = 0.29182934761047363\n",
      "    Batch 30: loss = 0.35142335295677185\n",
      "    Batch 40: loss = 0.4096445143222809\n",
      "    Batch 50: loss = 0.30758628249168396\n",
      "    Batch 60: loss = 0.48383381962776184\n",
      "    Batch 70: loss = 0.4548671543598175\n",
      "    Batch 80: loss = 0.3763805031776428\n",
      "    Batch 90: loss = 0.37022122740745544\n",
      "    Batch 100: loss = 0.6656994223594666\n",
      "    Batch 110: loss = 0.42969244718551636\n",
      "    Batch 120: loss = 0.32677724957466125\n",
      "    Batch 130: loss = 0.32900163531303406\n",
      "    Batch 140: loss = 0.7724692821502686\n",
      "    Batch 150: loss = 0.44220486283302307\n",
      "    Batch 160: loss = 0.5028329491615295\n",
      "    Batch 170: loss = 0.4410719573497772\n",
      "    Batch 180: loss = 0.26864951848983765\n",
      "RUNNING EPOCH: 44\n",
      "    Batch 0: loss = 0.5424371957778931\n",
      "    Batch 10: loss = 0.886870265007019\n",
      "    Batch 20: loss = 0.48436328768730164\n",
      "    Batch 30: loss = 0.4336429834365845\n",
      "    Batch 40: loss = 0.32174909114837646\n",
      "    Batch 50: loss = 0.29955512285232544\n",
      "    Batch 60: loss = 0.2183256298303604\n",
      "    Batch 70: loss = 0.34225958585739136\n",
      "    Batch 80: loss = 0.4305820167064667\n",
      "    Batch 90: loss = 0.6339622735977173\n",
      "    Batch 100: loss = 0.31525084376335144\n",
      "    Batch 110: loss = 0.27846765518188477\n",
      "    Batch 120: loss = 0.663146436214447\n",
      "    Batch 130: loss = 0.3839569091796875\n",
      "    Batch 140: loss = 0.6182703971862793\n",
      "    Batch 150: loss = 0.33165132999420166\n",
      "    Batch 160: loss = 0.5234481692314148\n",
      "    Batch 170: loss = 0.6145320534706116\n",
      "    Batch 180: loss = 1.0976405143737793\n",
      "RUNNING EPOCH: 45\n",
      "    Batch 0: loss = 0.8779141902923584\n",
      "    Batch 10: loss = 0.4768918454647064\n",
      "    Batch 20: loss = 0.4496261179447174\n",
      "    Batch 30: loss = 0.4592835009098053\n",
      "    Batch 40: loss = 0.5508468747138977\n",
      "    Batch 50: loss = 0.4485170841217041\n",
      "    Batch 60: loss = 0.5116416811943054\n",
      "    Batch 70: loss = 0.32024097442626953\n",
      "    Batch 80: loss = 0.3341406285762787\n",
      "    Batch 90: loss = 0.5437103509902954\n",
      "    Batch 100: loss = 0.23934493958950043\n",
      "    Batch 110: loss = 0.46678853034973145\n",
      "    Batch 120: loss = 0.36170604825019836\n",
      "    Batch 130: loss = 0.4623444378376007\n",
      "    Batch 140: loss = 0.24575559794902802\n",
      "    Batch 150: loss = 0.3671766519546509\n",
      "    Batch 160: loss = 0.33622151613235474\n",
      "    Batch 170: loss = 0.41011515259742737\n",
      "    Batch 180: loss = 0.32944878935813904\n",
      "RUNNING EPOCH: 46\n",
      "    Batch 0: loss = 0.42850184440612793\n",
      "    Batch 10: loss = 0.32585617899894714\n",
      "    Batch 20: loss = 0.2625039517879486\n",
      "    Batch 30: loss = 0.45061758160591125\n",
      "    Batch 40: loss = 0.26564469933509827\n",
      "    Batch 50: loss = 0.33059269189834595\n",
      "    Batch 60: loss = 0.2986461818218231\n",
      "    Batch 70: loss = 0.29763156175613403\n",
      "    Batch 80: loss = 0.2598033547401428\n",
      "    Batch 90: loss = 0.23826012015342712\n",
      "    Batch 100: loss = 0.2969582676887512\n",
      "    Batch 110: loss = 0.20534910261631012\n",
      "    Batch 120: loss = 0.2877780795097351\n",
      "    Batch 130: loss = 0.201167032122612\n",
      "    Batch 140: loss = 0.30730465054512024\n",
      "    Batch 150: loss = 0.40401676297187805\n",
      "    Batch 160: loss = 0.356960654258728\n",
      "    Batch 170: loss = 0.11018732935190201\n",
      "    Batch 180: loss = 0.2602442800998688\n",
      "RUNNING EPOCH: 47\n",
      "    Batch 0: loss = 0.2970380187034607\n",
      "    Batch 10: loss = 0.2534320652484894\n",
      "    Batch 20: loss = 0.1530250757932663\n",
      "    Batch 30: loss = 0.10192476212978363\n",
      "    Batch 40: loss = 0.2527679204940796\n",
      "    Batch 50: loss = 0.14744344353675842\n",
      "    Batch 60: loss = 0.3239549994468689\n",
      "    Batch 70: loss = 0.48274731636047363\n",
      "    Batch 80: loss = 0.35870394110679626\n",
      "    Batch 90: loss = 0.20105797052383423\n",
      "    Batch 100: loss = 0.20096644759178162\n",
      "    Batch 110: loss = 0.24855831265449524\n",
      "    Batch 120: loss = 0.45971977710723877\n",
      "    Batch 130: loss = 0.39014336466789246\n",
      "    Batch 140: loss = 0.5623607635498047\n",
      "    Batch 150: loss = 0.3300892114639282\n",
      "    Batch 160: loss = 0.37788620591163635\n",
      "    Batch 170: loss = 0.706365168094635\n",
      "    Batch 180: loss = 0.6990451216697693\n",
      "RUNNING EPOCH: 48\n",
      "    Batch 0: loss = 0.5160266757011414\n",
      "    Batch 10: loss = 0.2349773794412613\n",
      "    Batch 20: loss = 0.2811487317085266\n",
      "    Batch 30: loss = 0.6023007035255432\n",
      "    Batch 40: loss = 0.3540742099285126\n",
      "    Batch 50: loss = 0.46325838565826416\n",
      "    Batch 60: loss = 0.32503125071525574\n",
      "    Batch 70: loss = 0.2990282475948334\n",
      "    Batch 80: loss = 0.1678237020969391\n",
      "    Batch 90: loss = 0.16157956421375275\n",
      "    Batch 100: loss = 0.30764538049697876\n",
      "    Batch 110: loss = 0.25691458582878113\n",
      "    Batch 120: loss = 0.3140947222709656\n",
      "    Batch 130: loss = 0.6228460073471069\n",
      "    Batch 140: loss = 0.3623008131980896\n",
      "    Batch 150: loss = 0.36116790771484375\n",
      "    Batch 160: loss = 0.3323279917240143\n",
      "    Batch 170: loss = 0.44654300808906555\n",
      "    Batch 180: loss = 0.5309590101242065\n",
      "RUNNING EPOCH: 49\n",
      "    Batch 0: loss = 0.3866877853870392\n",
      "    Batch 10: loss = 0.30544984340667725\n",
      "    Batch 20: loss = 0.18941746652126312\n",
      "    Batch 30: loss = 0.26196959614753723\n",
      "    Batch 40: loss = 0.16051964461803436\n",
      "    Batch 50: loss = 0.3533686101436615\n",
      "    Batch 60: loss = 0.3560374677181244\n",
      "    Batch 70: loss = 0.596171498298645\n",
      "    Batch 80: loss = 0.3011983036994934\n",
      "    Batch 90: loss = 0.2844815254211426\n",
      "    Batch 100: loss = 0.4279918074607849\n",
      "    Batch 110: loss = 0.7415722608566284\n",
      "    Batch 120: loss = 0.8424598574638367\n",
      "    Batch 130: loss = 0.6305944919586182\n",
      "    Batch 140: loss = 0.4193016290664673\n",
      "    Batch 150: loss = 0.5761614441871643\n",
      "    Batch 160: loss = 0.5102618336677551\n",
      "    Batch 170: loss = 0.7789977788925171\n",
      "    Batch 180: loss = 0.5501040816307068\n",
      "RUNNING EPOCH: 50\n",
      "    Batch 0: loss = 0.5517842769622803\n",
      "    Batch 10: loss = 0.5804561376571655\n",
      "    Batch 20: loss = 0.3066767156124115\n",
      "    Batch 30: loss = 0.19380350410938263\n",
      "    Batch 40: loss = 1.2156051397323608\n",
      "    Batch 50: loss = 0.5493556261062622\n",
      "    Batch 60: loss = 0.31463921070098877\n",
      "    Batch 70: loss = 0.6855711936950684\n",
      "    Batch 80: loss = 0.643024206161499\n",
      "    Batch 90: loss = 0.5620898008346558\n",
      "    Batch 100: loss = 0.2675381004810333\n",
      "    Batch 110: loss = 0.2985965311527252\n",
      "    Batch 120: loss = 0.4465935528278351\n",
      "    Batch 130: loss = 0.3463435471057892\n",
      "    Batch 140: loss = 0.36171993613243103\n",
      "    Batch 150: loss = 0.2754470407962799\n",
      "    Batch 160: loss = 0.22773544490337372\n",
      "    Batch 170: loss = 0.3857888877391815\n",
      "    Batch 180: loss = 0.28930768370628357\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epochs):\n",
    "    print(\"RUNNING EPOCH: {}\".format(epoch+1))\n",
    "    train(single_TCN,training_dataloader , single_TCN_optimizer )\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f169920d",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(single_TCN, \"./single_tcn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fef88c2",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b82b72fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval import eval_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5556da3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TCNDataset(training=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,collate_fn=collate_fn_padd,  batch_size=1, shuffle=False, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "67b59d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acc: 49.2666\n",
      "Edit: 6.3570\n",
      "F1@0.10: 4.7232\n",
      "F1@0.25: 3.6205\n",
      "F1@0.50: 2.0951\n"
     ]
    }
   ],
   "source": [
    "eval_(single_TCN,test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
