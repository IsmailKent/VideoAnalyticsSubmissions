{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "edd834d2",
   "metadata": {},
   "source": [
    "# not sure about the implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23e78e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from dataset import TCNDataset\n",
    "from model import TCN , MultiStageTCN , ParallelTCNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8edaf4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,dataloader,optimizer):\n",
    "    i=0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    running_loss = 0 \n",
    "    for features, labels, masks in dataloader:\n",
    "        out = model(features,masks)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(out, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 10 == 0:\n",
    "                print(\"    Batch {}: loss = {}\".format(i ,loss.item()))\n",
    "        i += 1\n",
    "        running_loss = loss.item()\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "\n",
    "    \n",
    "def train_parallel(model, dataloader,optimizer):\n",
    "    model.train()\n",
    "    i=0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    running_loss = 0 \n",
    "    for features, labels, masks in dataloader:\n",
    "        out1, out2, out3 ,out_average = model(features,masks)\n",
    "        optimizer.zero_grad()\n",
    "        loss1 = criterion(out1, labels)\n",
    "        loss2 = criterion(out2, labels)\n",
    "        loss3 = criterion(out3, labels)\n",
    "        loss_average = criterion(out_average, labels)\n",
    "        loss = loss1 + loss2 + loss3 + loss_average\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 10 == 0:\n",
    "                print(\"    Batch {}: combined loss = {}\".format(i ,loss.item()))\n",
    "        i += 1\n",
    "        running_loss = loss.item()\n",
    "    return running_loss / len(dataloader)\n",
    "\n",
    "\n",
    "\n",
    "# function for zero padding for dataloader because of variable video length\n",
    "# inspired by the code from the paper\n",
    "def collate_fn_padd(batch):\n",
    "        batch_input , batch_target = [list(t) for t in zip(*batch)] \n",
    "        length_of_sequences = list(map(len, batch_target))\n",
    "        batch_input_tensor = torch.zeros(len(batch_input), np.shape(batch_input[0])[0], max(length_of_sequences), dtype=torch.float)\n",
    "        \n",
    "        batch_target_tensor = torch.ones(len(batch_input), max(length_of_sequences), dtype=torch.long)*(-100)\n",
    "        \n",
    "        mask = torch.zeros(len(batch_input), num_classes, max(length_of_sequences), dtype=torch.float)\n",
    "        \n",
    "        for i in range(len(batch_input)):\n",
    "            batch_input_tensor[i, :, :np.shape(batch_input[i])[1]] = batch_input[i]\n",
    "            \n",
    "            batch_target_tensor[i, :np.shape(batch_target[i])[0]] = batch_target[i]\n",
    "            \n",
    "            mask[i, :, :np.shape(batch_target[i])[0]] = torch.ones(num_classes, batch_target[i].shape[0])\n",
    "            \n",
    "        return batch_input_tensor, batch_target_tensor, mask\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2388d851",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "batch_size = 4\n",
    "epochs = 50\n",
    "num_classes = 48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d05c28e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = TCNDataset(training=True)\n",
    "training_dataloader = torch.utils.data.DataLoader(training_dataset,collate_fn=collate_fn_padd,  batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "test_dataset = TCNDataset(training=False)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset,collate_fn=collate_fn_padd,  batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "multi_stage_TCN = MultiStageTCN()\n",
    "multi_stage_TCN_optimizer = torch.optim.Adam(multi_stage_TCN.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5739c47f",
   "metadata": {},
   "source": [
    "Repeat Question 2 with an additional video-level loss.\n",
    "# 1. The new loss computes the binary cross-entropy between a multi-class video level prediction and a multi-class target that indicates which classes are present in the video.\n",
    "\n",
    "# The target is a vector with a dimension equals the number of classes in the dataset.  The i-th element of this vector is 1 if the i-th class is present in the video, otherwise it should be 0. \n",
    "\n",
    "# To get the video level prediction apply a max pooling on the temporal dimension of the predicted frame-wise logits. \n",
    "\n",
    "# Note that the final loss function is the sum of the video level loss and the frame-wise cross-entropy loss that is used in Question 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6fd1de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([])\n",
      "tensor(3.8380, grad_fn=<NllLoss2DBackward>)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "running_loss = 0 \n",
    "for features, labels, masks in training_dataloader:\n",
    "        out = multi_stage_TCN(features,masks)\n",
    "        multi_stage_TCN_optimizer.zero_grad()\n",
    "        loss = criterion(out, labels)# +video_level_loss(out, labels)\n",
    "        print(loss.shape)\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        multi_stage_TCN_optimizer.step()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d3363f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 915])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e0c6ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0,    0,    0, ..., -100, -100, -100],\n",
       "       [   0,    0,    0, ..., -100, -100, -100],\n",
       "       [   0,    0,    0, ..., -100, -100, -100],\n",
       "       [   0,    0,    0, ...,    0,    0,    0]], dtype=int64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49a3d82",
   "metadata": {},
   "source": [
    "# get_target_vector:\n",
    "The target is a vector with a dimension equals the number of classes in the dataset. The i-th element of this vector is 1 if the i-th class is present in the video, otherwise it should be 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5862fd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# get target vector\n",
    "# The target is a vector with a dimension equals the number of classes in the dataset. \n",
    "# The i-th element of this vector is 1 if the i-th class is present in the video, otherwise it should be 0. \n",
    "def get_target_vector(labels):\n",
    "    target = torch.zeros((labels.shape[0],48)) # labels 0-47\n",
    "    i = 0\n",
    "    \n",
    "    available_classes = [torch.unique(t) for t in labels]\n",
    "    available_classes_non_negative = [ torch.nn.functional.relu(ac) for ac in available_classes]\n",
    "    for i in range(labels.shape[0]):\n",
    "        target[i][available_classes_non_negative[i]] = 1\n",
    "\n",
    "    return(target) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "546616e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "t=get_target_vector(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "dc42fcfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 48])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "49032de4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 0.],\n",
       "        [1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 0., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [1., 1., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f5b8c39",
   "metadata": {},
   "source": [
    "# Get the video level prediction:\n",
    "Apply a max pooling on the temporal dimension of the predicted frame-wise logits. \n",
    "Question: Size of the max pooling? Is it necessary to apply softmax in the output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12544543",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To get the video level prediction apply a max pooling on\n",
    "#the temporal dimension of the predicted frame-wise logits.\n",
    "def get_video_level_prediction(out):\n",
    "    # Max pool in all the frames?????\n",
    "    m = nn.MaxPool1d(out.shape[2])\n",
    "    o = m(out)\n",
    "    return (o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bfe9be2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.MaxPool1d(out.shape[2])\n",
    "o = m(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "acc3fd3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 48, 915])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "10a99435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 48, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c058255f",
   "metadata": {},
   "source": [
    "# Def video_level_loss(out, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab70ec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def video_level_loss(input,target): #video_prediction, target_vector\n",
    "    return (torch.nn.functional.binary_cross_entropy(input, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "93f73478",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'target' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-ddb2b9571c15>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0minput_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_video_level_prediction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtarget_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_target_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mvideo_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvideo_level_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'target' is not defined"
     ]
    }
   ],
   "source": [
    "input_ = get_video_level_prediction(out)\n",
    "target_ = get_target_vector(labels)\n",
    "video_loss = video_level_loss(input,target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
