{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "name": "parallel_tcn_results.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Il6JtP4xsJqz",
        "outputId": "49936ae6-391b-4979-8139-ce723939f832"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "id": "Il6JtP4xsJqz",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hG6MbAY6sKYD",
        "outputId": "43d4f977-218d-42e3-db3e-b5e103f05e87"
      },
      "source": [
        "!unzip \"/content/gdrive/MyDrive/data(1).zip\"\n"
      ],
      "id": "hG6MbAY6sKYD",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/gdrive/MyDrive/data(1).zip\n",
            "replace data/features/P03_cam01_P03_cereals.npy? [y]es, [n]o, [A]ll, [N]one, [r]ename: N\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OrRWr4dp0Ilf"
      },
      "source": [
        "import torch\n",
        "from torchvision.io.video import read_video\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import torchvision.transforms.functional as tf\n",
        "\n",
        "\n",
        "\n",
        "class TCNDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self,path='/content/data' , training = True):\n",
        "        \n",
        "        self.path = path\n",
        "        # load classes mapping\n",
        "        self.class2index = {} \n",
        "        self.index2class = {}\n",
        "        classes_file = open(path+'/mapping.txt','r')\n",
        "        for line in classes_file:\n",
        "            line = line.rstrip('\\n') \n",
        "            splitted = line.split(' ')\n",
        "            self.class2index[splitted[1]] = splitted[0]\n",
        "            self.index2class[splitted[0]] = splitted[1]\n",
        "        classes_file.close()\n",
        "        \n",
        "        # load class names \n",
        "        video_list_location = '/train.bundle' if training else '/test.bundle'\n",
        "        video_list_file = open(path+video_list_location)\n",
        "        self.video_list = []\n",
        "        for line in video_list_file:\n",
        "            line = line.rstrip('\\n')\n",
        "            name = line.split('.txt')[0]\n",
        "            self.video_list.append(name)\n",
        "            \n",
        "        video_list_file.close()\n",
        "        \n",
        "        self.size = len(self.video_list)\n",
        "        self.training = training\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.size\n",
        "\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        video_name = self.video_list[index]\n",
        "        features = torch.from_numpy(np.load(self.path+'/features/'+video_name+'.npy'))\n",
        "        labels_file = open(self.path+'/groundTruth/'+video_name+'.txt')\n",
        "        labels = torch.zeros((features.shape[1],))\n",
        "        for idx, line in enumerate(labels_file):\n",
        "             line = line.rstrip('\\n')\n",
        "             label = self.class2index[line]\n",
        "             labels[idx] = int(label)\n",
        "        labels_file.close()\n",
        "        return features, labels\n",
        "\n",
        "                "
      ],
      "id": "OrRWr4dp0Ilf",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptj-pSvC0SZM"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# From figure 2 in paper\n",
        "class DilatedResidualLayer(torch.nn.Module):\n",
        "    def __init__(self, dilation_factor, in_channels, out_channels):\n",
        "        super(DilatedResidualLayer, self).__init__()\n",
        "        # padding = dilation_factor to keep size \n",
        "        self.block =  nn.Sequential(  nn.Conv1d(in_channels, out_channels, 3, padding=dilation_factor, dilation=dilation_factor),\n",
        "                                      nn.ReLU(inplace = True),\n",
        "                                      nn.Conv1d(out_channels, out_channels, 1)\n",
        "                                      )\n",
        "    def forward(self,x, mask):\n",
        "        return (self.block(x) + x )* mask[:, 0:1, :]\n",
        "\n",
        "    \n",
        "\n",
        "class TCN(torch.nn.Module):\n",
        "    def __init__(self, num_layers = 10, num_f_maps=64, dim = 2048, num_classes = 48):\n",
        "        super(TCN, self).__init__()\n",
        "        self.conv_1x1 = nn.Conv1d(dim, num_f_maps, 1)\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(num_layers):\n",
        "            # linear increasing of number dilation using i+1\n",
        "            layer = DilatedResidualLayer(i+1, num_f_maps, num_f_maps)\n",
        "            self.layers.append(layer)\n",
        "        self.last_conv = nn.Conv1d(num_f_maps, num_classes, 1)\n",
        "\n",
        "    def forward(self, x , mask):\n",
        "        out = self.conv_1x1(x)\n",
        "        for layer in self.layers:\n",
        "            out = layer(out,mask)\n",
        "        out = self.last_conv(out)  * mask[:, 0:1, :]\n",
        "        return out\n",
        "    \n",
        "    \n",
        "class MultiStageTCN(torch.nn.Module):\n",
        "    def __init__(self, num_stages = 4, num_layers = 10, num_f_maps=64, dim = 2048, num_classes = 48):\n",
        "        super(MultiStageTCN, self).__init__()\n",
        "        self.first_TCN = TCN(num_layers, num_f_maps, dim, num_classes)\n",
        "        self.TCNs = nn.ModuleList()\n",
        "        for i in range(num_layers-1):\n",
        "            tcn =  TCN(num_layers, num_f_maps, num_classes + dim, num_classes)\n",
        "            self.TCNs.append(tcn)\n",
        "        self.last_TCN =  TCN(num_layers, num_f_maps, num_classes + dim, num_classes)\n",
        "\n",
        "    def forward(self, x , mask):\n",
        "        out = self.first_TCN(x,mask)   \n",
        "        out = torch.cat((x, out), dim=1)\n",
        "        #out_wrapped = out.unsqueeze(0)\n",
        "        for stage in self.TCNs:\n",
        "            out = stage(out,mask)\n",
        "            out = F.softmax(out, dim=1)   \n",
        "            out = torch.cat((x, out), dim=1)\n",
        "            out = stage( out* mask[:, 0:1, :], mask)\n",
        "            out = torch.cat((x, out), dim=1)\n",
        "            #out_wrapped = torch.cat((out_wrapped, out.unsqueeze(0)), dim=0)\n",
        "        out = self.last_TCN(out,mask)   \n",
        "        #out_wrapped = torch.cat((out_wrapped, out.unsqueeze(0)), dim=0)\n",
        "        return out\n",
        "    \n",
        "\n",
        "# For question 4, supporting down and upsampling  \n",
        "class SampledTCN(torch.nn.Module):\n",
        "    def __init__(self, sampling_factor, num_layers = 10, num_f_maps=64, dim = 2048, num_classes = 48 ):\n",
        "        super(SampledTCN, self).__init__()\n",
        "        # we downsamble using convolutions \n",
        "        self.downsamble_conv = nn.Conv1d(dim, dim//sampling_factor, 1)\n",
        "        self.conv_1x1 = nn.Conv1d(dim//sampling_factor, num_f_maps, 1)\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(num_layers):\n",
        "            layer = DilatedResidualLayer(i+1, num_f_maps, num_f_maps)\n",
        "            self.layers.append(layer)\n",
        "        self.upsample_conv = nn.Conv1d(num_f_maps, dim, 1)\n",
        "\n",
        "    def forward(self, x , mask):\n",
        "        out = self.downsamble_conv(x)\n",
        "        out = self.conv_1x1(out)\n",
        "        for layer in self.layers:\n",
        "            out = layer(out,mask)\n",
        "        out = self.upsample_conv(out) * mask[:, 0:1, :]\n",
        "        return out\n",
        "    \n",
        "    \n",
        "    \n",
        "class ParallelTCNs(torch.nn.Module):\n",
        "        def __init__(self, num_layers = 10, num_f_maps=64, dim = 2048, num_classes = 48):\n",
        "            super(ParallelTCNs, self).__init__()\n",
        "            self.TCN1 = SampledTCN(1, num_layers, num_f_maps, dim, num_classes)\n",
        "            # scale reduced with factor 4 2048 -> 512\n",
        "            self.TCN2 = SampledTCN(4, num_layers, num_f_maps, dim, num_classes)\n",
        "             # scale reduced with factor 8 2048 -> 256\n",
        "            self.TCN3 = SampledTCN(8, num_layers, num_f_maps, dim, num_classes)\n",
        "            \n",
        "            self.prediction_conv1 = nn.Conv1d(dim, num_classes, 1)\n",
        "            self.prediction_conv2 = nn.Conv1d(dim, num_classes, 1)\n",
        "            self.prediction_conv3 = nn.Conv1d(dim, num_classes, 1)\n",
        "\n",
        "            self.prediction_conv_average = nn.Conv1d(dim, num_classes, 1)\n",
        "            \n",
        "            \n",
        "        def forward(self, x, mask):\n",
        "            out1 = self.TCN1(x,mask)\n",
        "            out2 = self.TCN2(x,mask)\n",
        "            out3 = self.TCN3(x,mask)\n",
        "            \n",
        "            average_out = torch.mean(torch.stack([out1,out2,out3]) , dim = 0)\n",
        "            out1 = self.prediction_conv1(out1) * mask[:, 0:1, :]\n",
        "            out2 = self.prediction_conv2(out2) * mask[:, 0:1, :]\n",
        "            out3 = self.prediction_conv3(out3) * mask[:, 0:1, :]\n",
        "            average_out = self.prediction_conv_average(average_out) * mask[:, 0:1, :]\n",
        "            return out1 , out2 , out3 , average_out\n",
        "            \n",
        "            \n",
        "\n"
      ],
      "id": "ptj-pSvC0SZM",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dfezvc6K0Wmw"
      },
      "source": [
        "# coding: utf-8\n",
        "import numpy as np\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "def get_labels_start_end_time(frame_wise_labels, bg_class=[\"background\"]):\n",
        "    labels = []\n",
        "    starts = []\n",
        "    ends = []\n",
        "    last_label = frame_wise_labels[0]\n",
        "    if frame_wise_labels[0] not in bg_class:\n",
        "        labels.append(frame_wise_labels[0])\n",
        "        starts.append(0)\n",
        "    for i in range(len(frame_wise_labels)):\n",
        "        if frame_wise_labels[i] != last_label:\n",
        "            if frame_wise_labels[i] not in bg_class:\n",
        "                labels.append(frame_wise_labels[i])\n",
        "                starts.append(i)\n",
        "            if last_label not in bg_class:\n",
        "                ends.append(i)\n",
        "            last_label = frame_wise_labels[i]\n",
        "    if last_label not in bg_class:\n",
        "        ends.append(i + 1)\n",
        "    return labels, starts, ends\n",
        "\n",
        "\n",
        "def levenstein(p, y, norm=False):\n",
        "    m_row = len(p)    \n",
        "    n_col = len(y)\n",
        "    D = np.zeros([m_row+1, n_col+1], np.float64)\n",
        "    for i in range(m_row+1):\n",
        "        D[i, 0] = i\n",
        "    for i in range(n_col+1):\n",
        "        D[0, i] = i\n",
        "\n",
        "    for j in range(1, n_col+1):\n",
        "        for i in range(1, m_row+1):\n",
        "            if y[j-1] == p[i-1]:\n",
        "                D[i, j] = D[i-1, j-1]\n",
        "            else:\n",
        "                D[i, j] = min(D[i-1, j] + 1,\n",
        "                              D[i, j-1] + 1,\n",
        "                              D[i-1, j-1] + 1)\n",
        "    \n",
        "    if norm:\n",
        "        score = (1 - D[-1, -1]/max(m_row, n_col)) * 100\n",
        "    else:\n",
        "        score = D[-1, -1]\n",
        "\n",
        "    return score\n",
        "\n",
        "\n",
        "def edit_score(recognized, ground_truth, norm=True, bg_class=[\"background\"]):\n",
        "    P, _, _ = get_labels_start_end_time(recognized, bg_class)\n",
        "    Y, _, _ = get_labels_start_end_time(ground_truth, bg_class)\n",
        "    return levenstein(P, Y, norm)\n",
        "\n",
        "\n",
        "def f_score(recognized, ground_truth, overlap, bg_class=[\"background\"]):\n",
        "    p_label, p_start, p_end = get_labels_start_end_time(recognized, bg_class)\n",
        "    y_label, y_start, y_end = get_labels_start_end_time(ground_truth, bg_class)\n",
        "\n",
        "    tp = 0\n",
        "    fp = 0\n",
        "\n",
        "    hits = np.zeros(len(y_label))\n",
        "\n",
        "    for j in range(len(p_label)):\n",
        "        intersection = np.minimum(p_end[j], y_end) - np.maximum(p_start[j], y_start)\n",
        "        union = np.maximum(p_end[j], y_end) - np.minimum(p_start[j], y_start)\n",
        "        IoU = (1.0*intersection / union)*([p_label[j] == y_label[x] for x in range(len(y_label))])\n",
        "        # Get the best scoring segment\n",
        "        idx = np.array(IoU).argmax()\n",
        "\n",
        "        if IoU[idx] >= overlap and not hits[idx]:\n",
        "            tp += 1\n",
        "            hits[idx] = 1\n",
        "        else:\n",
        "            fp += 1\n",
        "    fn = len(y_label) - sum(hits)\n",
        "    return float(tp), float(fp), float(fn)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def read_file(path):\n",
        "    with open(path, 'r') as f:\n",
        "        content = f.read()\n",
        "        f.close()\n",
        "    return content\n",
        "\n",
        "\n",
        "def get_results(model,features, labels, masks): # Input  = features, labels and masks for 1 image\n",
        "    _,_,_,out = model(features,masks)\n",
        "    out_hot = torch.max(out,1).indices #?? Should we apply softmax first?\n",
        "    recog_content = out_hot.cpu().numpy()\n",
        "    recog_content = recog_content.reshape(recog_content.shape[1])\n",
        "    gt_content = labels.cpu().numpy()\n",
        "    gt_content = gt_content.reshape(gt_content.shape[1])\n",
        "    return(gt_content,recog_content)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def eval_(model, test_dataloader):# Batch size 1\n",
        "    model.eval()\n",
        "    recog_path = sys.argv[1] # pass the path of the directory that contains your predictions as a command line parameter\n",
        "    ground_truth_path = \"/content/data/groundTruth/\"\n",
        "    file_list = \"/content/data/test.bundle\"\n",
        "\n",
        "    list_of_videos = read_file(file_list).split('\\n')[:-1]\n",
        "    \n",
        "    overlap = [.1, .25, .5]\n",
        "    tp, fp, fn = np.zeros(3), np.zeros(3), np.zeros(3)\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    edit = 0\n",
        "    \n",
        "    for features, labels, masks in test_dataloader:\n",
        "        features, labels, masks = features.cuda(), labels.cuda(), masks.cuda()\n",
        "        gt_content,recog_content = get_results(model,features, labels, masks)\n",
        "        \n",
        "        for i in range(len(gt_content)):\n",
        "            total += 1\n",
        "            if gt_content[i] == recog_content[i]:\n",
        "                correct += 1\n",
        "        \n",
        "        edit += edit_score(recog_content, gt_content)\n",
        "\n",
        "        for s in range(len(overlap)):\n",
        "            tp1, fp1, fn1 = f_score(recog_content, gt_content, overlap[s])\n",
        "            tp[s] += tp1\n",
        "            fp[s] += fp1\n",
        "            fn[s] += fn1\n",
        "\n",
        "    print (\"Acc: %.4f\" % (100*float(correct)/total))\n",
        "    print ('Edit: %.4f' % ((1.0*edit)/len(list_of_videos)))\n",
        "\n",
        "    for s in range(len(overlap)):\n",
        "        precision = tp[s] / float(tp[s]+fp[s])\n",
        "        recall = tp[s] / float(tp[s]+fn[s])\n",
        "\n",
        "        f1 = 2.0 * (precision*recall) / (precision+recall)\n",
        "\n",
        "        f1 = np.nan_to_num(f1)*100\n",
        "        print ('F1@%0.2f: %.4f' % (overlap[s], f1))\n",
        "\n"
      ],
      "id": "dfezvc6K0Wmw",
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "928d8edc"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "\n",
        "    \n",
        "def train_parallel(model, dataloader,optimizer):\n",
        "    model.train()\n",
        "    i=0\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    running_loss = 0 \n",
        "    for features, labels, masks in dataloader:\n",
        "        features , labels , masks = features.cuda() , labels.cuda() , masks.cuda()\n",
        "        out1, out2, out3 ,out_average = model(features,masks)\n",
        "        optimizer.zero_grad()\n",
        "        loss1 = criterion(out1, labels)\n",
        "        loss2 = criterion(out2, labels)\n",
        "        loss3 = criterion(out3, labels)\n",
        "        loss_average = criterion(out_average, labels)\n",
        "        loss = loss1 + loss2 + loss3 + loss_average\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if i % 10 == 0:\n",
        "                print(\"    Batch {}: combined loss = {} , average loss: {}\".format(i ,loss.item(), loss.item()/4 ))\n",
        "        i += 1\n",
        "        running_loss = loss.item()\n",
        "    return running_loss / len(dataloader)\n",
        "\n",
        "\n",
        "\n",
        "# function for zero padding for dataloader because of variable video length\n",
        "# inspired by the code from the paper\n",
        "def collate_fn_padd(batch):\n",
        "        batch_input , batch_target = [list(t) for t in zip(*batch)] \n",
        "        length_of_sequences = list(map(len, batch_target))\n",
        "        batch_input_tensor = torch.zeros(len(batch_input), np.shape(batch_input[0])[0], max(length_of_sequences), dtype=torch.float)\n",
        "        \n",
        "        batch_target_tensor = torch.ones(len(batch_input), max(length_of_sequences), dtype=torch.long)*(-100)\n",
        "        \n",
        "        mask = torch.zeros(len(batch_input), num_classes, max(length_of_sequences), dtype=torch.float)\n",
        "        \n",
        "        for i in range(len(batch_input)):\n",
        "            batch_input_tensor[i, :, :np.shape(batch_input[i])[1]] = batch_input[i]\n",
        "            \n",
        "            batch_target_tensor[i, :np.shape(batch_target[i])[0]] = batch_target[i]\n",
        "            \n",
        "            mask[i, :, :np.shape(batch_target[i])[0]] = torch.ones(num_classes, batch_target[i].shape[0])\n",
        "            \n",
        "        return batch_input_tensor, batch_target_tensor, mask\n",
        "            \n",
        "            \n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 4\n",
        "epochs = 50\n",
        "num_classes = 48\n",
        "\n",
        "\n",
        "# DATA LOADERS \n",
        "\n",
        "training_dataset = TCNDataset(training=True)\n",
        "training_dataloader = torch.utils.data.DataLoader(training_dataset,collate_fn=collate_fn_padd,  batch_size=batch_size, shuffle=True, drop_last=False)"
      ],
      "id": "928d8edc",
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "23389ea0"
      },
      "source": [
        "parallel_TCNs = ParallelTCNs().cuda()\n",
        "parallel_TCNs_optimizer = torch.optim.Adam(parallel_TCNs.parameters(),lr=0.001)"
      ],
      "id": "23389ea0",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b12bbe4d",
        "outputId": "8d60bd86-dd8f-46ef-a3c0-6d2d566a605f"
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    print(\"RUNNING EPOCH: {}\".format(epoch+1))\n",
        "    train_parallel(parallel_TCNs,training_dataloader , parallel_TCNs_optimizer )\n",
        "\n",
        "    torch.save(parallel_TCNs, \"./parallel_model_after_epoch_{}\".format(epoch))"
      ],
      "id": "b12bbe4d",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RUNNING EPOCH: 1\n",
            "    Batch 0: combined loss = 15.486039161682129 , average loss: 3.8715097904205322\n",
            "    Batch 10: combined loss = 13.135170936584473 , average loss: 3.283792734146118\n",
            "    Batch 20: combined loss = 13.925765991210938 , average loss: 3.4814414978027344\n",
            "    Batch 30: combined loss = 11.123424530029297 , average loss: 2.780856132507324\n",
            "    Batch 40: combined loss = 9.680692672729492 , average loss: 2.420173168182373\n",
            "    Batch 50: combined loss = 10.125143051147461 , average loss: 2.5312857627868652\n",
            "    Batch 60: combined loss = 9.382369041442871 , average loss: 2.3455922603607178\n",
            "    Batch 70: combined loss = 9.931092262268066 , average loss: 2.4827730655670166\n",
            "    Batch 80: combined loss = 10.402716636657715 , average loss: 2.6006791591644287\n",
            "    Batch 90: combined loss = 13.472755432128906 , average loss: 3.3681888580322266\n",
            "    Batch 100: combined loss = 7.836481094360352 , average loss: 1.959120273590088\n",
            "    Batch 110: combined loss = 9.887517929077148 , average loss: 2.471879482269287\n",
            "    Batch 120: combined loss = 9.917524337768555 , average loss: 2.4793810844421387\n",
            "    Batch 130: combined loss = 11.589879989624023 , average loss: 2.897469997406006\n",
            "    Batch 140: combined loss = 11.544082641601562 , average loss: 2.8860206604003906\n",
            "    Batch 150: combined loss = 9.0580472946167 , average loss: 2.264511823654175\n",
            "    Batch 160: combined loss = 9.040922164916992 , average loss: 2.260230541229248\n",
            "    Batch 170: combined loss = 8.37173080444336 , average loss: 2.09293270111084\n",
            "    Batch 180: combined loss = 9.26034164428711 , average loss: 2.3150854110717773\n",
            "RUNNING EPOCH: 2\n",
            "    Batch 0: combined loss = 10.134706497192383 , average loss: 2.5336766242980957\n",
            "    Batch 10: combined loss = 7.675649642944336 , average loss: 1.918912410736084\n",
            "    Batch 20: combined loss = 8.120880126953125 , average loss: 2.0302200317382812\n",
            "    Batch 30: combined loss = 13.471691131591797 , average loss: 3.367922782897949\n",
            "    Batch 40: combined loss = 10.483202934265137 , average loss: 2.620800733566284\n",
            "    Batch 50: combined loss = 7.901316165924072 , average loss: 1.975329041481018\n",
            "    Batch 60: combined loss = 9.127805709838867 , average loss: 2.281951427459717\n",
            "    Batch 70: combined loss = 11.065455436706543 , average loss: 2.7663638591766357\n",
            "    Batch 80: combined loss = 8.68795108795166 , average loss: 2.171987771987915\n",
            "    Batch 90: combined loss = 8.738136291503906 , average loss: 2.1845340728759766\n",
            "    Batch 100: combined loss = 8.359853744506836 , average loss: 2.089963436126709\n",
            "    Batch 110: combined loss = 10.202065467834473 , average loss: 2.550516366958618\n",
            "    Batch 120: combined loss = 9.49848747253418 , average loss: 2.374621868133545\n",
            "    Batch 130: combined loss = 6.714226245880127 , average loss: 1.6785565614700317\n",
            "    Batch 140: combined loss = 10.473282814025879 , average loss: 2.6183207035064697\n",
            "    Batch 150: combined loss = 7.706886291503906 , average loss: 1.9267215728759766\n",
            "    Batch 160: combined loss = 9.755720138549805 , average loss: 2.438930034637451\n",
            "    Batch 170: combined loss = 8.412120819091797 , average loss: 2.103030204772949\n",
            "    Batch 180: combined loss = 8.06309986114502 , average loss: 2.015774965286255\n",
            "RUNNING EPOCH: 3\n",
            "    Batch 0: combined loss = 7.303478240966797 , average loss: 1.8258695602416992\n",
            "    Batch 10: combined loss = 9.281375885009766 , average loss: 2.3203439712524414\n",
            "    Batch 20: combined loss = 10.595502853393555 , average loss: 2.6488757133483887\n",
            "    Batch 30: combined loss = 10.819806098937988 , average loss: 2.704951524734497\n",
            "    Batch 40: combined loss = 8.169403076171875 , average loss: 2.0423507690429688\n",
            "    Batch 50: combined loss = 8.302742958068848 , average loss: 2.075685739517212\n",
            "    Batch 60: combined loss = 12.29723834991455 , average loss: 3.0743095874786377\n",
            "    Batch 70: combined loss = 8.27319622039795 , average loss: 2.0682990550994873\n",
            "    Batch 80: combined loss = 7.128294944763184 , average loss: 1.782073736190796\n",
            "    Batch 90: combined loss = 5.798421859741211 , average loss: 1.4496054649353027\n",
            "    Batch 100: combined loss = 11.064491271972656 , average loss: 2.766122817993164\n",
            "    Batch 110: combined loss = 10.800676345825195 , average loss: 2.700169086456299\n",
            "    Batch 120: combined loss = 6.283841133117676 , average loss: 1.570960283279419\n",
            "    Batch 130: combined loss = 11.288801193237305 , average loss: 2.822200298309326\n",
            "    Batch 140: combined loss = 8.141892433166504 , average loss: 2.035473108291626\n",
            "    Batch 150: combined loss = 8.219178199768066 , average loss: 2.0547945499420166\n",
            "    Batch 160: combined loss = 7.647719383239746 , average loss: 1.9119298458099365\n",
            "    Batch 170: combined loss = 8.722553253173828 , average loss: 2.180638313293457\n",
            "    Batch 180: combined loss = 7.308187007904053 , average loss: 1.8270467519760132\n",
            "RUNNING EPOCH: 4\n",
            "    Batch 0: combined loss = 6.553277015686035 , average loss: 1.6383192539215088\n",
            "    Batch 10: combined loss = 7.275603294372559 , average loss: 1.8189008235931396\n",
            "    Batch 20: combined loss = 7.389986991882324 , average loss: 1.847496747970581\n",
            "    Batch 30: combined loss = 5.875873565673828 , average loss: 1.468968391418457\n",
            "    Batch 40: combined loss = 9.616403579711914 , average loss: 2.4041008949279785\n",
            "    Batch 50: combined loss = 5.634004592895508 , average loss: 1.408501148223877\n",
            "    Batch 60: combined loss = 7.565847873687744 , average loss: 1.891461968421936\n",
            "    Batch 70: combined loss = 8.733278274536133 , average loss: 2.183319568634033\n",
            "    Batch 80: combined loss = 8.545306205749512 , average loss: 2.136326551437378\n",
            "    Batch 90: combined loss = 11.15042781829834 , average loss: 2.787606954574585\n",
            "    Batch 100: combined loss = 8.454648971557617 , average loss: 2.1136622428894043\n",
            "    Batch 110: combined loss = 8.4625825881958 , average loss: 2.11564564704895\n",
            "    Batch 120: combined loss = 9.377729415893555 , average loss: 2.3444323539733887\n",
            "    Batch 130: combined loss = 8.35529899597168 , average loss: 2.08882474899292\n",
            "    Batch 140: combined loss = 8.243560791015625 , average loss: 2.0608901977539062\n",
            "    Batch 150: combined loss = 6.35555362701416 , average loss: 1.58888840675354\n",
            "    Batch 160: combined loss = 11.16170883178711 , average loss: 2.7904272079467773\n",
            "    Batch 170: combined loss = 5.461225986480713 , average loss: 1.3653064966201782\n",
            "    Batch 180: combined loss = 10.390839576721191 , average loss: 2.597709894180298\n",
            "RUNNING EPOCH: 5\n",
            "    Batch 0: combined loss = 8.77901840209961 , average loss: 2.1947546005249023\n",
            "    Batch 10: combined loss = 8.612472534179688 , average loss: 2.153118133544922\n",
            "    Batch 20: combined loss = 9.638154983520508 , average loss: 2.409538745880127\n",
            "    Batch 30: combined loss = 5.648857116699219 , average loss: 1.4122142791748047\n",
            "    Batch 40: combined loss = 9.262834548950195 , average loss: 2.315708637237549\n",
            "    Batch 50: combined loss = 8.029460906982422 , average loss: 2.0073652267456055\n",
            "    Batch 60: combined loss = 9.02005672454834 , average loss: 2.255014181137085\n",
            "    Batch 70: combined loss = 5.649236679077148 , average loss: 1.412309169769287\n",
            "    Batch 80: combined loss = 4.526534557342529 , average loss: 1.1316336393356323\n",
            "    Batch 90: combined loss = 6.475541114807129 , average loss: 1.6188852787017822\n",
            "    Batch 100: combined loss = 7.967807769775391 , average loss: 1.9919519424438477\n",
            "    Batch 110: combined loss = 9.393526077270508 , average loss: 2.348381519317627\n",
            "    Batch 120: combined loss = 7.889619827270508 , average loss: 1.972404956817627\n",
            "    Batch 130: combined loss = 7.338216781616211 , average loss: 1.8345541954040527\n",
            "    Batch 140: combined loss = 10.880152702331543 , average loss: 2.7200381755828857\n",
            "    Batch 150: combined loss = 6.063255310058594 , average loss: 1.5158138275146484\n",
            "    Batch 160: combined loss = 7.451178073883057 , average loss: 1.8627945184707642\n",
            "    Batch 170: combined loss = 5.937507629394531 , average loss: 1.4843769073486328\n",
            "    Batch 180: combined loss = 7.035801887512207 , average loss: 1.7589504718780518\n",
            "RUNNING EPOCH: 6\n",
            "    Batch 0: combined loss = 8.879636764526367 , average loss: 2.219909191131592\n",
            "    Batch 10: combined loss = 7.833485126495361 , average loss: 1.9583712816238403\n",
            "    Batch 20: combined loss = 6.157149314880371 , average loss: 1.5392873287200928\n",
            "    Batch 30: combined loss = 6.738317012786865 , average loss: 1.6845792531967163\n",
            "    Batch 40: combined loss = 8.419511795043945 , average loss: 2.1048779487609863\n",
            "    Batch 50: combined loss = 6.455900192260742 , average loss: 1.6139750480651855\n",
            "    Batch 60: combined loss = 9.1903657913208 , average loss: 2.2975914478302\n",
            "    Batch 70: combined loss = 7.876452445983887 , average loss: 1.9691131114959717\n",
            "    Batch 80: combined loss = 9.383111000061035 , average loss: 2.345777750015259\n",
            "    Batch 90: combined loss = 6.4596848487854 , average loss: 1.61492121219635\n",
            "    Batch 100: combined loss = 6.782804012298584 , average loss: 1.695701003074646\n",
            "    Batch 110: combined loss = 11.00432014465332 , average loss: 2.75108003616333\n",
            "    Batch 120: combined loss = 8.682479858398438 , average loss: 2.1706199645996094\n",
            "    Batch 130: combined loss = 4.659071922302246 , average loss: 1.1647679805755615\n",
            "    Batch 140: combined loss = 7.1980695724487305 , average loss: 1.7995173931121826\n",
            "    Batch 150: combined loss = 6.471164703369141 , average loss: 1.6177911758422852\n",
            "    Batch 160: combined loss = 4.3758368492126465 , average loss: 1.0939592123031616\n",
            "    Batch 170: combined loss = 7.192093372344971 , average loss: 1.7980233430862427\n",
            "    Batch 180: combined loss = 7.4139204025268555 , average loss: 1.8534801006317139\n",
            "RUNNING EPOCH: 7\n",
            "    Batch 0: combined loss = 7.6307830810546875 , average loss: 1.9076957702636719\n",
            "    Batch 10: combined loss = 9.153358459472656 , average loss: 2.288339614868164\n",
            "    Batch 20: combined loss = 4.733895778656006 , average loss: 1.1834739446640015\n",
            "    Batch 30: combined loss = 5.106332778930664 , average loss: 1.276583194732666\n",
            "    Batch 40: combined loss = 7.58111572265625 , average loss: 1.8952789306640625\n",
            "    Batch 50: combined loss = 6.615516662597656 , average loss: 1.653879165649414\n",
            "    Batch 60: combined loss = 7.727808475494385 , average loss: 1.9319521188735962\n",
            "    Batch 70: combined loss = 6.057381629943848 , average loss: 1.514345407485962\n",
            "    Batch 80: combined loss = 7.922591209411621 , average loss: 1.9806478023529053\n",
            "    Batch 90: combined loss = 7.513663291931152 , average loss: 1.878415822982788\n",
            "    Batch 100: combined loss = 5.972851753234863 , average loss: 1.4932129383087158\n",
            "    Batch 110: combined loss = 7.859966278076172 , average loss: 1.964991569519043\n",
            "    Batch 120: combined loss = 5.669066905975342 , average loss: 1.4172667264938354\n",
            "    Batch 130: combined loss = 6.5331292152404785 , average loss: 1.6332823038101196\n",
            "    Batch 140: combined loss = 6.794168472290039 , average loss: 1.6985421180725098\n",
            "    Batch 150: combined loss = 6.9178266525268555 , average loss: 1.7294566631317139\n",
            "    Batch 160: combined loss = 10.368865966796875 , average loss: 2.5922164916992188\n",
            "    Batch 170: combined loss = 8.147248268127441 , average loss: 2.0368120670318604\n",
            "    Batch 180: combined loss = 9.084723472595215 , average loss: 2.2711808681488037\n",
            "RUNNING EPOCH: 8\n",
            "    Batch 0: combined loss = 4.9951653480529785 , average loss: 1.2487913370132446\n",
            "    Batch 10: combined loss = 5.796969413757324 , average loss: 1.449242353439331\n",
            "    Batch 20: combined loss = 8.722294807434082 , average loss: 2.1805737018585205\n",
            "    Batch 30: combined loss = 7.748347282409668 , average loss: 1.937086820602417\n",
            "    Batch 40: combined loss = 3.545605421066284 , average loss: 0.886401355266571\n",
            "    Batch 50: combined loss = 6.306870460510254 , average loss: 1.5767176151275635\n",
            "    Batch 60: combined loss = 6.362625598907471 , average loss: 1.5906563997268677\n",
            "    Batch 70: combined loss = 7.029375076293945 , average loss: 1.7573437690734863\n",
            "    Batch 80: combined loss = 6.768882751464844 , average loss: 1.692220687866211\n",
            "    Batch 90: combined loss = 8.14311695098877 , average loss: 2.0357792377471924\n",
            "    Batch 100: combined loss = 6.20559024810791 , average loss: 1.5513975620269775\n",
            "    Batch 110: combined loss = 5.221381187438965 , average loss: 1.3053452968597412\n",
            "    Batch 120: combined loss = 7.230044841766357 , average loss: 1.8075112104415894\n",
            "    Batch 130: combined loss = 6.9515380859375 , average loss: 1.737884521484375\n",
            "    Batch 140: combined loss = 6.701226234436035 , average loss: 1.6753065586090088\n",
            "    Batch 150: combined loss = 8.34362506866455 , average loss: 2.0859062671661377\n",
            "    Batch 160: combined loss = 7.187769412994385 , average loss: 1.7969423532485962\n",
            "    Batch 170: combined loss = 5.36993932723999 , average loss: 1.3424848318099976\n",
            "    Batch 180: combined loss = 6.613970756530762 , average loss: 1.6534926891326904\n",
            "RUNNING EPOCH: 9\n",
            "    Batch 0: combined loss = 5.257228851318359 , average loss: 1.3143072128295898\n",
            "    Batch 10: combined loss = 6.700856685638428 , average loss: 1.675214171409607\n",
            "    Batch 20: combined loss = 5.927654266357422 , average loss: 1.4819135665893555\n",
            "    Batch 30: combined loss = 7.276341438293457 , average loss: 1.8190853595733643\n",
            "    Batch 40: combined loss = 6.182435989379883 , average loss: 1.5456089973449707\n",
            "    Batch 50: combined loss = 6.474228382110596 , average loss: 1.618557095527649\n",
            "    Batch 60: combined loss = 5.249129772186279 , average loss: 1.3122824430465698\n",
            "    Batch 70: combined loss = 8.24111270904541 , average loss: 2.0602781772613525\n",
            "    Batch 80: combined loss = 6.914001941680908 , average loss: 1.728500485420227\n",
            "    Batch 90: combined loss = 7.180967807769775 , average loss: 1.7952419519424438\n",
            "    Batch 100: combined loss = 5.647279739379883 , average loss: 1.4118199348449707\n",
            "    Batch 110: combined loss = 6.619711875915527 , average loss: 1.6549279689788818\n",
            "    Batch 120: combined loss = 6.4259257316589355 , average loss: 1.6064814329147339\n",
            "    Batch 130: combined loss = 4.589118003845215 , average loss: 1.1472795009613037\n",
            "    Batch 140: combined loss = 7.299246788024902 , average loss: 1.8248116970062256\n",
            "    Batch 150: combined loss = 5.872879981994629 , average loss: 1.4682199954986572\n",
            "    Batch 160: combined loss = 5.931478500366211 , average loss: 1.4828696250915527\n",
            "    Batch 170: combined loss = 4.269180774688721 , average loss: 1.0672951936721802\n",
            "    Batch 180: combined loss = 9.9309720993042 , average loss: 2.48274302482605\n",
            "RUNNING EPOCH: 10\n",
            "    Batch 0: combined loss = 4.187285423278809 , average loss: 1.0468213558197021\n",
            "    Batch 10: combined loss = 8.652603149414062 , average loss: 2.1631507873535156\n",
            "    Batch 20: combined loss = 7.174957752227783 , average loss: 1.7937394380569458\n",
            "    Batch 30: combined loss = 6.249435901641846 , average loss: 1.5623589754104614\n",
            "    Batch 40: combined loss = 9.902974128723145 , average loss: 2.475743532180786\n",
            "    Batch 50: combined loss = 8.564825057983398 , average loss: 2.1412062644958496\n",
            "    Batch 60: combined loss = 5.301464080810547 , average loss: 1.3253660202026367\n",
            "    Batch 70: combined loss = 5.789398193359375 , average loss: 1.4473495483398438\n",
            "    Batch 80: combined loss = 4.420287132263184 , average loss: 1.105071783065796\n",
            "    Batch 90: combined loss = 6.164059638977051 , average loss: 1.5410149097442627\n",
            "    Batch 100: combined loss = 5.349766254425049 , average loss: 1.3374415636062622\n",
            "    Batch 110: combined loss = 4.753793239593506 , average loss: 1.1884483098983765\n",
            "    Batch 120: combined loss = 8.204174995422363 , average loss: 2.051043748855591\n",
            "    Batch 130: combined loss = 8.038934707641602 , average loss: 2.0097336769104004\n",
            "    Batch 140: combined loss = 6.549399375915527 , average loss: 1.6373498439788818\n",
            "    Batch 150: combined loss = 7.187042713165283 , average loss: 1.7967606782913208\n",
            "    Batch 160: combined loss = 4.220158576965332 , average loss: 1.055039644241333\n",
            "    Batch 170: combined loss = 7.304625034332275 , average loss: 1.8261562585830688\n",
            "    Batch 180: combined loss = 6.190572738647461 , average loss: 1.5476431846618652\n",
            "RUNNING EPOCH: 11\n",
            "    Batch 0: combined loss = 6.132704734802246 , average loss: 1.5331761837005615\n",
            "    Batch 10: combined loss = 4.342892646789551 , average loss: 1.0857231616973877\n",
            "    Batch 20: combined loss = 6.348202228546143 , average loss: 1.5870505571365356\n",
            "    Batch 30: combined loss = 5.953167915344238 , average loss: 1.4882919788360596\n",
            "    Batch 40: combined loss = 8.014132499694824 , average loss: 2.003533124923706\n",
            "    Batch 50: combined loss = 4.331465721130371 , average loss: 1.0828664302825928\n",
            "    Batch 60: combined loss = 4.706547737121582 , average loss: 1.1766369342803955\n",
            "    Batch 70: combined loss = 7.793104648590088 , average loss: 1.948276162147522\n",
            "    Batch 80: combined loss = 7.527131080627441 , average loss: 1.8817827701568604\n",
            "    Batch 90: combined loss = 5.787361145019531 , average loss: 1.4468402862548828\n",
            "    Batch 100: combined loss = 4.843197822570801 , average loss: 1.2107994556427002\n",
            "    Batch 110: combined loss = 5.8229851722717285 , average loss: 1.4557462930679321\n",
            "    Batch 120: combined loss = 5.323366641998291 , average loss: 1.3308416604995728\n",
            "    Batch 130: combined loss = 5.0092363357543945 , average loss: 1.2523090839385986\n",
            "    Batch 140: combined loss = 5.505112648010254 , average loss: 1.3762781620025635\n",
            "    Batch 150: combined loss = 5.829198837280273 , average loss: 1.4572997093200684\n",
            "    Batch 160: combined loss = 6.010359764099121 , average loss: 1.5025899410247803\n",
            "    Batch 170: combined loss = 5.07284688949585 , average loss: 1.2682117223739624\n",
            "    Batch 180: combined loss = 8.80458927154541 , average loss: 2.2011473178863525\n",
            "RUNNING EPOCH: 12\n",
            "    Batch 0: combined loss = 3.846229076385498 , average loss: 0.9615572690963745\n",
            "    Batch 10: combined loss = 6.455718994140625 , average loss: 1.6139297485351562\n",
            "    Batch 20: combined loss = 5.897755146026611 , average loss: 1.4744387865066528\n",
            "    Batch 30: combined loss = 9.749466896057129 , average loss: 2.4373667240142822\n",
            "    Batch 40: combined loss = 4.925869464874268 , average loss: 1.231467366218567\n",
            "    Batch 50: combined loss = 5.0723490715026855 , average loss: 1.2680872678756714\n",
            "    Batch 60: combined loss = 7.936244964599609 , average loss: 1.9840612411499023\n",
            "    Batch 70: combined loss = 8.045614242553711 , average loss: 2.0114035606384277\n",
            "    Batch 80: combined loss = 5.298714637756348 , average loss: 1.324678659439087\n",
            "    Batch 90: combined loss = 6.340062141418457 , average loss: 1.5850155353546143\n",
            "    Batch 100: combined loss = 6.833035469055176 , average loss: 1.708258867263794\n",
            "    Batch 110: combined loss = 13.473663330078125 , average loss: 3.3684158325195312\n",
            "    Batch 120: combined loss = 7.8380866050720215 , average loss: 1.9595216512680054\n",
            "    Batch 130: combined loss = 7.728578567504883 , average loss: 1.9321446418762207\n",
            "    Batch 140: combined loss = 6.456477642059326 , average loss: 1.6141194105148315\n",
            "    Batch 150: combined loss = 6.4384446144104 , average loss: 1.6096111536026\n",
            "    Batch 160: combined loss = 10.358625411987305 , average loss: 2.589656352996826\n",
            "    Batch 170: combined loss = 5.50631046295166 , average loss: 1.376577615737915\n",
            "    Batch 180: combined loss = 5.13016939163208 , average loss: 1.28254234790802\n",
            "RUNNING EPOCH: 13\n",
            "    Batch 0: combined loss = 7.91363525390625 , average loss: 1.9784088134765625\n",
            "    Batch 10: combined loss = 5.838768482208252 , average loss: 1.459692120552063\n",
            "    Batch 20: combined loss = 5.466469764709473 , average loss: 1.3666174411773682\n",
            "    Batch 30: combined loss = 5.967618465423584 , average loss: 1.491904616355896\n",
            "    Batch 40: combined loss = 5.729047775268555 , average loss: 1.4322619438171387\n",
            "    Batch 50: combined loss = 5.3841753005981445 , average loss: 1.3460438251495361\n",
            "    Batch 60: combined loss = 5.584107398986816 , average loss: 1.396026849746704\n",
            "    Batch 70: combined loss = 6.553563117980957 , average loss: 1.6383907794952393\n",
            "    Batch 80: combined loss = 6.55946683883667 , average loss: 1.6398667097091675\n",
            "    Batch 90: combined loss = 4.159134864807129 , average loss: 1.0397837162017822\n",
            "    Batch 100: combined loss = 5.373012065887451 , average loss: 1.3432530164718628\n",
            "    Batch 110: combined loss = 8.10953140258789 , average loss: 2.0273828506469727\n",
            "    Batch 120: combined loss = 5.902485370635986 , average loss: 1.4756213426589966\n",
            "    Batch 130: combined loss = 9.094366073608398 , average loss: 2.2735915184020996\n",
            "    Batch 140: combined loss = 4.9194254875183105 , average loss: 1.2298563718795776\n",
            "    Batch 150: combined loss = 7.906922340393066 , average loss: 1.9767305850982666\n",
            "    Batch 160: combined loss = 4.766618728637695 , average loss: 1.1916546821594238\n",
            "    Batch 170: combined loss = 4.182507514953613 , average loss: 1.0456268787384033\n",
            "    Batch 180: combined loss = 6.824526309967041 , average loss: 1.7061315774917603\n",
            "RUNNING EPOCH: 14\n",
            "    Batch 0: combined loss = 4.867564678192139 , average loss: 1.2168911695480347\n",
            "    Batch 10: combined loss = 6.801001071929932 , average loss: 1.700250267982483\n",
            "    Batch 20: combined loss = 8.044855117797852 , average loss: 2.011213779449463\n",
            "    Batch 30: combined loss = 5.537327766418457 , average loss: 1.3843319416046143\n",
            "    Batch 40: combined loss = 4.700201034545898 , average loss: 1.1750502586364746\n",
            "    Batch 50: combined loss = 4.232776165008545 , average loss: 1.0581940412521362\n",
            "    Batch 60: combined loss = 5.5361433029174805 , average loss: 1.3840358257293701\n",
            "    Batch 70: combined loss = 5.23496150970459 , average loss: 1.3087403774261475\n",
            "    Batch 80: combined loss = 5.493889331817627 , average loss: 1.3734723329544067\n",
            "    Batch 90: combined loss = 4.708902359008789 , average loss: 1.1772255897521973\n",
            "    Batch 100: combined loss = 4.213221073150635 , average loss: 1.0533052682876587\n",
            "    Batch 110: combined loss = 4.472616195678711 , average loss: 1.1181540489196777\n",
            "    Batch 120: combined loss = 6.674611568450928 , average loss: 1.668652892112732\n",
            "    Batch 130: combined loss = 6.598202705383301 , average loss: 1.6495506763458252\n",
            "    Batch 140: combined loss = 5.167749881744385 , average loss: 1.2919374704360962\n",
            "    Batch 150: combined loss = 6.9905009269714355 , average loss: 1.7476252317428589\n",
            "    Batch 160: combined loss = 4.839422225952148 , average loss: 1.209855556488037\n",
            "    Batch 170: combined loss = 2.5231306552886963 , average loss: 0.6307826638221741\n",
            "    Batch 180: combined loss = 5.52035665512085 , average loss: 1.3800891637802124\n",
            "RUNNING EPOCH: 15\n",
            "    Batch 0: combined loss = 4.719498634338379 , average loss: 1.1798746585845947\n",
            "    Batch 10: combined loss = 5.054769515991211 , average loss: 1.2636923789978027\n",
            "    Batch 20: combined loss = 6.065192699432373 , average loss: 1.5162981748580933\n",
            "    Batch 30: combined loss = 4.109755039215088 , average loss: 1.027438759803772\n",
            "    Batch 40: combined loss = 7.080239295959473 , average loss: 1.7700598239898682\n",
            "    Batch 50: combined loss = 4.569162845611572 , average loss: 1.142290711402893\n",
            "    Batch 60: combined loss = 5.565594673156738 , average loss: 1.3913986682891846\n",
            "    Batch 70: combined loss = 3.719107151031494 , average loss: 0.9297767877578735\n",
            "    Batch 80: combined loss = 7.320821285247803 , average loss: 1.8302053213119507\n",
            "    Batch 90: combined loss = 5.110183238983154 , average loss: 1.2775458097457886\n",
            "    Batch 100: combined loss = 5.782348155975342 , average loss: 1.4455870389938354\n",
            "    Batch 110: combined loss = 4.944818019866943 , average loss: 1.2362045049667358\n",
            "    Batch 120: combined loss = 3.3174924850463867 , average loss: 0.8293731212615967\n",
            "    Batch 130: combined loss = 9.739347457885742 , average loss: 2.4348368644714355\n",
            "    Batch 140: combined loss = 4.408799648284912 , average loss: 1.102199912071228\n",
            "    Batch 150: combined loss = 7.542953014373779 , average loss: 1.8857382535934448\n",
            "    Batch 160: combined loss = 7.022631645202637 , average loss: 1.7556579113006592\n",
            "    Batch 170: combined loss = 4.038080215454102 , average loss: 1.0095200538635254\n",
            "    Batch 180: combined loss = 7.309174060821533 , average loss: 1.8272935152053833\n",
            "RUNNING EPOCH: 16\n",
            "    Batch 0: combined loss = 8.156304359436035 , average loss: 2.039076089859009\n",
            "    Batch 10: combined loss = 4.901854515075684 , average loss: 1.225463628768921\n",
            "    Batch 20: combined loss = 6.138612747192383 , average loss: 1.5346531867980957\n",
            "    Batch 30: combined loss = 7.52537202835083 , average loss: 1.8813430070877075\n",
            "    Batch 40: combined loss = 4.629947185516357 , average loss: 1.1574867963790894\n",
            "    Batch 50: combined loss = 5.379217624664307 , average loss: 1.3448044061660767\n",
            "    Batch 60: combined loss = 4.088464736938477 , average loss: 1.0221161842346191\n",
            "    Batch 70: combined loss = 7.267852783203125 , average loss: 1.8169631958007812\n",
            "    Batch 80: combined loss = 5.230854034423828 , average loss: 1.307713508605957\n",
            "    Batch 90: combined loss = 9.12240219116211 , average loss: 2.2806005477905273\n",
            "    Batch 100: combined loss = 5.509214401245117 , average loss: 1.3773036003112793\n",
            "    Batch 110: combined loss = 4.772092819213867 , average loss: 1.1930232048034668\n",
            "    Batch 120: combined loss = 6.19609260559082 , average loss: 1.549023151397705\n",
            "    Batch 130: combined loss = 4.699237823486328 , average loss: 1.174809455871582\n",
            "    Batch 140: combined loss = 3.9476239681243896 , average loss: 0.9869059920310974\n",
            "    Batch 150: combined loss = 7.237712860107422 , average loss: 1.8094282150268555\n",
            "    Batch 160: combined loss = 6.854215145111084 , average loss: 1.713553786277771\n",
            "    Batch 170: combined loss = 4.548638820648193 , average loss: 1.1371597051620483\n",
            "    Batch 180: combined loss = 8.80072021484375 , average loss: 2.2001800537109375\n",
            "RUNNING EPOCH: 17\n",
            "    Batch 0: combined loss = 5.187836647033691 , average loss: 1.2969591617584229\n",
            "    Batch 10: combined loss = 6.138114929199219 , average loss: 1.5345287322998047\n",
            "    Batch 20: combined loss = 3.109099864959717 , average loss: 0.7772749662399292\n",
            "    Batch 30: combined loss = 8.125773429870605 , average loss: 2.0314433574676514\n",
            "    Batch 40: combined loss = 6.404637336730957 , average loss: 1.6011593341827393\n",
            "    Batch 50: combined loss = 4.069797992706299 , average loss: 1.0174494981765747\n",
            "    Batch 60: combined loss = 5.086808681488037 , average loss: 1.2717021703720093\n",
            "    Batch 70: combined loss = 5.045711040496826 , average loss: 1.2614277601242065\n",
            "    Batch 80: combined loss = 5.067267417907715 , average loss: 1.2668168544769287\n",
            "    Batch 90: combined loss = 5.534153938293457 , average loss: 1.3835384845733643\n",
            "    Batch 100: combined loss = 5.11181640625 , average loss: 1.2779541015625\n",
            "    Batch 110: combined loss = 6.841256618499756 , average loss: 1.710314154624939\n",
            "    Batch 120: combined loss = 6.891501426696777 , average loss: 1.7228753566741943\n",
            "    Batch 130: combined loss = 5.360971450805664 , average loss: 1.340242862701416\n",
            "    Batch 140: combined loss = 4.373727321624756 , average loss: 1.093431830406189\n",
            "    Batch 150: combined loss = 5.329963684082031 , average loss: 1.3324909210205078\n",
            "    Batch 160: combined loss = 4.5062408447265625 , average loss: 1.1265602111816406\n",
            "    Batch 170: combined loss = 5.019987106323242 , average loss: 1.2549967765808105\n",
            "    Batch 180: combined loss = 6.752967834472656 , average loss: 1.688241958618164\n",
            "RUNNING EPOCH: 18\n",
            "    Batch 0: combined loss = 5.242352485656738 , average loss: 1.3105881214141846\n",
            "    Batch 10: combined loss = 6.356594085693359 , average loss: 1.5891485214233398\n",
            "    Batch 20: combined loss = 3.4658870697021484 , average loss: 0.8664717674255371\n",
            "    Batch 30: combined loss = 5.8439202308654785 , average loss: 1.4609800577163696\n",
            "    Batch 40: combined loss = 4.879412651062012 , average loss: 1.219853162765503\n",
            "    Batch 50: combined loss = 5.252348899841309 , average loss: 1.3130872249603271\n",
            "    Batch 60: combined loss = 5.695867538452148 , average loss: 1.423966884613037\n",
            "    Batch 70: combined loss = 6.991128921508789 , average loss: 1.7477822303771973\n",
            "    Batch 80: combined loss = 4.676151752471924 , average loss: 1.169037938117981\n",
            "    Batch 90: combined loss = 5.634146690368652 , average loss: 1.408536672592163\n",
            "    Batch 100: combined loss = 5.699347496032715 , average loss: 1.4248368740081787\n",
            "    Batch 110: combined loss = 4.302611351013184 , average loss: 1.075652837753296\n",
            "    Batch 120: combined loss = 3.562469005584717 , average loss: 0.8906172513961792\n",
            "    Batch 130: combined loss = 4.7544264793396 , average loss: 1.1886066198349\n",
            "    Batch 140: combined loss = 5.208676815032959 , average loss: 1.3021692037582397\n",
            "    Batch 150: combined loss = 6.119591236114502 , average loss: 1.5298978090286255\n",
            "    Batch 160: combined loss = 6.660623073577881 , average loss: 1.6651557683944702\n",
            "    Batch 170: combined loss = 6.396697044372559 , average loss: 1.5991742610931396\n",
            "    Batch 180: combined loss = 6.772168159484863 , average loss: 1.6930420398712158\n",
            "RUNNING EPOCH: 19\n",
            "    Batch 0: combined loss = 5.729949474334717 , average loss: 1.4324873685836792\n",
            "    Batch 10: combined loss = 4.384251594543457 , average loss: 1.0960628986358643\n",
            "    Batch 20: combined loss = 4.768590450286865 , average loss: 1.1921476125717163\n",
            "    Batch 30: combined loss = 6.077634811401367 , average loss: 1.5194087028503418\n",
            "    Batch 40: combined loss = 5.309135437011719 , average loss: 1.3272838592529297\n",
            "    Batch 50: combined loss = 5.9711503982543945 , average loss: 1.4927875995635986\n",
            "    Batch 60: combined loss = 6.592081069946289 , average loss: 1.6480202674865723\n",
            "    Batch 70: combined loss = 5.904047012329102 , average loss: 1.4760117530822754\n",
            "    Batch 80: combined loss = 3.7273268699645996 , average loss: 0.9318317174911499\n",
            "    Batch 90: combined loss = 4.38581657409668 , average loss: 1.09645414352417\n",
            "    Batch 100: combined loss = 5.950984001159668 , average loss: 1.487746000289917\n",
            "    Batch 110: combined loss = 6.430389881134033 , average loss: 1.6075974702835083\n",
            "    Batch 120: combined loss = 6.523471832275391 , average loss: 1.6308679580688477\n",
            "    Batch 130: combined loss = 4.136315822601318 , average loss: 1.0340789556503296\n",
            "    Batch 140: combined loss = 4.255648612976074 , average loss: 1.0639121532440186\n",
            "    Batch 150: combined loss = 8.370473861694336 , average loss: 2.092618465423584\n",
            "    Batch 160: combined loss = 4.5058369636535645 , average loss: 1.1264592409133911\n",
            "    Batch 170: combined loss = 5.008516788482666 , average loss: 1.2521291971206665\n",
            "    Batch 180: combined loss = 7.339707851409912 , average loss: 1.834926962852478\n",
            "RUNNING EPOCH: 20\n",
            "    Batch 0: combined loss = 5.5569047927856445 , average loss: 1.3892261981964111\n",
            "    Batch 10: combined loss = 6.020219802856445 , average loss: 1.5050549507141113\n",
            "    Batch 20: combined loss = 6.361915588378906 , average loss: 1.5904788970947266\n",
            "    Batch 30: combined loss = 3.661583423614502 , average loss: 0.9153958559036255\n",
            "    Batch 40: combined loss = 5.514266014099121 , average loss: 1.3785665035247803\n",
            "    Batch 50: combined loss = 4.5412139892578125 , average loss: 1.1353034973144531\n",
            "    Batch 60: combined loss = 4.293211460113525 , average loss: 1.0733028650283813\n",
            "    Batch 70: combined loss = 3.2203526496887207 , average loss: 0.8050881624221802\n",
            "    Batch 80: combined loss = 4.379812717437744 , average loss: 1.094953179359436\n",
            "    Batch 90: combined loss = 8.195512771606445 , average loss: 2.0488781929016113\n",
            "    Batch 100: combined loss = 5.630175590515137 , average loss: 1.4075438976287842\n",
            "    Batch 110: combined loss = 4.246353626251221 , average loss: 1.0615884065628052\n",
            "    Batch 120: combined loss = 3.8155014514923096 , average loss: 0.9538753628730774\n",
            "    Batch 130: combined loss = 3.5967702865600586 , average loss: 0.8991925716400146\n",
            "    Batch 140: combined loss = 3.8702189922332764 , average loss: 0.9675547480583191\n",
            "    Batch 150: combined loss = 5.689342975616455 , average loss: 1.4223357439041138\n",
            "    Batch 160: combined loss = 4.937570571899414 , average loss: 1.2343926429748535\n",
            "    Batch 170: combined loss = 4.859795093536377 , average loss: 1.2149487733840942\n",
            "    Batch 180: combined loss = 4.822641372680664 , average loss: 1.205660343170166\n",
            "RUNNING EPOCH: 21\n",
            "    Batch 0: combined loss = 7.405463218688965 , average loss: 1.8513658046722412\n",
            "    Batch 10: combined loss = 4.648715019226074 , average loss: 1.1621787548065186\n",
            "    Batch 20: combined loss = 6.463499546051025 , average loss: 1.6158748865127563\n",
            "    Batch 30: combined loss = 4.832938194274902 , average loss: 1.2082345485687256\n",
            "    Batch 40: combined loss = 5.057683944702148 , average loss: 1.264420986175537\n",
            "    Batch 50: combined loss = 4.753689289093018 , average loss: 1.1884223222732544\n",
            "    Batch 60: combined loss = 6.089914321899414 , average loss: 1.5224785804748535\n",
            "    Batch 70: combined loss = 4.231257915496826 , average loss: 1.0578144788742065\n",
            "    Batch 80: combined loss = 3.659083366394043 , average loss: 0.9147708415985107\n",
            "    Batch 90: combined loss = 4.498426914215088 , average loss: 1.124606728553772\n",
            "    Batch 100: combined loss = 2.7506113052368164 , average loss: 0.6876528263092041\n",
            "    Batch 110: combined loss = 6.4102044105529785 , average loss: 1.6025511026382446\n",
            "    Batch 120: combined loss = 3.9420318603515625 , average loss: 0.9855079650878906\n",
            "    Batch 130: combined loss = 8.255509376525879 , average loss: 2.0638773441314697\n",
            "    Batch 140: combined loss = 5.361485481262207 , average loss: 1.3403713703155518\n",
            "    Batch 150: combined loss = 5.982759475708008 , average loss: 1.495689868927002\n",
            "    Batch 160: combined loss = 2.7994699478149414 , average loss: 0.6998674869537354\n",
            "    Batch 170: combined loss = 4.594000816345215 , average loss: 1.1485002040863037\n",
            "    Batch 180: combined loss = 6.849615097045898 , average loss: 1.7124037742614746\n",
            "RUNNING EPOCH: 22\n",
            "    Batch 0: combined loss = 5.530284404754639 , average loss: 1.3825711011886597\n",
            "    Batch 10: combined loss = 5.355161190032959 , average loss: 1.3387902975082397\n",
            "    Batch 20: combined loss = 3.8059017658233643 , average loss: 0.9514754414558411\n",
            "    Batch 30: combined loss = 5.90936279296875 , average loss: 1.4773406982421875\n",
            "    Batch 40: combined loss = 6.273682594299316 , average loss: 1.568420648574829\n",
            "    Batch 50: combined loss = 5.363019943237305 , average loss: 1.3407549858093262\n",
            "    Batch 60: combined loss = 5.193633079528809 , average loss: 1.2984082698822021\n",
            "    Batch 70: combined loss = 3.009777545928955 , average loss: 0.7524443864822388\n",
            "    Batch 80: combined loss = 6.429971694946289 , average loss: 1.6074929237365723\n",
            "    Batch 90: combined loss = 4.8979268074035645 , average loss: 1.2244817018508911\n",
            "    Batch 100: combined loss = 5.142267227172852 , average loss: 1.285566806793213\n",
            "    Batch 110: combined loss = 4.5337605476379395 , average loss: 1.1334401369094849\n",
            "    Batch 120: combined loss = 2.8984851837158203 , average loss: 0.7246212959289551\n",
            "    Batch 130: combined loss = 5.011733531951904 , average loss: 1.252933382987976\n",
            "    Batch 140: combined loss = 3.9746718406677246 , average loss: 0.9936679601669312\n",
            "    Batch 150: combined loss = 4.892051696777344 , average loss: 1.223012924194336\n",
            "    Batch 160: combined loss = 3.430807590484619 , average loss: 0.8577018976211548\n",
            "    Batch 170: combined loss = 4.1830830574035645 , average loss: 1.0457707643508911\n",
            "    Batch 180: combined loss = 5.084948539733887 , average loss: 1.2712371349334717\n",
            "RUNNING EPOCH: 23\n",
            "    Batch 0: combined loss = 6.890861511230469 , average loss: 1.7227153778076172\n",
            "    Batch 10: combined loss = 3.8845810890197754 , average loss: 0.9711452722549438\n",
            "    Batch 20: combined loss = 5.563884258270264 , average loss: 1.390971064567566\n",
            "    Batch 30: combined loss = 4.799619674682617 , average loss: 1.1999049186706543\n",
            "    Batch 40: combined loss = 4.850898742675781 , average loss: 1.2127246856689453\n",
            "    Batch 50: combined loss = 3.0540871620178223 , average loss: 0.7635217905044556\n",
            "    Batch 60: combined loss = 3.0381922721862793 , average loss: 0.7595480680465698\n",
            "    Batch 70: combined loss = 4.928469181060791 , average loss: 1.2321172952651978\n",
            "    Batch 80: combined loss = 5.859339237213135 , average loss: 1.4648348093032837\n",
            "    Batch 90: combined loss = 2.9103827476501465 , average loss: 0.7275956869125366\n",
            "    Batch 100: combined loss = 5.712102890014648 , average loss: 1.428025722503662\n",
            "    Batch 110: combined loss = 4.226809024810791 , average loss: 1.0567022562026978\n",
            "    Batch 120: combined loss = 4.549237251281738 , average loss: 1.1373093128204346\n",
            "    Batch 130: combined loss = 4.08729362487793 , average loss: 1.0218234062194824\n",
            "    Batch 140: combined loss = 4.289863586425781 , average loss: 1.0724658966064453\n",
            "    Batch 150: combined loss = 3.662627935409546 , average loss: 0.9156569838523865\n",
            "    Batch 160: combined loss = 5.1907501220703125 , average loss: 1.2976875305175781\n",
            "    Batch 170: combined loss = 3.5280277729034424 , average loss: 0.8820069432258606\n",
            "    Batch 180: combined loss = 4.082831859588623 , average loss: 1.0207079648971558\n",
            "RUNNING EPOCH: 24\n",
            "    Batch 0: combined loss = 5.8453192710876465 , average loss: 1.4613298177719116\n",
            "    Batch 10: combined loss = 2.9516196250915527 , average loss: 0.7379049062728882\n",
            "    Batch 20: combined loss = 3.2321832180023193 , average loss: 0.8080458045005798\n",
            "    Batch 30: combined loss = 3.471137762069702 , average loss: 0.8677844405174255\n",
            "    Batch 40: combined loss = 4.44860315322876 , average loss: 1.11215078830719\n",
            "    Batch 50: combined loss = 2.7934160232543945 , average loss: 0.6983540058135986\n",
            "    Batch 60: combined loss = 3.820711612701416 , average loss: 0.955177903175354\n",
            "    Batch 70: combined loss = 3.7895491123199463 , average loss: 0.9473872780799866\n",
            "    Batch 80: combined loss = 4.877325057983398 , average loss: 1.2193312644958496\n",
            "    Batch 90: combined loss = 5.516463279724121 , average loss: 1.3791158199310303\n",
            "    Batch 100: combined loss = 5.037256240844727 , average loss: 1.2593140602111816\n",
            "    Batch 110: combined loss = 6.340653419494629 , average loss: 1.5851633548736572\n",
            "    Batch 120: combined loss = 3.177050828933716 , average loss: 0.794262707233429\n",
            "    Batch 130: combined loss = 4.836061000823975 , average loss: 1.2090152502059937\n",
            "    Batch 140: combined loss = 3.1967921257019043 , average loss: 0.7991980314254761\n",
            "    Batch 150: combined loss = 4.927488803863525 , average loss: 1.2318722009658813\n",
            "    Batch 160: combined loss = 3.600851058959961 , average loss: 0.9002127647399902\n",
            "    Batch 170: combined loss = 6.415879726409912 , average loss: 1.603969931602478\n",
            "    Batch 180: combined loss = 3.406432628631592 , average loss: 0.851608157157898\n",
            "RUNNING EPOCH: 25\n",
            "    Batch 0: combined loss = 5.252627372741699 , average loss: 1.3131568431854248\n",
            "    Batch 10: combined loss = 4.538763046264648 , average loss: 1.134690761566162\n",
            "    Batch 20: combined loss = 4.357774257659912 , average loss: 1.089443564414978\n",
            "    Batch 30: combined loss = 3.445981979370117 , average loss: 0.8614954948425293\n",
            "    Batch 40: combined loss = 4.39037561416626 , average loss: 1.097593903541565\n",
            "    Batch 50: combined loss = 3.2474265098571777 , average loss: 0.8118566274642944\n",
            "    Batch 60: combined loss = 5.299924850463867 , average loss: 1.3249812126159668\n",
            "    Batch 70: combined loss = 5.5643839836120605 , average loss: 1.3910959959030151\n",
            "    Batch 80: combined loss = 3.590070962905884 , average loss: 0.897517740726471\n",
            "    Batch 90: combined loss = 3.7149734497070312 , average loss: 0.9287433624267578\n",
            "    Batch 100: combined loss = 4.5393967628479 , average loss: 1.134849190711975\n",
            "    Batch 110: combined loss = 4.405826568603516 , average loss: 1.101456642150879\n",
            "    Batch 120: combined loss = 5.445379257202148 , average loss: 1.361344814300537\n",
            "    Batch 130: combined loss = 5.241663932800293 , average loss: 1.3104159832000732\n",
            "    Batch 140: combined loss = 2.7201108932495117 , average loss: 0.6800277233123779\n",
            "    Batch 150: combined loss = 5.010575294494629 , average loss: 1.2526438236236572\n",
            "    Batch 160: combined loss = 3.946363687515259 , average loss: 0.9865909218788147\n",
            "    Batch 170: combined loss = 3.5471158027648926 , average loss: 0.8867789506912231\n",
            "    Batch 180: combined loss = 4.6379780769348145 , average loss: 1.1594945192337036\n",
            "RUNNING EPOCH: 26\n",
            "    Batch 0: combined loss = 4.609649181365967 , average loss: 1.1524122953414917\n",
            "    Batch 10: combined loss = 5.160327911376953 , average loss: 1.2900819778442383\n",
            "    Batch 20: combined loss = 5.0739336013793945 , average loss: 1.2684834003448486\n",
            "    Batch 30: combined loss = 5.345878601074219 , average loss: 1.3364696502685547\n",
            "    Batch 40: combined loss = 5.1335039138793945 , average loss: 1.2833759784698486\n",
            "    Batch 50: combined loss = 4.231119632720947 , average loss: 1.0577799081802368\n",
            "    Batch 60: combined loss = 6.428483009338379 , average loss: 1.6071207523345947\n",
            "    Batch 70: combined loss = 4.79470157623291 , average loss: 1.1986753940582275\n",
            "    Batch 80: combined loss = 4.9362897872924805 , average loss: 1.2340724468231201\n",
            "    Batch 90: combined loss = 5.946452617645264 , average loss: 1.486613154411316\n",
            "    Batch 100: combined loss = 4.2091546058654785 , average loss: 1.0522886514663696\n",
            "    Batch 110: combined loss = 3.4705915451049805 , average loss: 0.8676478862762451\n",
            "    Batch 120: combined loss = 3.064812183380127 , average loss: 0.7662030458450317\n",
            "    Batch 130: combined loss = 4.622616767883301 , average loss: 1.1556541919708252\n",
            "    Batch 140: combined loss = 3.7890563011169434 , average loss: 0.9472640752792358\n",
            "    Batch 150: combined loss = 2.55309796333313 , average loss: 0.6382744908332825\n",
            "    Batch 160: combined loss = 5.045779228210449 , average loss: 1.2614448070526123\n",
            "    Batch 170: combined loss = 5.339597702026367 , average loss: 1.3348994255065918\n",
            "    Batch 180: combined loss = 3.776276111602783 , average loss: 0.9440690279006958\n",
            "RUNNING EPOCH: 27\n",
            "    Batch 0: combined loss = 4.179166793823242 , average loss: 1.0447916984558105\n",
            "    Batch 10: combined loss = 4.310781955718994 , average loss: 1.0776954889297485\n",
            "    Batch 20: combined loss = 5.130631923675537 , average loss: 1.2826579809188843\n",
            "    Batch 30: combined loss = 4.4421586990356445 , average loss: 1.1105396747589111\n",
            "    Batch 40: combined loss = 4.2144317626953125 , average loss: 1.0536079406738281\n",
            "    Batch 50: combined loss = 4.611386299133301 , average loss: 1.1528465747833252\n",
            "    Batch 60: combined loss = 3.773982048034668 , average loss: 0.943495512008667\n",
            "    Batch 70: combined loss = 4.282517910003662 , average loss: 1.0706294775009155\n",
            "    Batch 80: combined loss = 1.4745771884918213 , average loss: 0.3686442971229553\n",
            "    Batch 90: combined loss = 3.766716241836548 , average loss: 0.941679060459137\n",
            "    Batch 100: combined loss = 5.070501327514648 , average loss: 1.267625331878662\n",
            "    Batch 110: combined loss = 4.305179595947266 , average loss: 1.0762948989868164\n",
            "    Batch 120: combined loss = 3.296116590499878 , average loss: 0.8240291476249695\n",
            "    Batch 130: combined loss = 4.996509075164795 , average loss: 1.2491272687911987\n",
            "    Batch 140: combined loss = 4.433625221252441 , average loss: 1.1084063053131104\n",
            "    Batch 150: combined loss = 3.6865234375 , average loss: 0.921630859375\n",
            "    Batch 160: combined loss = 3.324564218521118 , average loss: 0.8311410546302795\n",
            "    Batch 170: combined loss = 5.2989726066589355 , average loss: 1.3247431516647339\n",
            "    Batch 180: combined loss = 5.995584011077881 , average loss: 1.4988960027694702\n",
            "RUNNING EPOCH: 28\n",
            "    Batch 0: combined loss = 4.4299726486206055 , average loss: 1.1074931621551514\n",
            "    Batch 10: combined loss = 4.247493267059326 , average loss: 1.0618733167648315\n",
            "    Batch 20: combined loss = 4.750529766082764 , average loss: 1.187632441520691\n",
            "    Batch 30: combined loss = 3.7701077461242676 , average loss: 0.9425269365310669\n",
            "    Batch 40: combined loss = 3.3506555557250977 , average loss: 0.8376638889312744\n",
            "    Batch 50: combined loss = 2.926942825317383 , average loss: 0.7317357063293457\n",
            "    Batch 60: combined loss = 4.173832416534424 , average loss: 1.043458104133606\n",
            "    Batch 70: combined loss = 4.961796760559082 , average loss: 1.2404491901397705\n",
            "    Batch 80: combined loss = 4.371406555175781 , average loss: 1.0928516387939453\n",
            "    Batch 90: combined loss = 4.570136547088623 , average loss: 1.1425341367721558\n",
            "    Batch 100: combined loss = 3.213270664215088 , average loss: 0.803317666053772\n",
            "    Batch 110: combined loss = 3.5437374114990234 , average loss: 0.8859343528747559\n",
            "    Batch 120: combined loss = 5.298213005065918 , average loss: 1.3245532512664795\n",
            "    Batch 130: combined loss = 5.734159469604492 , average loss: 1.433539867401123\n",
            "    Batch 140: combined loss = 4.5943145751953125 , average loss: 1.1485786437988281\n",
            "    Batch 150: combined loss = 4.0711350440979 , average loss: 1.017783761024475\n",
            "    Batch 160: combined loss = 6.066919326782227 , average loss: 1.5167298316955566\n",
            "    Batch 170: combined loss = 2.939969301223755 , average loss: 0.7349923253059387\n",
            "    Batch 180: combined loss = 5.600064754486084 , average loss: 1.400016188621521\n",
            "RUNNING EPOCH: 29\n",
            "    Batch 0: combined loss = 3.1975769996643066 , average loss: 0.7993942499160767\n",
            "    Batch 10: combined loss = 2.7657289505004883 , average loss: 0.6914322376251221\n",
            "    Batch 20: combined loss = 4.79902458190918 , average loss: 1.199756145477295\n",
            "    Batch 30: combined loss = 4.7313151359558105 , average loss: 1.1828287839889526\n",
            "    Batch 40: combined loss = 3.0650131702423096 , average loss: 0.7662532925605774\n",
            "    Batch 50: combined loss = 5.072785377502441 , average loss: 1.2681963443756104\n",
            "    Batch 60: combined loss = 5.425968170166016 , average loss: 1.356492042541504\n",
            "    Batch 70: combined loss = 2.9149913787841797 , average loss: 0.7287478446960449\n",
            "    Batch 80: combined loss = 3.8359291553497314 , average loss: 0.9589822888374329\n",
            "    Batch 90: combined loss = 3.0209007263183594 , average loss: 0.7552251815795898\n",
            "    Batch 100: combined loss = 4.8168158531188965 , average loss: 1.2042039632797241\n",
            "    Batch 110: combined loss = 3.1341023445129395 , average loss: 0.7835255861282349\n",
            "    Batch 120: combined loss = 3.912667751312256 , average loss: 0.978166937828064\n",
            "    Batch 130: combined loss = 3.7224814891815186 , average loss: 0.9306203722953796\n",
            "    Batch 140: combined loss = 3.814971923828125 , average loss: 0.9537429809570312\n",
            "    Batch 150: combined loss = 5.490005016326904 , average loss: 1.372501254081726\n",
            "    Batch 160: combined loss = 5.7208051681518555 , average loss: 1.4302012920379639\n",
            "    Batch 170: combined loss = 3.5767621994018555 , average loss: 0.8941905498504639\n",
            "    Batch 180: combined loss = 3.4948482513427734 , average loss: 0.8737120628356934\n",
            "RUNNING EPOCH: 30\n",
            "    Batch 0: combined loss = 4.917582035064697 , average loss: 1.2293955087661743\n",
            "    Batch 10: combined loss = 5.6566972732543945 , average loss: 1.4141743183135986\n",
            "    Batch 20: combined loss = 3.1586217880249023 , average loss: 0.7896554470062256\n",
            "    Batch 30: combined loss = 3.361268997192383 , average loss: 0.8403172492980957\n",
            "    Batch 40: combined loss = 3.7415285110473633 , average loss: 0.9353821277618408\n",
            "    Batch 50: combined loss = 2.740076780319214 , average loss: 0.6850191950798035\n",
            "    Batch 60: combined loss = 4.49554443359375 , average loss: 1.1238861083984375\n",
            "    Batch 70: combined loss = 5.496744632720947 , average loss: 1.3741861581802368\n",
            "    Batch 80: combined loss = 4.571740627288818 , average loss: 1.1429351568222046\n",
            "    Batch 90: combined loss = 4.832887172698975 , average loss: 1.2082217931747437\n",
            "    Batch 100: combined loss = 3.1694304943084717 , average loss: 0.7923576235771179\n",
            "    Batch 110: combined loss = 4.288759708404541 , average loss: 1.0721899271011353\n",
            "    Batch 120: combined loss = 3.269380569458008 , average loss: 0.817345142364502\n",
            "    Batch 130: combined loss = 3.9041249752044678 , average loss: 0.9760312438011169\n",
            "    Batch 140: combined loss = 5.783588886260986 , average loss: 1.4458972215652466\n",
            "    Batch 150: combined loss = 5.027223110198975 , average loss: 1.2568057775497437\n",
            "    Batch 160: combined loss = 4.505672931671143 , average loss: 1.1264182329177856\n",
            "    Batch 170: combined loss = 3.3966245651245117 , average loss: 0.8491561412811279\n",
            "    Batch 180: combined loss = 3.3253097534179688 , average loss: 0.8313274383544922\n",
            "RUNNING EPOCH: 31\n",
            "    Batch 0: combined loss = 3.932396173477173 , average loss: 0.9830990433692932\n",
            "    Batch 10: combined loss = 4.373546123504639 , average loss: 1.0933865308761597\n",
            "    Batch 20: combined loss = 2.4274709224700928 , average loss: 0.6068677306175232\n",
            "    Batch 30: combined loss = 2.3335771560668945 , average loss: 0.5833942890167236\n",
            "    Batch 40: combined loss = 3.586785316467285 , average loss: 0.8966963291168213\n",
            "    Batch 50: combined loss = 2.673907995223999 , average loss: 0.6684769988059998\n",
            "    Batch 60: combined loss = 4.306139945983887 , average loss: 1.0765349864959717\n",
            "    Batch 70: combined loss = 3.6325011253356934 , average loss: 0.9081252813339233\n",
            "    Batch 80: combined loss = 3.3848416805267334 , average loss: 0.8462104201316833\n",
            "    Batch 90: combined loss = 3.15303111076355 , average loss: 0.7882577776908875\n",
            "    Batch 100: combined loss = 3.2683417797088623 , average loss: 0.8170854449272156\n",
            "    Batch 110: combined loss = 2.9026105403900146 , average loss: 0.7256526350975037\n",
            "    Batch 120: combined loss = 3.5205047130584717 , average loss: 0.8801261782646179\n",
            "    Batch 130: combined loss = 5.877801418304443 , average loss: 1.4694503545761108\n",
            "    Batch 140: combined loss = 2.972945213317871 , average loss: 0.7432363033294678\n",
            "    Batch 150: combined loss = 3.485765218734741 , average loss: 0.8714413046836853\n",
            "    Batch 160: combined loss = 4.029303073883057 , average loss: 1.0073257684707642\n",
            "    Batch 170: combined loss = 4.1271162033081055 , average loss: 1.0317790508270264\n",
            "    Batch 180: combined loss = 3.754467010498047 , average loss: 0.9386167526245117\n",
            "RUNNING EPOCH: 32\n",
            "    Batch 0: combined loss = 4.988799095153809 , average loss: 1.2471997737884521\n",
            "    Batch 10: combined loss = 3.3915162086486816 , average loss: 0.8478790521621704\n",
            "    Batch 20: combined loss = 3.931124448776245 , average loss: 0.9827811121940613\n",
            "    Batch 30: combined loss = 3.357326030731201 , average loss: 0.8393315076828003\n",
            "    Batch 40: combined loss = 5.063339710235596 , average loss: 1.265834927558899\n",
            "    Batch 50: combined loss = 2.147130012512207 , average loss: 0.5367825031280518\n",
            "    Batch 60: combined loss = 3.202000141143799 , average loss: 0.8005000352859497\n",
            "    Batch 70: combined loss = 4.417488098144531 , average loss: 1.1043720245361328\n",
            "    Batch 80: combined loss = 4.476997375488281 , average loss: 1.1192493438720703\n",
            "    Batch 90: combined loss = 5.401803493499756 , average loss: 1.350450873374939\n",
            "    Batch 100: combined loss = 4.056297302246094 , average loss: 1.0140743255615234\n",
            "    Batch 110: combined loss = 5.205930709838867 , average loss: 1.3014826774597168\n",
            "    Batch 120: combined loss = 4.938582897186279 , average loss: 1.2346457242965698\n",
            "    Batch 130: combined loss = 3.071904182434082 , average loss: 0.7679760456085205\n",
            "    Batch 140: combined loss = 4.294668674468994 , average loss: 1.0736671686172485\n",
            "    Batch 150: combined loss = 3.577113628387451 , average loss: 0.8942784070968628\n",
            "    Batch 160: combined loss = 2.4377169609069824 , average loss: 0.6094292402267456\n",
            "    Batch 170: combined loss = 3.1731626987457275 , average loss: 0.7932906746864319\n",
            "    Batch 180: combined loss = 3.6873011589050293 , average loss: 0.9218252897262573\n",
            "RUNNING EPOCH: 33\n",
            "    Batch 0: combined loss = 2.589818000793457 , average loss: 0.6474545001983643\n",
            "    Batch 10: combined loss = 3.837165355682373 , average loss: 0.9592913389205933\n",
            "    Batch 20: combined loss = 2.623249053955078 , average loss: 0.6558122634887695\n",
            "    Batch 30: combined loss = 3.7916595935821533 , average loss: 0.9479148983955383\n",
            "    Batch 40: combined loss = 4.125121116638184 , average loss: 1.031280279159546\n",
            "    Batch 50: combined loss = 2.963120937347412 , average loss: 0.740780234336853\n",
            "    Batch 60: combined loss = 3.767705202102661 , average loss: 0.9419263005256653\n",
            "    Batch 70: combined loss = 4.014320373535156 , average loss: 1.003580093383789\n",
            "    Batch 80: combined loss = 2.4058730602264404 , average loss: 0.6014682650566101\n",
            "    Batch 90: combined loss = 2.495476007461548 , average loss: 0.623869001865387\n",
            "    Batch 100: combined loss = 4.173172950744629 , average loss: 1.0432932376861572\n",
            "    Batch 110: combined loss = 4.616280555725098 , average loss: 1.1540701389312744\n",
            "    Batch 120: combined loss = 4.736711025238037 , average loss: 1.1841777563095093\n",
            "    Batch 130: combined loss = 5.152783393859863 , average loss: 1.2881958484649658\n",
            "    Batch 140: combined loss = 5.184442520141602 , average loss: 1.2961106300354004\n",
            "    Batch 150: combined loss = 3.138575553894043 , average loss: 0.7846438884735107\n",
            "    Batch 160: combined loss = 3.3949947357177734 , average loss: 0.8487486839294434\n",
            "    Batch 170: combined loss = 3.9606614112854004 , average loss: 0.9901653528213501\n",
            "    Batch 180: combined loss = 5.595738410949707 , average loss: 1.3989346027374268\n",
            "RUNNING EPOCH: 34\n",
            "    Batch 0: combined loss = 3.295640230178833 , average loss: 0.8239100575447083\n",
            "    Batch 10: combined loss = 2.9630532264709473 , average loss: 0.7407633066177368\n",
            "    Batch 20: combined loss = 3.0997514724731445 , average loss: 0.7749378681182861\n",
            "    Batch 30: combined loss = 2.155660390853882 , average loss: 0.5389150977134705\n",
            "    Batch 40: combined loss = 3.641942262649536 , average loss: 0.910485565662384\n",
            "    Batch 50: combined loss = 2.919530153274536 , average loss: 0.729882538318634\n",
            "    Batch 60: combined loss = 3.5527172088623047 , average loss: 0.8881793022155762\n",
            "    Batch 70: combined loss = 2.7205376625061035 , average loss: 0.6801344156265259\n",
            "    Batch 80: combined loss = 3.1175568103790283 , average loss: 0.7793892025947571\n",
            "    Batch 90: combined loss = 2.634861469268799 , average loss: 0.6587153673171997\n",
            "    Batch 100: combined loss = 3.6982271671295166 , average loss: 0.9245567917823792\n",
            "    Batch 110: combined loss = 7.182440757751465 , average loss: 1.7956101894378662\n",
            "    Batch 120: combined loss = 4.958362579345703 , average loss: 1.2395906448364258\n",
            "    Batch 130: combined loss = 3.2971138954162598 , average loss: 0.8242784738540649\n",
            "    Batch 140: combined loss = 3.387270450592041 , average loss: 0.8468176126480103\n",
            "    Batch 150: combined loss = 2.413818597793579 , average loss: 0.6034546494483948\n",
            "    Batch 160: combined loss = 3.1465835571289062 , average loss: 0.7866458892822266\n",
            "    Batch 170: combined loss = 3.5550875663757324 , average loss: 0.8887718915939331\n",
            "    Batch 180: combined loss = 2.60249662399292 , average loss: 0.65062415599823\n",
            "RUNNING EPOCH: 35\n",
            "    Batch 0: combined loss = 4.075108528137207 , average loss: 1.0187771320343018\n",
            "    Batch 10: combined loss = 2.5548644065856934 , average loss: 0.6387161016464233\n",
            "    Batch 20: combined loss = 3.1515884399414062 , average loss: 0.7878971099853516\n",
            "    Batch 30: combined loss = 3.90690541267395 , average loss: 0.9767263531684875\n",
            "    Batch 40: combined loss = 3.45450758934021 , average loss: 0.8636268973350525\n",
            "    Batch 50: combined loss = 3.765235185623169 , average loss: 0.9413087964057922\n",
            "    Batch 60: combined loss = 4.033778190612793 , average loss: 1.0084445476531982\n",
            "    Batch 70: combined loss = 4.407930850982666 , average loss: 1.1019827127456665\n",
            "    Batch 80: combined loss = 3.877175807952881 , average loss: 0.9692939519882202\n",
            "    Batch 90: combined loss = 3.164531946182251 , average loss: 0.7911329865455627\n",
            "    Batch 100: combined loss = 2.937995195388794 , average loss: 0.7344987988471985\n",
            "    Batch 110: combined loss = 4.186022758483887 , average loss: 1.0465056896209717\n",
            "    Batch 120: combined loss = 3.972151279449463 , average loss: 0.9930378198623657\n",
            "    Batch 130: combined loss = 2.0375959873199463 , average loss: 0.5093989968299866\n",
            "    Batch 140: combined loss = 2.2731761932373047 , average loss: 0.5682940483093262\n",
            "    Batch 150: combined loss = 3.3407132625579834 , average loss: 0.8351783156394958\n",
            "    Batch 160: combined loss = 3.74429988861084 , average loss: 0.93607497215271\n",
            "    Batch 170: combined loss = 5.6688666343688965 , average loss: 1.4172166585922241\n",
            "    Batch 180: combined loss = 4.117592811584473 , average loss: 1.0293982028961182\n",
            "RUNNING EPOCH: 36\n",
            "    Batch 0: combined loss = 3.2106409072875977 , average loss: 0.8026602268218994\n",
            "    Batch 10: combined loss = 3.8520169258117676 , average loss: 0.9630042314529419\n",
            "    Batch 20: combined loss = 1.4795269966125488 , average loss: 0.3698817491531372\n",
            "    Batch 30: combined loss = 3.1534438133239746 , average loss: 0.7883609533309937\n",
            "    Batch 40: combined loss = 3.3077518939971924 , average loss: 0.8269379734992981\n",
            "    Batch 50: combined loss = 2.880802631378174 , average loss: 0.7202006578445435\n",
            "    Batch 60: combined loss = 4.381301403045654 , average loss: 1.0953253507614136\n",
            "    Batch 70: combined loss = 3.708606243133545 , average loss: 0.9271515607833862\n",
            "    Batch 80: combined loss = 3.934239149093628 , average loss: 0.983559787273407\n",
            "    Batch 90: combined loss = 2.8942253589630127 , average loss: 0.7235563397407532\n",
            "    Batch 100: combined loss = 2.696739673614502 , average loss: 0.6741849184036255\n",
            "    Batch 110: combined loss = 3.9887351989746094 , average loss: 0.9971837997436523\n",
            "    Batch 120: combined loss = 1.8318325281143188 , average loss: 0.4579581320285797\n",
            "    Batch 130: combined loss = 4.312708377838135 , average loss: 1.0781770944595337\n",
            "    Batch 140: combined loss = 5.648073673248291 , average loss: 1.4120184183120728\n",
            "    Batch 150: combined loss = 4.534194469451904 , average loss: 1.133548617362976\n",
            "    Batch 160: combined loss = 4.336658000946045 , average loss: 1.0841645002365112\n",
            "    Batch 170: combined loss = 3.610525131225586 , average loss: 0.9026312828063965\n",
            "    Batch 180: combined loss = 3.2792177200317383 , average loss: 0.8198044300079346\n",
            "RUNNING EPOCH: 37\n",
            "    Batch 0: combined loss = 3.2908709049224854 , average loss: 0.8227177262306213\n",
            "    Batch 10: combined loss = 3.163492202758789 , average loss: 0.7908730506896973\n",
            "    Batch 20: combined loss = 3.183236598968506 , average loss: 0.7958091497421265\n",
            "    Batch 30: combined loss = 4.482602596282959 , average loss: 1.1206506490707397\n",
            "    Batch 40: combined loss = 3.3540842533111572 , average loss: 0.8385210633277893\n",
            "    Batch 50: combined loss = 3.1355738639831543 , average loss: 0.7838934659957886\n",
            "    Batch 60: combined loss = 4.561985015869141 , average loss: 1.1404962539672852\n",
            "    Batch 70: combined loss = 4.183460712432861 , average loss: 1.0458651781082153\n",
            "    Batch 80: combined loss = 3.4879744052886963 , average loss: 0.8719936013221741\n",
            "    Batch 90: combined loss = 4.3151373863220215 , average loss: 1.0787843465805054\n",
            "    Batch 100: combined loss = 3.555001974105835 , average loss: 0.8887504935264587\n",
            "    Batch 110: combined loss = 3.510148763656616 , average loss: 0.877537190914154\n",
            "    Batch 120: combined loss = 3.757659912109375 , average loss: 0.9394149780273438\n",
            "    Batch 130: combined loss = 3.6740968227386475 , average loss: 0.9185242056846619\n",
            "    Batch 140: combined loss = 2.510528564453125 , average loss: 0.6276321411132812\n",
            "    Batch 150: combined loss = 2.9294493198394775 , average loss: 0.7323623299598694\n",
            "    Batch 160: combined loss = 2.5667552947998047 , average loss: 0.6416888236999512\n",
            "    Batch 170: combined loss = 3.2895874977111816 , average loss: 0.8223968744277954\n",
            "    Batch 180: combined loss = 3.7641453742980957 , average loss: 0.9410363435745239\n",
            "RUNNING EPOCH: 38\n",
            "    Batch 0: combined loss = 3.1055445671081543 , average loss: 0.7763861417770386\n",
            "    Batch 10: combined loss = 3.2729945182800293 , average loss: 0.8182486295700073\n",
            "    Batch 20: combined loss = 3.6053466796875 , average loss: 0.901336669921875\n",
            "    Batch 30: combined loss = 3.6739542484283447 , average loss: 0.9184885621070862\n",
            "    Batch 40: combined loss = 3.1586835384368896 , average loss: 0.7896708846092224\n",
            "    Batch 50: combined loss = 3.562164068222046 , average loss: 0.8905410170555115\n",
            "    Batch 60: combined loss = 5.724346160888672 , average loss: 1.431086540222168\n",
            "    Batch 70: combined loss = 3.7023873329162598 , average loss: 0.9255968332290649\n",
            "    Batch 80: combined loss = 4.463123321533203 , average loss: 1.1157808303833008\n",
            "    Batch 90: combined loss = 4.277960777282715 , average loss: 1.0694901943206787\n",
            "    Batch 100: combined loss = 3.251525402069092 , average loss: 0.812881350517273\n",
            "    Batch 110: combined loss = 2.888298273086548 , average loss: 0.722074568271637\n",
            "    Batch 120: combined loss = 3.2036373615264893 , average loss: 0.8009093403816223\n",
            "    Batch 130: combined loss = 3.2037758827209473 , average loss: 0.8009439706802368\n",
            "    Batch 140: combined loss = 2.7986679077148438 , average loss: 0.6996669769287109\n",
            "    Batch 150: combined loss = 3.1610240936279297 , average loss: 0.7902560234069824\n",
            "    Batch 160: combined loss = 3.6056437492370605 , average loss: 0.9014109373092651\n",
            "    Batch 170: combined loss = 2.65644907951355 , average loss: 0.6641122698783875\n",
            "    Batch 180: combined loss = 2.0018343925476074 , average loss: 0.5004585981369019\n",
            "RUNNING EPOCH: 39\n",
            "    Batch 0: combined loss = 3.1044914722442627 , average loss: 0.7761228680610657\n",
            "    Batch 10: combined loss = 3.7654836177825928 , average loss: 0.9413709044456482\n",
            "    Batch 20: combined loss = 5.468786239624023 , average loss: 1.3671965599060059\n",
            "    Batch 30: combined loss = 2.7809197902679443 , average loss: 0.6952299475669861\n",
            "    Batch 40: combined loss = 4.625865459442139 , average loss: 1.1564663648605347\n",
            "    Batch 50: combined loss = 4.479308128356934 , average loss: 1.1198270320892334\n",
            "    Batch 60: combined loss = 3.153214454650879 , average loss: 0.7883036136627197\n",
            "    Batch 70: combined loss = 2.92541766166687 , average loss: 0.7313544154167175\n",
            "    Batch 80: combined loss = 2.433089017868042 , average loss: 0.6082722544670105\n",
            "    Batch 90: combined loss = 2.9181482791900635 , average loss: 0.7295370697975159\n",
            "    Batch 100: combined loss = 2.417541265487671 , average loss: 0.6043853163719177\n",
            "    Batch 110: combined loss = 3.5137922763824463 , average loss: 0.8784480690956116\n",
            "    Batch 120: combined loss = 1.7648741006851196 , average loss: 0.4412185251712799\n",
            "    Batch 130: combined loss = 3.2482237815856934 , average loss: 0.8120559453964233\n",
            "    Batch 140: combined loss = 3.0139801502227783 , average loss: 0.7534950375556946\n",
            "    Batch 150: combined loss = 2.865529775619507 , average loss: 0.7163824439048767\n",
            "    Batch 160: combined loss = 3.6303505897521973 , average loss: 0.9075876474380493\n",
            "    Batch 170: combined loss = 3.4366610050201416 , average loss: 0.8591652512550354\n",
            "    Batch 180: combined loss = 3.816751480102539 , average loss: 0.9541878700256348\n",
            "RUNNING EPOCH: 40\n",
            "    Batch 0: combined loss = 2.574388265609741 , average loss: 0.6435970664024353\n",
            "    Batch 10: combined loss = 3.7795727252960205 , average loss: 0.9448931813240051\n",
            "    Batch 20: combined loss = 2.454413890838623 , average loss: 0.6136034727096558\n",
            "    Batch 30: combined loss = 3.0971975326538086 , average loss: 0.7742993831634521\n",
            "    Batch 40: combined loss = 3.1888082027435303 , average loss: 0.7972020506858826\n",
            "    Batch 50: combined loss = 3.085540294647217 , average loss: 0.7713850736618042\n",
            "    Batch 60: combined loss = 3.4448421001434326 , average loss: 0.8612105250358582\n",
            "    Batch 70: combined loss = 3.4675118923187256 , average loss: 0.8668779730796814\n",
            "    Batch 80: combined loss = 3.2650516033172607 , average loss: 0.8162629008293152\n",
            "    Batch 90: combined loss = 4.662842273712158 , average loss: 1.1657105684280396\n",
            "    Batch 100: combined loss = 4.074803352355957 , average loss: 1.0187008380889893\n",
            "    Batch 110: combined loss = 2.887714385986328 , average loss: 0.721928596496582\n",
            "    Batch 120: combined loss = 3.780668258666992 , average loss: 0.945167064666748\n",
            "    Batch 130: combined loss = 2.870607852935791 , average loss: 0.7176519632339478\n",
            "    Batch 140: combined loss = 3.9723222255706787 , average loss: 0.9930805563926697\n",
            "    Batch 150: combined loss = 2.8675129413604736 , average loss: 0.7168782353401184\n",
            "    Batch 160: combined loss = 3.895472764968872 , average loss: 0.973868191242218\n",
            "    Batch 170: combined loss = 3.947737216949463 , average loss: 0.9869343042373657\n",
            "    Batch 180: combined loss = 2.767726421356201 , average loss: 0.6919316053390503\n",
            "RUNNING EPOCH: 41\n",
            "    Batch 0: combined loss = 3.7490885257720947 , average loss: 0.9372721314430237\n",
            "    Batch 10: combined loss = 4.3120222091674805 , average loss: 1.0780055522918701\n",
            "    Batch 20: combined loss = 3.0257599353790283 , average loss: 0.7564399838447571\n",
            "    Batch 30: combined loss = 1.3179328441619873 , average loss: 0.3294832110404968\n",
            "    Batch 40: combined loss = 2.888617992401123 , average loss: 0.7221544981002808\n",
            "    Batch 50: combined loss = 5.245300769805908 , average loss: 1.311325192451477\n",
            "    Batch 60: combined loss = 2.7808456420898438 , average loss: 0.6952114105224609\n",
            "    Batch 70: combined loss = 2.376479148864746 , average loss: 0.5941197872161865\n",
            "    Batch 80: combined loss = 3.890899896621704 , average loss: 0.972724974155426\n",
            "    Batch 90: combined loss = 3.21205997467041 , average loss: 0.8030149936676025\n",
            "    Batch 100: combined loss = 2.7146711349487305 , average loss: 0.6786677837371826\n",
            "    Batch 110: combined loss = 4.379081726074219 , average loss: 1.0947704315185547\n",
            "    Batch 120: combined loss = 3.670482873916626 , average loss: 0.9176207184791565\n",
            "    Batch 130: combined loss = 4.080833911895752 , average loss: 1.020208477973938\n",
            "    Batch 140: combined loss = 2.9644718170166016 , average loss: 0.7411179542541504\n",
            "    Batch 150: combined loss = 1.847504734992981 , average loss: 0.46187618374824524\n",
            "    Batch 160: combined loss = 2.5962748527526855 , average loss: 0.6490687131881714\n",
            "    Batch 170: combined loss = 3.4375667572021484 , average loss: 0.8593916893005371\n",
            "    Batch 180: combined loss = 2.6196653842926025 , average loss: 0.6549163460731506\n",
            "RUNNING EPOCH: 42\n",
            "    Batch 0: combined loss = 2.9645800590515137 , average loss: 0.7411450147628784\n",
            "    Batch 10: combined loss = 1.3282312154769897 , average loss: 0.33205780386924744\n",
            "    Batch 20: combined loss = 2.511629343032837 , average loss: 0.6279073357582092\n",
            "    Batch 30: combined loss = 3.5114173889160156 , average loss: 0.8778543472290039\n",
            "    Batch 40: combined loss = 2.211638927459717 , average loss: 0.5529097318649292\n",
            "    Batch 50: combined loss = 1.6119283437728882 , average loss: 0.40298208594322205\n",
            "    Batch 60: combined loss = 2.548814296722412 , average loss: 0.637203574180603\n",
            "    Batch 70: combined loss = 2.843627452850342 , average loss: 0.7109068632125854\n",
            "    Batch 80: combined loss = 3.1575582027435303 , average loss: 0.7893895506858826\n",
            "    Batch 90: combined loss = 2.8181939125061035 , average loss: 0.7045484781265259\n",
            "    Batch 100: combined loss = 2.710968494415283 , average loss: 0.6777421236038208\n",
            "    Batch 110: combined loss = 2.9145259857177734 , average loss: 0.7286314964294434\n",
            "    Batch 120: combined loss = 2.698789596557617 , average loss: 0.6746973991394043\n",
            "    Batch 130: combined loss = 5.05595588684082 , average loss: 1.263988971710205\n",
            "    Batch 140: combined loss = 3.0812649726867676 , average loss: 0.7703162431716919\n",
            "    Batch 150: combined loss = 2.27142596244812 , average loss: 0.56785649061203\n",
            "    Batch 160: combined loss = 3.5876150131225586 , average loss: 0.8969037532806396\n",
            "    Batch 170: combined loss = 3.1848654747009277 , average loss: 0.7962163686752319\n",
            "    Batch 180: combined loss = 3.1344032287597656 , average loss: 0.7836008071899414\n",
            "RUNNING EPOCH: 43\n",
            "    Batch 0: combined loss = 3.0133495330810547 , average loss: 0.7533373832702637\n",
            "    Batch 10: combined loss = 3.0204522609710693 , average loss: 0.7551130652427673\n",
            "    Batch 20: combined loss = 4.911438941955566 , average loss: 1.2278597354888916\n",
            "    Batch 30: combined loss = 4.479445457458496 , average loss: 1.119861364364624\n",
            "    Batch 40: combined loss = 4.45993709564209 , average loss: 1.1149842739105225\n",
            "    Batch 50: combined loss = 3.9796133041381836 , average loss: 0.9949033260345459\n",
            "    Batch 60: combined loss = 4.1541428565979 , average loss: 1.038535714149475\n",
            "    Batch 70: combined loss = 3.632298707962036 , average loss: 0.908074676990509\n",
            "    Batch 80: combined loss = 2.028266429901123 , average loss: 0.5070666074752808\n",
            "    Batch 90: combined loss = 3.5854859352111816 , average loss: 0.8963714838027954\n",
            "    Batch 100: combined loss = 3.2532808780670166 , average loss: 0.8133202195167542\n",
            "    Batch 110: combined loss = 1.9867112636566162 , average loss: 0.49667781591415405\n",
            "    Batch 120: combined loss = 2.259467601776123 , average loss: 0.5648669004440308\n",
            "    Batch 130: combined loss = 2.6714746952056885 , average loss: 0.6678686738014221\n",
            "    Batch 140: combined loss = 3.8444924354553223 , average loss: 0.9611231088638306\n",
            "    Batch 150: combined loss = 4.297250270843506 , average loss: 1.0743125677108765\n",
            "    Batch 160: combined loss = 2.231337785720825 , average loss: 0.5578344464302063\n",
            "    Batch 170: combined loss = 4.092798233032227 , average loss: 1.0231995582580566\n",
            "    Batch 180: combined loss = 4.450693130493164 , average loss: 1.112673282623291\n",
            "RUNNING EPOCH: 44\n",
            "    Batch 0: combined loss = 3.052926540374756 , average loss: 0.763231635093689\n",
            "    Batch 10: combined loss = 3.5575411319732666 , average loss: 0.8893852829933167\n",
            "    Batch 20: combined loss = 2.945056915283203 , average loss: 0.7362642288208008\n",
            "    Batch 30: combined loss = 2.568150520324707 , average loss: 0.6420376300811768\n",
            "    Batch 40: combined loss = 1.7114958763122559 , average loss: 0.42787396907806396\n",
            "    Batch 50: combined loss = 2.775022029876709 , average loss: 0.6937555074691772\n",
            "    Batch 60: combined loss = 3.285331964492798 , average loss: 0.8213329911231995\n",
            "    Batch 70: combined loss = 2.7330591678619385 , average loss: 0.6832647919654846\n",
            "    Batch 80: combined loss = 1.93107271194458 , average loss: 0.482768177986145\n",
            "    Batch 90: combined loss = 1.8455162048339844 , average loss: 0.4613790512084961\n",
            "    Batch 100: combined loss = 4.704950332641602 , average loss: 1.1762375831604004\n",
            "    Batch 110: combined loss = 2.46376371383667 , average loss: 0.6159409284591675\n",
            "    Batch 120: combined loss = 2.1131772994995117 , average loss: 0.5282943248748779\n",
            "    Batch 130: combined loss = 1.7799272537231445 , average loss: 0.44498181343078613\n",
            "    Batch 140: combined loss = 2.603262186050415 , average loss: 0.6508155465126038\n",
            "    Batch 150: combined loss = 2.388026475906372 , average loss: 0.597006618976593\n",
            "    Batch 160: combined loss = 3.1126954555511475 , average loss: 0.7781738638877869\n",
            "    Batch 170: combined loss = 3.897038221359253 , average loss: 0.9742595553398132\n",
            "    Batch 180: combined loss = 2.1932952404022217 , average loss: 0.5483238101005554\n",
            "RUNNING EPOCH: 45\n",
            "    Batch 0: combined loss = 3.005324602127075 , average loss: 0.7513311505317688\n",
            "    Batch 10: combined loss = 2.774104356765747 , average loss: 0.6935260891914368\n",
            "    Batch 20: combined loss = 5.531949996948242 , average loss: 1.3829874992370605\n",
            "    Batch 30: combined loss = 4.470945358276367 , average loss: 1.1177363395690918\n",
            "    Batch 40: combined loss = 3.0904829502105713 , average loss: 0.7726207375526428\n",
            "    Batch 50: combined loss = 2.6165735721588135 , average loss: 0.6541433930397034\n",
            "    Batch 60: combined loss = 3.1176981925964355 , average loss: 0.7794245481491089\n",
            "    Batch 70: combined loss = 1.976380705833435 , average loss: 0.49409517645835876\n",
            "    Batch 80: combined loss = 3.7001805305480957 , average loss: 0.9250451326370239\n",
            "    Batch 90: combined loss = 2.6707701683044434 , average loss: 0.6676925420761108\n",
            "    Batch 100: combined loss = 1.9583938121795654 , average loss: 0.48959845304489136\n",
            "    Batch 110: combined loss = 3.4393858909606934 , average loss: 0.8598464727401733\n",
            "    Batch 120: combined loss = 3.547264814376831 , average loss: 0.8868162035942078\n",
            "    Batch 130: combined loss = 2.812227725982666 , average loss: 0.7030569314956665\n",
            "    Batch 140: combined loss = 4.035651206970215 , average loss: 1.0089128017425537\n",
            "    Batch 150: combined loss = 2.9942007064819336 , average loss: 0.7485501766204834\n",
            "    Batch 160: combined loss = 3.8458001613616943 , average loss: 0.9614500403404236\n",
            "    Batch 170: combined loss = 3.3509931564331055 , average loss: 0.8377482891082764\n",
            "    Batch 180: combined loss = 3.019318103790283 , average loss: 0.7548295259475708\n",
            "RUNNING EPOCH: 46\n",
            "    Batch 0: combined loss = 2.9197826385498047 , average loss: 0.7299456596374512\n",
            "    Batch 10: combined loss = 2.392219305038452 , average loss: 0.598054826259613\n",
            "    Batch 20: combined loss = 2.4438652992248535 , average loss: 0.6109663248062134\n",
            "    Batch 30: combined loss = 2.2135210037231445 , average loss: 0.5533802509307861\n",
            "    Batch 40: combined loss = 2.994832992553711 , average loss: 0.7487082481384277\n",
            "    Batch 50: combined loss = 2.1107771396636963 , average loss: 0.5276942849159241\n",
            "    Batch 60: combined loss = 4.239802837371826 , average loss: 1.0599507093429565\n",
            "    Batch 70: combined loss = 3.4618141651153564 , average loss: 0.8654535412788391\n",
            "    Batch 80: combined loss = 3.079632043838501 , average loss: 0.7699080109596252\n",
            "    Batch 90: combined loss = 2.490281105041504 , average loss: 0.622570276260376\n",
            "    Batch 100: combined loss = 3.7216057777404785 , average loss: 0.9304014444351196\n",
            "    Batch 110: combined loss = 2.430877208709717 , average loss: 0.6077193021774292\n",
            "    Batch 120: combined loss = 3.3150343894958496 , average loss: 0.8287585973739624\n",
            "    Batch 130: combined loss = 3.279970645904541 , average loss: 0.8199926614761353\n",
            "    Batch 140: combined loss = 3.328556537628174 , average loss: 0.8321391344070435\n",
            "    Batch 150: combined loss = 3.1511600017547607 , average loss: 0.7877900004386902\n",
            "    Batch 160: combined loss = 2.6824896335601807 , average loss: 0.6706224083900452\n",
            "    Batch 170: combined loss = 2.9118003845214844 , average loss: 0.7279500961303711\n",
            "    Batch 180: combined loss = 3.6702260971069336 , average loss: 0.9175565242767334\n",
            "RUNNING EPOCH: 47\n",
            "    Batch 0: combined loss = 1.7452659606933594 , average loss: 0.43631649017333984\n",
            "    Batch 10: combined loss = 2.2327497005462646 , average loss: 0.5581874251365662\n",
            "    Batch 20: combined loss = 2.27303409576416 , average loss: 0.56825852394104\n",
            "    Batch 30: combined loss = 2.7931764125823975 , average loss: 0.6982941031455994\n",
            "    Batch 40: combined loss = 1.8244752883911133 , average loss: 0.4561188220977783\n",
            "    Batch 50: combined loss = 3.3400073051452637 , average loss: 0.8350018262863159\n",
            "    Batch 60: combined loss = 1.8952865600585938 , average loss: 0.47382164001464844\n",
            "    Batch 70: combined loss = 2.086763620376587 , average loss: 0.5216909050941467\n",
            "    Batch 80: combined loss = 2.637906312942505 , average loss: 0.6594765782356262\n",
            "    Batch 90: combined loss = 2.2837371826171875 , average loss: 0.5709342956542969\n",
            "    Batch 100: combined loss = 1.8826408386230469 , average loss: 0.4706602096557617\n",
            "    Batch 110: combined loss = 1.6253916025161743 , average loss: 0.4063479006290436\n",
            "    Batch 120: combined loss = 2.5846025943756104 , average loss: 0.6461506485939026\n",
            "    Batch 130: combined loss = 2.844968318939209 , average loss: 0.7112420797348022\n",
            "    Batch 140: combined loss = 2.157139778137207 , average loss: 0.5392849445343018\n",
            "    Batch 150: combined loss = 3.3108468055725098 , average loss: 0.8277117013931274\n",
            "    Batch 160: combined loss = 3.5516245365142822 , average loss: 0.8879061341285706\n",
            "    Batch 170: combined loss = 4.145395278930664 , average loss: 1.036348819732666\n",
            "    Batch 180: combined loss = 3.266852855682373 , average loss: 0.8167132139205933\n",
            "RUNNING EPOCH: 48\n",
            "    Batch 0: combined loss = 2.442626714706421 , average loss: 0.6106566786766052\n",
            "    Batch 10: combined loss = 2.6226956844329834 , average loss: 0.6556739211082458\n",
            "    Batch 20: combined loss = 1.6871795654296875 , average loss: 0.4217948913574219\n",
            "    Batch 30: combined loss = 1.6692143678665161 , average loss: 0.41730359196662903\n",
            "    Batch 40: combined loss = 2.5395541191101074 , average loss: 0.6348885297775269\n",
            "    Batch 50: combined loss = 2.476407051086426 , average loss: 0.6191017627716064\n",
            "    Batch 60: combined loss = 3.7153704166412354 , average loss: 0.9288426041603088\n",
            "    Batch 70: combined loss = 3.4408106803894043 , average loss: 0.8602026700973511\n",
            "    Batch 80: combined loss = 1.9582910537719727 , average loss: 0.48957276344299316\n",
            "    Batch 90: combined loss = 3.506558895111084 , average loss: 0.876639723777771\n",
            "    Batch 100: combined loss = 3.784672260284424 , average loss: 0.946168065071106\n",
            "    Batch 110: combined loss = 2.04992938041687 , average loss: 0.5124823451042175\n",
            "    Batch 120: combined loss = 1.7101569175720215 , average loss: 0.42753922939300537\n",
            "    Batch 130: combined loss = 2.962488889694214 , average loss: 0.7406222224235535\n",
            "    Batch 140: combined loss = 3.8057937622070312 , average loss: 0.9514484405517578\n",
            "    Batch 150: combined loss = 3.705230712890625 , average loss: 0.9263076782226562\n",
            "    Batch 160: combined loss = 3.0472633838653564 , average loss: 0.7618158459663391\n",
            "    Batch 170: combined loss = 4.911045551300049 , average loss: 1.2277613878250122\n",
            "    Batch 180: combined loss = 2.451535940170288 , average loss: 0.612883985042572\n",
            "RUNNING EPOCH: 49\n",
            "    Batch 0: combined loss = 1.9125577211380005 , average loss: 0.4781394302845001\n",
            "    Batch 10: combined loss = 2.517923593521118 , average loss: 0.6294808983802795\n",
            "    Batch 20: combined loss = 2.719597101211548 , average loss: 0.679899275302887\n",
            "    Batch 30: combined loss = 2.8841912746429443 , average loss: 0.7210478186607361\n",
            "    Batch 40: combined loss = 2.1699936389923096 , average loss: 0.5424984097480774\n",
            "    Batch 50: combined loss = 1.4158992767333984 , average loss: 0.3539748191833496\n",
            "    Batch 60: combined loss = 2.558351993560791 , average loss: 0.6395879983901978\n",
            "    Batch 70: combined loss = 2.7971105575561523 , average loss: 0.6992776393890381\n",
            "    Batch 80: combined loss = 3.141035795211792 , average loss: 0.785258948802948\n",
            "    Batch 90: combined loss = 3.3530735969543457 , average loss: 0.8382683992385864\n",
            "    Batch 100: combined loss = 2.623584508895874 , average loss: 0.6558961272239685\n",
            "    Batch 110: combined loss = 3.0086004734039307 , average loss: 0.7521501183509827\n",
            "    Batch 120: combined loss = 3.6150152683258057 , average loss: 0.9037538170814514\n",
            "    Batch 130: combined loss = 1.8852521181106567 , average loss: 0.4713130295276642\n",
            "    Batch 140: combined loss = 2.2701547145843506 , average loss: 0.5675386786460876\n",
            "    Batch 150: combined loss = 2.1000711917877197 , average loss: 0.5250177979469299\n",
            "    Batch 160: combined loss = 3.766963005065918 , average loss: 0.9417407512664795\n",
            "    Batch 170: combined loss = 3.1044068336486816 , average loss: 0.7761017084121704\n",
            "    Batch 180: combined loss = 3.2363839149475098 , average loss: 0.8090959787368774\n",
            "RUNNING EPOCH: 50\n",
            "    Batch 0: combined loss = 3.01348876953125 , average loss: 0.7533721923828125\n",
            "    Batch 10: combined loss = 2.908916473388672 , average loss: 0.727229118347168\n",
            "    Batch 20: combined loss = 2.5385642051696777 , average loss: 0.6346410512924194\n",
            "    Batch 30: combined loss = 2.620312213897705 , average loss: 0.6550780534744263\n",
            "    Batch 40: combined loss = 3.3728818893432617 , average loss: 0.8432204723358154\n",
            "    Batch 50: combined loss = 3.1695053577423096 , average loss: 0.7923763394355774\n",
            "    Batch 60: combined loss = 2.4349968433380127 , average loss: 0.6087492108345032\n",
            "    Batch 70: combined loss = 3.8653173446655273 , average loss: 0.9663293361663818\n",
            "    Batch 80: combined loss = 2.7109286785125732 , average loss: 0.6777321696281433\n",
            "    Batch 90: combined loss = 3.0240280628204346 , average loss: 0.7560070157051086\n",
            "    Batch 100: combined loss = 3.460177421569824 , average loss: 0.865044355392456\n",
            "    Batch 110: combined loss = 2.2259483337402344 , average loss: 0.5564870834350586\n",
            "    Batch 120: combined loss = 2.634732484817505 , average loss: 0.6586831212043762\n",
            "    Batch 130: combined loss = 1.7566022872924805 , average loss: 0.4391505718231201\n",
            "    Batch 140: combined loss = 2.8455045223236084 , average loss: 0.7113761305809021\n",
            "    Batch 150: combined loss = 2.9722256660461426 , average loss: 0.7430564165115356\n",
            "    Batch 160: combined loss = 2.7890045642852783 , average loss: 0.6972511410713196\n",
            "    Batch 170: combined loss = 2.0129010677337646 , average loss: 0.5032252669334412\n",
            "    Batch 180: combined loss = 2.274224042892456 , average loss: 0.568556010723114\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f169920d"
      },
      "source": [
        "torch.save(parallel_TCNs, \"./parallel_tcn\")"
      ],
      "id": "f169920d",
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fef88c2"
      },
      "source": [
        "# Evaluation\n",
        "\n"
      ],
      "id": "3fef88c2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5556da3a"
      },
      "source": [
        "test_dataset = TCNDataset(training=False)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset,collate_fn=collate_fn_padd,  batch_size=1, shuffle=False, drop_last=False)"
      ],
      "id": "5556da3a",
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67b59d95",
        "outputId": "97445339-2f6d-4cd8-eb6b-8e5498c3fb1e"
      },
      "source": [
        "eval_(parallel_TCNs,test_dataloader)"
      ],
      "id": "67b59d95",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Acc: 53.2346\n",
            "Edit: 5.6930\n",
            "F1@0.10: 4.2908\n",
            "F1@0.25: 3.4816\n",
            "F1@0.50: 2.0822\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}