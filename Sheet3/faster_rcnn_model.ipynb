{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd9a4802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "from torch.autograd import Variable\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad39a298",
   "metadata": {},
   "source": [
    "### 1. Transfer Learning:\n",
    "The input images are represented as Height×Width×Depth $\\mathit{Height} \\times \\mathit{Width} \\times \\mathit{Depth}$ Height×Width×Depth tensors (multidimensional arrays), which are passed through a pre-trained CNN up until an intermediate layer, ending up with a convolutional feature map.\n",
    "\n",
    "### 2. Region Proposal Network:\n",
    "\n",
    "### 3. Region of Interest Pooling:\n",
    "\n",
    "### 4. R-CNN Module:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215669a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e614df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "from ..utils.config import cfg\n",
    "from .bbox_transform import bbox_overlaps_batch, bbox_transform_batch\n",
    "import pdb\n",
    "\n",
    "class _ProposalTargetLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Assign object detection proposals to ground-truth targets. Produces proposal\n",
    "    classification labels and bounding-box regression targets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nclasses):\n",
    "        super(_ProposalTargetLayer, self).__init__()\n",
    "        self._num_classes = nclasses\n",
    "        self.BBOX_NORMALIZE_MEANS = torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS)\n",
    "        self.BBOX_NORMALIZE_STDS = torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS)\n",
    "        self.BBOX_INSIDE_WEIGHTS = torch.FloatTensor(cfg.TRAIN.BBOX_INSIDE_WEIGHTS)\n",
    "\n",
    "    def forward(self, all_rois, gt_boxes, num_boxes):\n",
    "\n",
    "        self.BBOX_NORMALIZE_MEANS = self.BBOX_NORMALIZE_MEANS.type_as(gt_boxes)\n",
    "        self.BBOX_NORMALIZE_STDS = self.BBOX_NORMALIZE_STDS.type_as(gt_boxes)\n",
    "        self.BBOX_INSIDE_WEIGHTS = self.BBOX_INSIDE_WEIGHTS.type_as(gt_boxes)\n",
    "\n",
    "        gt_boxes_append = gt_boxes.new(gt_boxes.size()).zero_()\n",
    "        gt_boxes_append[:,:,1:5] = gt_boxes[:,:,:4]\n",
    "\n",
    "        # Include ground-truth boxes in the set of candidate rois\n",
    "        all_rois = torch.cat([all_rois, gt_boxes_append], 1)\n",
    "\n",
    "        num_images = 1\n",
    "        rois_per_image = int(cfg.TRAIN.BATCH_SIZE / num_images)\n",
    "        fg_rois_per_image = int(np.round(cfg.TRAIN.FG_FRACTION * rois_per_image))\n",
    "        fg_rois_per_image = 1 if fg_rois_per_image == 0 else fg_rois_per_image\n",
    "\n",
    "        labels, rois, bbox_targets, bbox_inside_weights = self._sample_rois_pytorch(\n",
    "            all_rois, gt_boxes, fg_rois_per_image,\n",
    "            rois_per_image, self._num_classes)\n",
    "\n",
    "        bbox_outside_weights = (bbox_inside_weights > 0).float()\n",
    "\n",
    "        return rois, labels, bbox_targets, bbox_inside_weights, bbox_outside_weights\n",
    "\n",
    "    def backward(self, top, propagate_down, bottom):\n",
    "        \"\"\"This layer does not propagate gradients.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def reshape(self, bottom, top):\n",
    "        \"\"\"Reshaping happens during the call to forward.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def _get_bbox_regression_labels_pytorch(self, bbox_target_data, labels_batch, num_classes):\n",
    "        \"\"\"Bounding-box regression targets (bbox_target_data) are stored in a\n",
    "        compact form b x N x (class, tx, ty, tw, th)\n",
    "        This function expands those targets into the 4-of-4*K representation used\n",
    "        by the network (i.e. only one class has non-zero targets).\n",
    "        Returns:\n",
    "            bbox_target (ndarray): b x N x 4K blob of regression targets\n",
    "            bbox_inside_weights (ndarray): b x N x 4K blob of loss weights\n",
    "        \"\"\"\n",
    "        batch_size = labels_batch.size(0)\n",
    "        rois_per_image = labels_batch.size(1)\n",
    "        clss = labels_batch\n",
    "        bbox_targets = bbox_target_data.new(batch_size, rois_per_image, 4).zero_()\n",
    "        bbox_inside_weights = bbox_target_data.new(bbox_targets.size()).zero_()\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            # assert clss[b].sum() > 0\n",
    "            if clss[b].sum() == 0:\n",
    "                continue\n",
    "            inds = torch.nonzero(clss[b] > 0).view(-1)\n",
    "            for i in range(inds.numel()):\n",
    "                ind = inds[i]\n",
    "                bbox_targets[b, ind, :] = bbox_target_data[b, ind, :]\n",
    "                bbox_inside_weights[b, ind, :] = self.BBOX_INSIDE_WEIGHTS\n",
    "\n",
    "        return bbox_targets, bbox_inside_weights\n",
    "\n",
    "\n",
    "    def _compute_targets_pytorch(self, ex_rois, gt_rois):\n",
    "        \"\"\"Compute bounding-box regression targets for an image.\"\"\"\n",
    "\n",
    "        assert ex_rois.size(1) == gt_rois.size(1)\n",
    "        assert ex_rois.size(2) == 4\n",
    "        assert gt_rois.size(2) == 4\n",
    "\n",
    "        batch_size = ex_rois.size(0)\n",
    "        rois_per_image = ex_rois.size(1)\n",
    "\n",
    "        targets = bbox_transform_batch(ex_rois, gt_rois)\n",
    "\n",
    "        if cfg.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED:\n",
    "            # Optionally normalize targets by a precomputed mean and stdev\n",
    "            targets = ((targets - self.BBOX_NORMALIZE_MEANS.expand_as(targets))\n",
    "                        / self.BBOX_NORMALIZE_STDS.expand_as(targets))\n",
    "\n",
    "        return targets\n",
    "\n",
    "\n",
    "    def _sample_rois_pytorch(self, all_rois, gt_boxes, fg_rois_per_image, rois_per_image, num_classes):\n",
    "        \"\"\"Generate a random sample of RoIs comprising foreground and background\n",
    "        examples.\n",
    "        \"\"\"\n",
    "        # overlaps: (rois x gt_boxes)\n",
    "\n",
    "        overlaps = bbox_overlaps_batch(all_rois, gt_boxes)\n",
    "\n",
    "        max_overlaps, gt_assignment = torch.max(overlaps, 2)\n",
    "\n",
    "        batch_size = overlaps.size(0)\n",
    "        num_proposal = overlaps.size(1)\n",
    "        num_boxes_per_img = overlaps.size(2)\n",
    "\n",
    "        offset = torch.arange(0, batch_size)*gt_boxes.size(1)\n",
    "        offset = offset.view(-1, 1).type_as(gt_assignment) + gt_assignment\n",
    "\n",
    "        labels = gt_boxes[:,:,4].contiguous().view(-1).index((offset.view(-1),)).view(batch_size, -1)\n",
    "        \n",
    "        labels_batch = labels.new(batch_size, rois_per_image).zero_()\n",
    "        rois_batch  = all_rois.new(batch_size, rois_per_image, 5).zero_()\n",
    "        gt_rois_batch = all_rois.new(batch_size, rois_per_image, 5).zero_()\n",
    "        # Guard against the case when an image has fewer than max_fg_rois_per_image\n",
    "        # foreground RoIs\n",
    "        for i in range(batch_size):\n",
    "\n",
    "            fg_inds = torch.nonzero(max_overlaps[i] >= cfg.TRAIN.FG_THRESH).view(-1)\n",
    "            fg_num_rois = fg_inds.numel()\n",
    "\n",
    "            # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n",
    "            bg_inds = torch.nonzero((max_overlaps[i] < cfg.TRAIN.BG_THRESH_HI) &\n",
    "                                    (max_overlaps[i] >= cfg.TRAIN.BG_THRESH_LO)).view(-1)\n",
    "            bg_num_rois = bg_inds.numel()\n",
    "\n",
    "            if fg_num_rois > 0 and bg_num_rois > 0:\n",
    "                # sampling fg\n",
    "                fg_rois_per_this_image = min(fg_rois_per_image, fg_num_rois)\n",
    "\n",
    "                # torch.randperm seems has a bug on multi-gpu setting that cause the segfault.\n",
    "                # See https://github.com/pytorch/pytorch/issues/1868 for more details.\n",
    "                # use numpy instead.\n",
    "                #rand_num = torch.randperm(fg_num_rois).long().cuda()\n",
    "                rand_num = torch.from_numpy(np.random.permutation(fg_num_rois)).type_as(gt_boxes).long()\n",
    "                fg_inds = fg_inds[rand_num[:fg_rois_per_this_image]]\n",
    "\n",
    "                # sampling bg\n",
    "                bg_rois_per_this_image = rois_per_image - fg_rois_per_this_image\n",
    "\n",
    "                # Seems torch.rand has a bug, it will generate very large number and make an error.\n",
    "                # We use numpy rand instead.\n",
    "                #rand_num = (torch.rand(bg_rois_per_this_image) * bg_num_rois).long().cuda()\n",
    "                rand_num = np.floor(np.random.rand(bg_rois_per_this_image) * bg_num_rois)\n",
    "                rand_num = torch.from_numpy(rand_num).type_as(gt_boxes).long()\n",
    "                bg_inds = bg_inds[rand_num]\n",
    "\n",
    "            elif fg_num_rois > 0 and bg_num_rois == 0:\n",
    "                # sampling fg\n",
    "                #rand_num = torch.floor(torch.rand(rois_per_image) * fg_num_rois).long().cuda()\n",
    "                rand_num = np.floor(np.random.rand(rois_per_image) * fg_num_rois)\n",
    "                rand_num = torch.from_numpy(rand_num).type_as(gt_boxes).long()\n",
    "                fg_inds = fg_inds[rand_num]\n",
    "                fg_rois_per_this_image = rois_per_image\n",
    "                bg_rois_per_this_image = 0\n",
    "            elif bg_num_rois > 0 and fg_num_rois == 0:\n",
    "                # sampling bg\n",
    "                #rand_num = torch.floor(torch.rand(rois_per_image) * bg_num_rois).long().cuda()\n",
    "                rand_num = np.floor(np.random.rand(rois_per_image) * bg_num_rois)\n",
    "                rand_num = torch.from_numpy(rand_num).type_as(gt_boxes).long()\n",
    "\n",
    "                bg_inds = bg_inds[rand_num]\n",
    "                bg_rois_per_this_image = rois_per_image\n",
    "                fg_rois_per_this_image = 0\n",
    "            else:\n",
    "                raise ValueError(\"bg_num_rois = 0 and fg_num_rois = 0, this should not happen!\")\n",
    "\n",
    "            # The indices that we're selecting (both fg and bg)\n",
    "            keep_inds = torch.cat([fg_inds, bg_inds], 0)\n",
    "\n",
    "            # Select sampled values from various arrays:\n",
    "            labels_batch[i].copy_(labels[i][keep_inds])\n",
    "\n",
    "            # Clamp labels for the background RoIs to 0\n",
    "            if fg_rois_per_this_image < rois_per_image:\n",
    "                labels_batch[i][fg_rois_per_this_image:] = 0\n",
    "\n",
    "            rois_batch[i] = all_rois[i][keep_inds]\n",
    "            rois_batch[i,:,0] = i\n",
    "\n",
    "            gt_rois_batch[i] = gt_boxes[i][gt_assignment[i][keep_inds]]\n",
    "\n",
    "        bbox_target_data = self._compute_targets_pytorch(\n",
    "                rois_batch[:,:,1:5], gt_rois_batch[:,:,:4])\n",
    "\n",
    "        bbox_targets, bbox_inside_weights = \\\n",
    "                self._get_bbox_regression_labels_pytorch(bbox_target_data, labels_batch, num_classes)\n",
    "\n",
    "        return labels_batch, rois_batch, bbox_targets, bbox_inside_weightsimport torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "from ..utils.config import cfg\n",
    "from .bbox_transform import bbox_overlaps_batch, bbox_transform_batch\n",
    "import pdb\n",
    "\n",
    "class _ProposalTargetLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Assign object detection proposals to ground-truth targets. Produces proposal\n",
    "    classification labels and bounding-box regression targets.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, nclasses):\n",
    "        super(_ProposalTargetLayer, self).__init__()\n",
    "        self._num_classes = nclasses\n",
    "        self.BBOX_NORMALIZE_MEANS = torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_MEANS)\n",
    "        self.BBOX_NORMALIZE_STDS = torch.FloatTensor(cfg.TRAIN.BBOX_NORMALIZE_STDS)\n",
    "        self.BBOX_INSIDE_WEIGHTS = torch.FloatTensor(cfg.TRAIN.BBOX_INSIDE_WEIGHTS)\n",
    "\n",
    "    def forward(self, all_rois, gt_boxes, num_boxes):\n",
    "\n",
    "        self.BBOX_NORMALIZE_MEANS = self.BBOX_NORMALIZE_MEANS.type_as(gt_boxes)\n",
    "        self.BBOX_NORMALIZE_STDS = self.BBOX_NORMALIZE_STDS.type_as(gt_boxes)\n",
    "        self.BBOX_INSIDE_WEIGHTS = self.BBOX_INSIDE_WEIGHTS.type_as(gt_boxes)\n",
    "\n",
    "        gt_boxes_append = gt_boxes.new(gt_boxes.size()).zero_()\n",
    "        gt_boxes_append[:,:,1:5] = gt_boxes[:,:,:4]\n",
    "\n",
    "        # Include ground-truth boxes in the set of candidate rois\n",
    "        all_rois = torch.cat([all_rois, gt_boxes_append], 1)\n",
    "\n",
    "        num_images = 1\n",
    "        rois_per_image = int(cfg.TRAIN.BATCH_SIZE / num_images)\n",
    "        fg_rois_per_image = int(np.round(cfg.TRAIN.FG_FRACTION * rois_per_image))\n",
    "        fg_rois_per_image = 1 if fg_rois_per_image == 0 else fg_rois_per_image\n",
    "\n",
    "        labels, rois, bbox_targets, bbox_inside_weights = self._sample_rois_pytorch(\n",
    "            all_rois, gt_boxes, fg_rois_per_image,\n",
    "            rois_per_image, self._num_classes)\n",
    "\n",
    "        bbox_outside_weights = (bbox_inside_weights > 0).float()\n",
    "\n",
    "        return rois, labels, bbox_targets, bbox_inside_weights, bbox_outside_weights\n",
    "\n",
    "    def backward(self, top, propagate_down, bottom):\n",
    "        \"\"\"This layer does not propagate gradients.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def reshape(self, bottom, top):\n",
    "        \"\"\"Reshaping happens during the call to forward.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def _get_bbox_regression_labels_pytorch(self, bbox_target_data, labels_batch, num_classes):\n",
    "        \"\"\"Bounding-box regression targets (bbox_target_data) are stored in a\n",
    "        compact form b x N x (class, tx, ty, tw, th)\n",
    "        This function expands those targets into the 4-of-4*K representation used\n",
    "        by the network (i.e. only one class has non-zero targets).\n",
    "        Returns:\n",
    "            bbox_target (ndarray): b x N x 4K blob of regression targets\n",
    "            bbox_inside_weights (ndarray): b x N x 4K blob of loss weights\n",
    "        \"\"\"\n",
    "        batch_size = labels_batch.size(0)\n",
    "        rois_per_image = labels_batch.size(1)\n",
    "        clss = labels_batch\n",
    "        bbox_targets = bbox_target_data.new(batch_size, rois_per_image, 4).zero_()\n",
    "        bbox_inside_weights = bbox_target_data.new(bbox_targets.size()).zero_()\n",
    "\n",
    "        for b in range(batch_size):\n",
    "            # assert clss[b].sum() > 0\n",
    "            if clss[b].sum() == 0:\n",
    "                continue\n",
    "            inds = torch.nonzero(clss[b] > 0).view(-1)\n",
    "            for i in range(inds.numel()):\n",
    "                ind = inds[i]\n",
    "                bbox_targets[b, ind, :] = bbox_target_data[b, ind, :]\n",
    "                bbox_inside_weights[b, ind, :] = self.BBOX_INSIDE_WEIGHTS\n",
    "\n",
    "        return bbox_targets, bbox_inside_weights\n",
    "\n",
    "\n",
    "    def _compute_targets_pytorch(self, ex_rois, gt_rois):\n",
    "        \"\"\"Compute bounding-box regression targets for an image.\"\"\"\n",
    "\n",
    "        assert ex_rois.size(1) == gt_rois.size(1)\n",
    "        assert ex_rois.size(2) == 4\n",
    "        assert gt_rois.size(2) == 4\n",
    "\n",
    "        batch_size = ex_rois.size(0)\n",
    "        rois_per_image = ex_rois.size(1)\n",
    "\n",
    "        targets = bbox_transform_batch(ex_rois, gt_rois)\n",
    "\n",
    "        if cfg.TRAIN.BBOX_NORMALIZE_TARGETS_PRECOMPUTED:\n",
    "            # Optionally normalize targets by a precomputed mean and stdev\n",
    "            targets = ((targets - self.BBOX_NORMALIZE_MEANS.expand_as(targets))\n",
    "                        / self.BBOX_NORMALIZE_STDS.expand_as(targets))\n",
    "\n",
    "        return targets\n",
    "\n",
    "\n",
    "    def _sample_rois_pytorch(self, all_rois, gt_boxes, fg_rois_per_image, rois_per_image, num_classes):\n",
    "        \"\"\"Generate a random sample of RoIs comprising foreground and background\n",
    "        examples.\n",
    "        \"\"\"\n",
    "        # overlaps: (rois x gt_boxes)\n",
    "\n",
    "        overlaps = bbox_overlaps_batch(all_rois, gt_boxes)\n",
    "\n",
    "        max_overlaps, gt_assignment = torch.max(overlaps, 2)\n",
    "\n",
    "        batch_size = overlaps.size(0)\n",
    "        num_proposal = overlaps.size(1)\n",
    "        num_boxes_per_img = overlaps.size(2)\n",
    "\n",
    "        offset = torch.arange(0, batch_size)*gt_boxes.size(1)\n",
    "        offset = offset.view(-1, 1).type_as(gt_assignment) + gt_assignment\n",
    "\n",
    "        labels = gt_boxes[:,:,4].contiguous().view(-1).index((offset.view(-1),)).view(batch_size, -1)\n",
    "        \n",
    "        labels_batch = labels.new(batch_size, rois_per_image).zero_()\n",
    "        rois_batch  = all_rois.new(batch_size, rois_per_image, 5).zero_()\n",
    "        gt_rois_batch = all_rois.new(batch_size, rois_per_image, 5).zero_()\n",
    "        # Guard against the case when an image has fewer than max_fg_rois_per_image\n",
    "        # foreground RoIs\n",
    "        for i in range(batch_size):\n",
    "\n",
    "            fg_inds = torch.nonzero(max_overlaps[i] >= cfg.TRAIN.FG_THRESH).view(-1)\n",
    "            fg_num_rois = fg_inds.numel()\n",
    "\n",
    "            # Select background RoIs as those within [BG_THRESH_LO, BG_THRESH_HI)\n",
    "            bg_inds = torch.nonzero((max_overlaps[i] < cfg.TRAIN.BG_THRESH_HI) &\n",
    "                                    (max_overlaps[i] >= cfg.TRAIN.BG_THRESH_LO)).view(-1)\n",
    "            bg_num_rois = bg_inds.numel()\n",
    "\n",
    "            if fg_num_rois > 0 and bg_num_rois > 0:\n",
    "                # sampling fg\n",
    "                fg_rois_per_this_image = min(fg_rois_per_image, fg_num_rois)\n",
    "\n",
    "                # torch.randperm seems has a bug on multi-gpu setting that cause the segfault.\n",
    "                # See https://github.com/pytorch/pytorch/issues/1868 for more details.\n",
    "                # use numpy instead.\n",
    "                #rand_num = torch.randperm(fg_num_rois).long().cuda()\n",
    "                rand_num = torch.from_numpy(np.random.permutation(fg_num_rois)).type_as(gt_boxes).long()\n",
    "                fg_inds = fg_inds[rand_num[:fg_rois_per_this_image]]\n",
    "\n",
    "                # sampling bg\n",
    "                bg_rois_per_this_image = rois_per_image - fg_rois_per_this_image\n",
    "\n",
    "                # Seems torch.rand has a bug, it will generate very large number and make an error.\n",
    "                # We use numpy rand instead.\n",
    "                #rand_num = (torch.rand(bg_rois_per_this_image) * bg_num_rois).long().cuda()\n",
    "                rand_num = np.floor(np.random.rand(bg_rois_per_this_image) * bg_num_rois)\n",
    "                rand_num = torch.from_numpy(rand_num).type_as(gt_boxes).long()\n",
    "                bg_inds = bg_inds[rand_num]\n",
    "\n",
    "            elif fg_num_rois > 0 and bg_num_rois == 0:\n",
    "                # sampling fg\n",
    "                #rand_num = torch.floor(torch.rand(rois_per_image) * fg_num_rois).long().cuda()\n",
    "                rand_num = np.floor(np.random.rand(rois_per_image) * fg_num_rois)\n",
    "                rand_num = torch.from_numpy(rand_num).type_as(gt_boxes).long()\n",
    "                fg_inds = fg_inds[rand_num]\n",
    "                fg_rois_per_this_image = rois_per_image\n",
    "                bg_rois_per_this_image = 0\n",
    "            elif bg_num_rois > 0 and fg_num_rois == 0:\n",
    "                # sampling bg\n",
    "                #rand_num = torch.floor(torch.rand(rois_per_image) * bg_num_rois).long().cuda()\n",
    "                rand_num = np.floor(np.random.rand(rois_per_image) * bg_num_rois)\n",
    "                rand_num = torch.from_numpy(rand_num).type_as(gt_boxes).long()\n",
    "\n",
    "                bg_inds = bg_inds[rand_num]\n",
    "                bg_rois_per_this_image = rois_per_image\n",
    "                fg_rois_per_this_image = 0\n",
    "            else:\n",
    "                raise ValueError(\"bg_num_rois = 0 and fg_num_rois = 0, this should not happen!\")\n",
    "\n",
    "            # The indices that we're selecting (both fg and bg)\n",
    "            keep_inds = torch.cat([fg_inds, bg_inds], 0)\n",
    "\n",
    "            # Select sampled values from various arrays:\n",
    "            labels_batch[i].copy_(labels[i][keep_inds])\n",
    "\n",
    "            # Clamp labels for the background RoIs to 0\n",
    "            if fg_rois_per_this_image < rois_per_image:\n",
    "                labels_batch[i][fg_rois_per_this_image:] = 0\n",
    "\n",
    "            rois_batch[i] = all_rois[i][keep_inds]\n",
    "            rois_batch[i,:,0] = i\n",
    "\n",
    "            gt_rois_batch[i] = gt_boxes[i][gt_assignment[i][keep_inds]]\n",
    "\n",
    "        bbox_target_data = self._compute_targets_pytorch(\n",
    "                rois_batch[:,:,1:5], gt_rois_batch[:,:,:4])\n",
    "\n",
    "        bbox_targets, bbox_inside_weights = \\\n",
    "                self._get_bbox_regression_labels_pytorch(bbox_target_data, labels_batch, num_classes)\n",
    "\n",
    "        return labels_batch, rois_batch, bbox_targets, bbox_inside_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96558ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_anchors(base_size=16, ratios=[0.5, 1, 2],\n",
    "                     scales=2**np.arange(3, 6)):\n",
    "    \"\"\"\n",
    "    Generate anchor (reference) windows by enumerating aspect ratios X\n",
    "    scales wrt a reference (0, 0, 15, 15) window.\n",
    "    \"\"\"\n",
    "\n",
    "    base_anchor = np.array([1, 1, base_size, base_size]) - 1\n",
    "    ratio_anchors = _ratio_enum(base_anchor, ratios)\n",
    "    anchors = np.vstack([_scale_enum(ratio_anchors[i, :], scales)\n",
    "                         for i in xrange(ratio_anchors.shape[0])])\n",
    "    return anchors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cbca6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "# --------------------------------------------------------\n",
    "# Faster R-CNN\n",
    "# Copyright (c) 2015 Microsoft\n",
    "# Licensed under The MIT License [see LICENSE for details]\n",
    "# Written by Ross Girshick and Sean Bell\n",
    "# --------------------------------------------------------\n",
    "# --------------------------------------------------------\n",
    "# Reorganized and modified by Jianwei Yang and Jiasen Lu\n",
    "# --------------------------------------------------------\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import numpy.random as npr\n",
    "\n",
    "from model.utils.config import cfg\n",
    "from .generate_anchors import generate_anchors\n",
    "from .bbox_transform import clip_boxes, bbox_overlaps_batch, bbox_transform_batch\n",
    "\n",
    "import pdb\n",
    "\n",
    "DEBUG = False\n",
    "\n",
    "try:\n",
    "    long        # Python 2\n",
    "except NameError:\n",
    "    long = int  # Python 3\n",
    "\n",
    "\n",
    "class _AnchorTargetLayer(nn.Module):\n",
    "    \"\"\"\n",
    "        Assign anchors to ground-truth targets. Produces anchor classification\n",
    "        labels and bounding-box regression targets.\n",
    "    \"\"\"\n",
    "    def __init__(self, feat_stride, scales, ratios):\n",
    "        super(_AnchorTargetLayer, self).__init__()\n",
    "\n",
    "        self._feat_stride = feat_stride\n",
    "        self._scales = scales\n",
    "        anchor_scales = scales\n",
    "        # Use anchors generation.\n",
    "        self._anchors = torch.from_numpy(generate_anchors(scales=np.array(anchor_scales), ratios=np.array(ratios))).float()\n",
    "        self._num_anchors = self._anchors.size(0)\n",
    "\n",
    "        # allow boxes to sit over the edge by a small amount\n",
    "        self._allowed_border = 0  # default is 0\n",
    "\n",
    "    def forward(self, input):\n",
    "        # Algorithm:\n",
    "        #\n",
    "        # for each (H, W) location i\n",
    "        #   generate 9 anchor boxes centered on cell i\n",
    "        #   apply predicted bbox deltas at cell i to each of the 9 anchors\n",
    "        # filter out-of-image anchors\n",
    "\n",
    "        rpn_cls_score = input[0]\n",
    "        gt_boxes = input[1]\n",
    "        im_info = input[2]\n",
    "        num_boxes = input[3]\n",
    "\n",
    "        # map of shape (..., H, W)\n",
    "        height, width = rpn_cls_score.size(2), rpn_cls_score.size(3)\n",
    "\n",
    "        batch_size = gt_boxes.size(0)\n",
    "\n",
    "        feat_height, feat_width = rpn_cls_score.size(2), rpn_cls_score.size(3)\n",
    "        shift_x = np.arange(0, feat_width) * self._feat_stride\n",
    "        shift_y = np.arange(0, feat_height) * self._feat_stride\n",
    "        shift_x, shift_y = np.meshgrid(shift_x, shift_y)\n",
    "        shifts = torch.from_numpy(np.vstack((shift_x.ravel(), shift_y.ravel(),\n",
    "                                  shift_x.ravel(), shift_y.ravel())).transpose())\n",
    "        shifts = shifts.contiguous().type_as(rpn_cls_score).float()\n",
    "\n",
    "        A = self._num_anchors\n",
    "        K = shifts.size(0)\n",
    "\n",
    "        self._anchors = self._anchors.type_as(gt_boxes) # move to specific gpu.\n",
    "        all_anchors = self._anchors.view(1, A, 4) + shifts.view(K, 1, 4)\n",
    "        all_anchors = all_anchors.view(K * A, 4)\n",
    "\n",
    "        total_anchors = int(K * A)\n",
    "\n",
    "        keep = ((all_anchors[:, 0] >= -self._allowed_border) &\n",
    "                (all_anchors[:, 1] >= -self._allowed_border) &\n",
    "                (all_anchors[:, 2] < long(im_info[0][1]) + self._allowed_border) &\n",
    "                (all_anchors[:, 3] < long(im_info[0][0]) + self._allowed_border))\n",
    "\n",
    "        inds_inside = torch.nonzero(keep).view(-1)\n",
    "\n",
    "        # keep only inside anchors\n",
    "        anchors = all_anchors[inds_inside, :]\n",
    "\n",
    "        # label: 1 is positive, 0 is negative, -1 is dont care\n",
    "        labels = gt_boxes.new(batch_size, inds_inside.size(0)).fill_(-1)\n",
    "        bbox_inside_weights = gt_boxes.new(batch_size, inds_inside.size(0)).zero_()\n",
    "        bbox_outside_weights = gt_boxes.new(batch_size, inds_inside.size(0)).zero_()\n",
    "\n",
    "        overlaps = bbox_overlaps_batch(anchors, gt_boxes)\n",
    "\n",
    "        max_overlaps, argmax_overlaps = torch.max(overlaps, 2)\n",
    "        gt_max_overlaps, _ = torch.max(overlaps, 1)\n",
    "\n",
    "        if not cfg.TRAIN.RPN_CLOBBER_POSITIVES:\n",
    "            labels[max_overlaps < cfg.TRAIN.RPN_NEGATIVE_OVERLAP] = 0\n",
    "\n",
    "        gt_max_overlaps[gt_max_overlaps==0] = 1e-5\n",
    "        keep = torch.sum(overlaps.eq(gt_max_overlaps.view(batch_size,1,-1).expand_as(overlaps)), 2)\n",
    "\n",
    "        if torch.sum(keep) > 0:\n",
    "            labels[keep>0] = 1\n",
    "\n",
    "        # fg label: above threshold IOU\n",
    "        labels[max_overlaps >= cfg.TRAIN.RPN_POSITIVE_OVERLAP] = 1\n",
    "\n",
    "        if cfg.TRAIN.RPN_CLOBBER_POSITIVES:\n",
    "            labels[max_overlaps < cfg.TRAIN.RPN_NEGATIVE_OVERLAP] = 0\n",
    "\n",
    "        num_fg = int(cfg.TRAIN.RPN_FG_FRACTION * cfg.TRAIN.RPN_BATCHSIZE)\n",
    "\n",
    "        sum_fg = torch.sum((labels == 1).int(), 1)\n",
    "        sum_bg = torch.sum((labels == 0).int(), 1)\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            # subsample positive labels if we have too many\n",
    "            if sum_fg[i] > num_fg:\n",
    "                fg_inds = torch.nonzero(labels[i] == 1).view(-1)\n",
    "                # torch.randperm seems has a bug on multi-gpu setting that cause the segfault.\n",
    "                # See https://github.com/pytorch/pytorch/issues/1868 for more details.\n",
    "                # use numpy instead.\n",
    "                #rand_num = torch.randperm(fg_inds.size(0)).type_as(gt_boxes).long()\n",
    "                rand_num = torch.from_numpy(np.random.permutation(fg_inds.size(0))).type_as(gt_boxes).long()\n",
    "                disable_inds = fg_inds[rand_num[:fg_inds.size(0)-num_fg]]\n",
    "                labels[i][disable_inds] = -1\n",
    "\n",
    "#           num_bg = cfg.TRAIN.RPN_BATCHSIZE - sum_fg[i]\n",
    "            num_bg = cfg.TRAIN.RPN_BATCHSIZE - torch.sum((labels == 1).int(), 1)[i]\n",
    "\n",
    "            # subsample negative labels if we have too many\n",
    "            if sum_bg[i] > num_bg:\n",
    "                bg_inds = torch.nonzero(labels[i] == 0).view(-1)\n",
    "                #rand_num = torch.randperm(bg_inds.size(0)).type_as(gt_boxes).long()\n",
    "\n",
    "                rand_num = torch.from_numpy(np.random.permutation(bg_inds.size(0))).type_as(gt_boxes).long()\n",
    "                disable_inds = bg_inds[rand_num[:bg_inds.size(0)-num_bg]]\n",
    "                labels[i][disable_inds] = -1\n",
    "\n",
    "        offset = torch.arange(0, batch_size)*gt_boxes.size(1)\n",
    "\n",
    "        argmax_overlaps = argmax_overlaps + offset.view(batch_size, 1).type_as(argmax_overlaps)\n",
    "        bbox_targets = _compute_targets_batch(anchors, gt_boxes.view(-1,5)[argmax_overlaps.view(-1), :].view(batch_size, -1, 5))\n",
    "\n",
    "        # use a single value instead of 4 values for easy index.\n",
    "        bbox_inside_weights[labels==1] = cfg.TRAIN.RPN_BBOX_INSIDE_WEIGHTS[0]\n",
    "\n",
    "        if cfg.TRAIN.RPN_POSITIVE_WEIGHT < 0:\n",
    "            num_examples = torch.sum(labels[i] >= 0)\n",
    "            positive_weights = 1.0 / num_examples.item()\n",
    "            negative_weights = 1.0 / num_examples.item()\n",
    "        else:\n",
    "            assert ((cfg.TRAIN.RPN_POSITIVE_WEIGHT > 0) &\n",
    "                    (cfg.TRAIN.RPN_POSITIVE_WEIGHT < 1))\n",
    "\n",
    "        bbox_outside_weights[labels == 1] = positive_weights\n",
    "        bbox_outside_weights[labels == 0] = negative_weights\n",
    "\n",
    "        labels = _unmap(labels, total_anchors, inds_inside, batch_size, fill=-1)\n",
    "        bbox_targets = _unmap(bbox_targets, total_anchors, inds_inside, batch_size, fill=0)\n",
    "        bbox_inside_weights = _unmap(bbox_inside_weights, total_anchors, inds_inside, batch_size, fill=0)\n",
    "        bbox_outside_weights = _unmap(bbox_outside_weights, total_anchors, inds_inside, batch_size, fill=0)\n",
    "\n",
    "        outputs = []\n",
    "\n",
    "        labels = labels.view(batch_size, height, width, A).permute(0,3,1,2).contiguous()\n",
    "        labels = labels.view(batch_size, 1, A * height, width)\n",
    "        outputs.append(labels)\n",
    "\n",
    "        bbox_targets = bbox_targets.view(batch_size, height, width, A*4).permute(0,3,1,2).contiguous()\n",
    "        outputs.append(bbox_targets)\n",
    "\n",
    "        anchors_count = bbox_inside_weights.size(1)\n",
    "        bbox_inside_weights = bbox_inside_weights.view(batch_size,anchors_count,1).expand(batch_size, anchors_count, 4)\n",
    "\n",
    "        bbox_inside_weights = bbox_inside_weights.contiguous().view(batch_size, height, width, 4*A)\\\n",
    "                            .permute(0,3,1,2).contiguous()\n",
    "\n",
    "        outputs.append(bbox_inside_weights)\n",
    "\n",
    "        bbox_outside_weights = bbox_outside_weights.view(batch_size,anchors_count,1).expand(batch_size, anchors_count, 4)\n",
    "        bbox_outside_weights = bbox_outside_weights.contiguous().view(batch_size, height, width, 4*A)\\\n",
    "                            .permute(0,3,1,2).contiguous()\n",
    "        outputs.append(bbox_outside_weights)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def backward(self, top, propagate_down, bottom):\n",
    "        \"\"\"This layer does not propagate gradients.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def reshape(self, bottom, top):\n",
    "        \"\"\"Reshaping happens during the call to forward.\"\"\"\n",
    "        pass\n",
    "\n",
    "def _unmap(data, count, inds, batch_size, fill=0):\n",
    "    \"\"\" Unmap a subset of item (data) back to the original set of items (of\n",
    "    size count) \"\"\"\n",
    "\n",
    "    if data.dim() == 2:\n",
    "        ret = torch.Tensor(batch_size, count).fill_(fill).type_as(data)\n",
    "        ret[:, inds] = data\n",
    "    else:\n",
    "        ret = torch.Tensor(batch_size, count, data.size(2)).fill_(fill).type_as(data)\n",
    "        ret[:, inds,:] = data\n",
    "    return ret\n",
    "\n",
    "\n",
    "def _compute_targets_batch(ex_rois, gt_rois):\n",
    "    \"\"\"Compute bounding-box regression targets for an image.\"\"\"\n",
    "\n",
    "    return bbox_transform_batch(ex_rois, gt_rois[:, :, :4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec52f832",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _RPN(nn.Module):\n",
    "    \"\"\" region proposal network \"\"\"\n",
    "    def __init__(self, din):\n",
    "        super(_RPN, self).__init__()\n",
    "        \n",
    "        self.din = din  # get depth of input feature map, e.g., 512\n",
    "        #https://github.com/jwyang/faster-rcnn.pytorch/blob/f9d984d27b48a067b29792932bcb5321a39c1f09/lib/model/utils/config.py\n",
    "        self.anchor_scales = [8,16,32]\n",
    "        self.anchor_ratios = [0.5,1,2]\n",
    "        self.feat_stride = 16 #= FEAT_STRIDE[0] - In the file [16, ]\n",
    "\n",
    "        # define the convrelu layers processing input feature map\n",
    "        self.RPN_Conv = nn.Conv2d(self.din, 512, 3, 1, 1, bias=True)\n",
    "\n",
    "        # define bg/fg classifcation score layer\n",
    "        self.nc_score_out = len(self.anchor_scales) * len(self.anchor_ratios) * 2 # 2(bg/fg) * 9 (anchors)\n",
    "        self.RPN_cls_score = nn.Conv2d(512, self.nc_score_out, 1, 1, 0)\n",
    "\n",
    "        # define anchor box offset prediction layer\n",
    "        self.nc_bbox_out = len(self.anchor_scales) * len(self.anchor_ratios) * 4 # 4(coords) * 9 (anchors)\n",
    "        self.RPN_bbox_pred = nn.Conv2d(512, self.nc_bbox_out, 1, 1, 0)\n",
    "\n",
    "        # define proposal layer\n",
    "        #https://github.com/jwyang/faster-rcnn.pytorch/blob/f9d984d27b48a067b29792932bcb5321a39c1f09/lib/model/rpn/proposal_target_layer_cascade.py#L20\n",
    "        self.RPN_proposal = _ProposalLayer(self.feat_stride, self.anchor_scales, self.anchor_ratios)\n",
    "\n",
    "        # define anchor target layer\n",
    "        self.RPN_anchor_target = _AnchorTargetLayer(self.feat_stride, self.anchor_scales, self.anchor_ratios)\n",
    "\n",
    "        self.rpn_loss_cls = 0\n",
    "        self.rpn_loss_box = 0\n",
    "\n",
    "    @staticmethod\n",
    "    def reshape(x, d):\n",
    "        input_shape = x.size()\n",
    "        x = x.view(\n",
    "            input_shape[0],\n",
    "            int(d),\n",
    "            int(float(input_shape[1] * input_shape[2]) / float(d)),\n",
    "            input_shape[3]\n",
    "        )\n",
    "        return x\n",
    "\n",
    "    def forward(self, base_feat, im_info, gt_boxes, num_boxes):\n",
    "\n",
    "        batch_size = base_feat.size(0)\n",
    "\n",
    "        # return feature map after convrelu layer\n",
    "        rpn_conv1 = F.relu(self.RPN_Conv(base_feat), inplace=True)\n",
    "        # get rpn classification score\n",
    "        rpn_cls_score = self.RPN_cls_score(rpn_conv1)\n",
    "\n",
    "        rpn_cls_score_reshape = self.reshape(rpn_cls_score, 2)\n",
    "        rpn_cls_prob_reshape = F.softmax(rpn_cls_score_reshape, 1)\n",
    "        rpn_cls_prob = self.reshape(rpn_cls_prob_reshape, self.nc_score_out)\n",
    "\n",
    "        # get rpn offsets to the anchor boxes\n",
    "        rpn_bbox_pred = self.RPN_bbox_pred(rpn_conv1)\n",
    "\n",
    "        # proposal layer\n",
    "        cfg_key = 'TRAIN' if self.training else 'TEST'\n",
    "\n",
    "        rois = self.RPN_proposal((rpn_cls_prob.data, rpn_bbox_pred.data,\n",
    "                                 im_info, cfg_key))\n",
    "\n",
    "        self.rpn_loss_cls = 0\n",
    "        self.rpn_loss_box = 0\n",
    "\n",
    "        # generating training labels and build the rpn loss\n",
    "        if self.training:\n",
    "            assert gt_boxes is not None\n",
    "            \n",
    "            # REPLACE ANCHOR GENERATOR HERE\n",
    "            rpn_data = self.RPN_anchor_target((rpn_cls_score.data, gt_boxes, im_info, num_boxes))\n",
    "\n",
    "            # compute classification loss\n",
    "            rpn_cls_score = rpn_cls_score_reshape.permute(0, 2, 3, 1).contiguous().view(batch_size, -1, 2)\n",
    "            rpn_label = rpn_data[0].view(batch_size, -1)\n",
    "\n",
    "            rpn_keep = Variable(rpn_label.view(-1).ne(-1).nonzero().view(-1))\n",
    "            rpn_cls_score = torch.index_select(rpn_cls_score.view(-1,2), 0, rpn_keep)\n",
    "            rpn_label = torch.index_select(rpn_label.view(-1), 0, rpn_keep.data)\n",
    "            rpn_label = Variable(rpn_label.long())\n",
    "            self.rpn_loss_cls = F.cross_entropy(rpn_cls_score, rpn_label)\n",
    "            fg_cnt = torch.sum(rpn_label.data.ne(0))\n",
    "\n",
    "            rpn_bbox_targets, rpn_bbox_inside_weights, rpn_bbox_outside_weights = rpn_data[1:]\n",
    "\n",
    "            # compute bbox regression loss\n",
    "            rpn_bbox_inside_weights = Variable(rpn_bbox_inside_weights)\n",
    "            rpn_bbox_outside_weights = Variable(rpn_bbox_outside_weights)\n",
    "            rpn_bbox_targets = Variable(rpn_bbox_targets)\n",
    "\n",
    "            self.rpn_loss_box = _smooth_l1_loss(rpn_bbox_pred, rpn_bbox_targets, rpn_bbox_inside_weights,\n",
    "                                                            rpn_bbox_outside_weights, sigma=3, dim=[1,2,3])\n",
    "\n",
    "        return rois, self.rpn_loss_cls, self.rpn_loss_box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb78e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _fasterRCNN(nn.Module):\n",
    "#   \"\"\" faster RCNN \"\"\"\n",
    "    def __init__(self, classes, class_agnostic):\n",
    "        \n",
    "        super(_fasterRCNN, self).__init__()\n",
    "        self.classes = classes\n",
    "        self.n_classes = len(classes)\n",
    "        self.class_agnostic = class_agnostic\n",
    "   \n",
    " ######################### PART I ############################################################# \n",
    "## The CNN model applied to the full image:\n",
    "    # https://pytorch.org/hub/pytorch_vision_resnet/\n",
    "        self.resnetFixedModel = torchvision.models.resnet18(pretrained=True)\n",
    "        for param in resnetFixedModel.parameters(): param.requires_grad = False ## Should I change for bn?\n",
    "        \n",
    "           ## or I can use this....\n",
    "        pretrained_model = models.resnet18(pretrained=True)\n",
    "        my_model1 = nn.Sequential(*list(pretrained_model.children())[:-1])\n",
    "   \n",
    " ######################### PART II ############################################################# \n",
    "    # define rpn\n",
    "    # self.RCNN_rpn = _RPN(self.dout_base_model)\n",
    "    \n",
    "    #\n",
    "\n",
    " ######################### PART III ############################################################# \n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, im_data, im_info, gt_boxes, num_boxes):\n",
    "            \n",
    "        # Attributes class data\n",
    "        batch_size = im_data.size(0)\n",
    "        im_info = im_info.data \n",
    "        gt_boxes = gt_boxes.data\n",
    "        num_boxes = num_boxes.data\n",
    "        \n",
    "        \n",
    "        \n",
    " ######################### PART I ############################################################# \n",
    "## The CNN model applied to the full image:\n",
    "    # feed image data to base model to obtain base feature map\n",
    "    # base_feat = self.RCNN_base(im_data)\n",
    "   # https://github.com/jwyang/faster-rcnn.pytorch/blob/f9d984d27b48a067b29792932bcb5321a39c1f09/lib/model/faster_rcnn/resnet.py\n",
    "        \n",
    "        # 1. Feed image data to base model to obtain base feature map\n",
    "        # how to declare this rcnn_base?????\n",
    "        feature_map = resnetFixedModel(im_data)\n",
    "        num_logits = resnetFixedModel.fc.in_features # in_feature is the number of inputs for the linear layer\n",
    "        resnetFixedModel.fc = nn.Linear(num_logits, 22) ## 22 classes = 21+ background\n",
    "\n",
    "        \n",
    "######################### PART II #############################################################       \n",
    "        ### feed base feature map to RPN to obtain rois\n",
    "        #rois, rpn_loss_cls, rpn_loss_bbox = self.RCNN_rpn(base_feat, im_info, gt_boxes, num_boxes)    \n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:CudaLab] *",
   "language": "python",
   "name": "conda-env-CudaLab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
